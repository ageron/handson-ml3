{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTxNhzr7MY3i"
      },
      "source": [
        "**Chapter 10 – Introduction to Artificial Neural Networks with Keras**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VS631XKqMY3p"
      },
      "source": [
        "_This notebook contains all the sample code and solutions to the exercises in chapter 10._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yugdz0GbMY3q"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml3/blob/main/10_neural_nets_with_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/10_neural_nets_with_keras.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Vcy6jZMY3r"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNAHkiTnMY3t"
      },
      "source": [
        "This project requires Python 3.7 or above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ojv63o6dMY3t"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1xJ6jQbMY3v"
      },
      "source": [
        "It also requires Scikit-Learn ≥ 1.0.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hLhHNN12MY3x"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import sklearn\n",
        "\n",
        "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWTY6YK9MY3x"
      },
      "source": [
        "And TensorFlow ≥ 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysFXFBWRMY3y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQmFOWKfMY3z"
      },
      "source": [
        "As we did in previous chapters, let's define the default font sizes to make the figures prettier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjomWsryMY3z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Li_aYSdMY3z"
      },
      "source": [
        "And let's create the `images/ann` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M49GASjNMY30"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"ann\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YaYZfNhMY30"
      },
      "source": [
        "# From Biological to Artificial Neurons\n",
        "## The Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4NznZg-MY31"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "\n",
        "iris = load_iris(as_frame=True)\n",
        "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
        "y = (iris.target == 0)  # Iris setosa\n",
        "\n",
        "per_clf = Perceptron(random_state=42)\n",
        "per_clf.fit(X, y)\n",
        "\n",
        "X_new = [[2, 0.5], [3, 1]]\n",
        "y_pred = per_clf.predict(X_new)  # predicts True and False for these 2 flowers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rbw2ESsMY31"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding Template </b></summary>\n",
        "\n",
        "## 1. What is it?\n",
        "A single-layer linear classifier...\n",
        "\n",
        "## 2. How does it reason?\n",
        "The perceptron learns...\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "iFtCfufjXqjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding – Perceptron Classifier</b></summary>\n",
        "\n",
        "## ✅**AI Understanding Template – Perceptron Classifier**\n",
        "\n",
        "  ## **1. What is it?**\n",
        "\n",
        "  A **single-layer linear classifier** that finds a straight line to separate two classes using simple updates.\n",
        "\n",
        "  In your code → classifies **Iris Setosa vs not-Setosa** using two features.\n",
        "\n",
        "  ---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "The perceptron learns a **weight vector** and **bias**:\n",
        "\n",
        "> **prediction = sign(w·x + b)**\n",
        "\n",
        "It adjusts weights whenever it misclassifies a sample.\n",
        "It basically **pushes the decision boundary** until it can separate the classes.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* If classes are **not linearly separable** (perceptron will never converge).\n",
        "* Sensitive to **feature scaling**.\n",
        "* No probability outputs (only True/False).\n",
        "* Cannot model complex patterns — only a **single straight line**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use it when:\n",
        "\n",
        "* Data is **simple, linear, binary**.\n",
        "* You want a **very fast, explainable** model.\n",
        "* You want to understand the basics of neural networks.\n",
        "\n",
        "Not ideal for real-world complex datasets.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> **A yes/no switch that draws one straight boundary to split two groups.**\n",
        "\n",
        "It tries to push wrong predictions to the correct side by nudging weights.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(“Prompt” = how to give input / how to use it.)\n",
        "\n",
        "* Give numeric feature vectors to `fit()`:\n",
        "  `per_clf.fit(X, y)`\n",
        "* Pass new samples to `predict()`:\n",
        "  `per_clf.predict([[2, 0.5]])`\n",
        "* Ensure classes are **binary** (True/False).\n",
        "\n",
        "No hyperparameters needed except `max_iter`, `eta0`, `random_state`.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "Better linear/binary classifiers:\n",
        "\n",
        "* **Logistic Regression**\n",
        "* **Linear SVM**\n",
        "* **SGDClassifier(log loss)**\n",
        "\n",
        "Better for non-linear patterns:\n",
        "\n",
        "* **Kernel SVM**\n",
        "* **Random Forest**\n",
        "* **Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "## **Code Explanation (Short & Clear)**\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import Perceptron\n",
        "```\n",
        "\n",
        "Imports Iris dataset and Perceptron model.\n",
        "\n",
        "---\n",
        "\n",
        "### **Load dataset**\n",
        "\n",
        "```python\n",
        "iris = load_iris(as_frame=True)\n",
        "```\n",
        "\n",
        "Loads Iris into a pandas-like structure.\n",
        "\n",
        "---\n",
        "\n",
        "### **Select two features**\n",
        "\n",
        "```python\n",
        "X = iris.data[[\"petal length (cm)\", \"petal width (cm)\"]].values\n",
        "```\n",
        "\n",
        "Use only petal length & width → 2D input.\n",
        "\n",
        "---\n",
        "\n",
        "### **Create binary labels**\n",
        "\n",
        "```python\n",
        "y = (iris.target == 0)  # Iris setosa\n",
        "```\n",
        "\n",
        "True if class = Setosa, False otherwise.\n",
        "\n",
        "---\n",
        "\n",
        "### **Train perceptron**\n",
        "\n",
        "```python\n",
        "per_clf = Perceptron(random_state=42)\n",
        "per_clf.fit(X, y)\n",
        "```\n",
        "\n",
        "Learns a linear boundary separating Setosa vs non-Setosa.\n",
        "\n",
        "---\n",
        "\n",
        "### **Predict on new samples**\n",
        "\n",
        "```python\n",
        "X_new = [[2, 0.5], [3, 1]]\n",
        "y_pred = per_clf.predict(X_new)\n",
        "```\n",
        "\n",
        "Returns:\n",
        "\n",
        "* **True** → predicted Setosa\n",
        "* **False** → predicted non-Setosa\n",
        "\n",
        "<details>"
      ],
      "metadata": {
        "id": "MedR3lNgV55T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OX6RUfVMY32"
      },
      "source": [
        "The `Perceptron` is equivalent to a `SGDClassifier` with `loss=\"perceptron\"`, no regularization, and a constant learning rate equal to 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_0P1PIPMY32"
      },
      "outputs": [],
      "source": [
        "# extra code – shows how to build and train a Perceptron\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(loss=\"perceptron\", penalty=None,\n",
        "                        learning_rate=\"constant\", eta0=1, random_state=42)\n",
        "sgd_clf.fit(X, y)\n",
        "assert (sgd_clf.coef_ == per_clf.coef_).all()\n",
        "assert (sgd_clf.intercept_ == per_clf.intercept_).all()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7swxIkVpMY33"
      },
      "source": [
        "When the Perceptron finds a decision boundary that properly separates the classes, it stops learning. This means that the decision boundary is often quite close to one class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SIckuWnMY33"
      },
      "outputs": [],
      "source": [
        "# extra code – plots the decision boundary of a Perceptron on the iris dataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "a = -per_clf.coef_[0, 0] / per_clf.coef_[0, 1]\n",
        "b = -per_clf.intercept_ / per_clf.coef_[0, 1]\n",
        "axes = [0, 5, 0, 2]\n",
        "x0, x1 = np.meshgrid(\n",
        "    np.linspace(axes[0], axes[1], 500).reshape(-1, 1),\n",
        "    np.linspace(axes[2], axes[3], 200).reshape(-1, 1),\n",
        ")\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "y_predict = per_clf.predict(X_new)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "custom_cmap = ListedColormap(['#9898ff', '#fafab0'])\n",
        "\n",
        "plt.figure(figsize=(7, 3))\n",
        "plt.plot(X[y == 0, 0], X[y == 0, 1], \"bs\", label=\"Not Iris setosa\")\n",
        "plt.plot(X[y == 1, 0], X[y == 1, 1], \"yo\", label=\"Iris setosa\")\n",
        "plt.plot([axes[0], axes[1]], [a * axes[0] + b, a * axes[1] + b], \"k-\",\n",
        "         linewidth=3)\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "plt.xlabel(\"Petal length\")\n",
        "plt.ylabel(\"Petal width\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.axis(axes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding - Perceptron-2 </b></summary>\n",
        "\n",
        "## ✅**AI Understanding Template — Perceptron (using SGDClassifier)**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A **linear binary classifier** that learns a separating line (hyperplane) by adjusting weights whenever it misclassifies a sample.\n",
        "\n",
        "Equivalent to:\n",
        "**SGDClassifier(loss=\"perceptron\") → classic Perceptron algorithm.**\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "Each training sample updates weights:\n",
        "\n",
        "> **If prediction is wrong → move the decision boundary toward correct class.**\n",
        "> If correct → no update.\n",
        "\n",
        "Mathematically:\n",
        "`w_new = w_old + learning_rate * (y * x)` for misclassified samples.\n",
        "\n",
        "It learns by **incremental weight nudges** based on errors.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* If data is **not linearly separable**, it keeps oscillating → no convergence.\n",
        "* Sensitive to **feature scaling**.\n",
        "* Performs poorly with **noise** or **overlapping classes**.\n",
        "* Only good for **binary classification** (multi-class via one-vs-all).\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "* When you need an **extremely fast** linear classifier.\n",
        "* For **online learning** (updates as new data arrives).\n",
        "* When dataset is **large** and you want **incremental training**.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> **A line that keeps pivoting every time it makes a mistake.**\n",
        "\n",
        "Like a student adjusting their answer after every error.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(Meaning: how to use it effectively.)\n",
        "\n",
        "* Use **scaled features** (`StandardScaler`).\n",
        "* Ensure **binary labels** (0/1 or -1/+1).\n",
        "* Tune these:\n",
        "\n",
        "  * `learning_rate`,\n",
        "  * `eta0`,\n",
        "  * `max_iter`,\n",
        "  * `penalty` (if regularization needed).\n",
        "* Provide **shuffled data** (important for SGD).\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "Better linear models:\n",
        "\n",
        "* **Logistic Regression** – probabilistic, stable\n",
        "* **Linear SVM** – maximizes margin → better accuracy\n",
        "* **SGDClassifier(loss=\"log\")** – SGD + logistic regression\n",
        "* **Perceptron()** (Scikit-learn's dedicated class)\n",
        "\n",
        "For non-linear:\n",
        "\n",
        "* **Random Forest**, **XGBoost**, **Neural Networks**\n",
        "\n",
        "---\n",
        "\n",
        "## **Code Explanation (Short + Clear)**\n",
        "\n",
        "### **1. Create Perceptron via SGD**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(\n",
        "    loss=\"perceptron\",   # tells SGD to mimic the Perceptron update rule\n",
        "    penalty=None,        # no regularization\n",
        "    learning_rate=\"constant\",\n",
        "    eta0=1,              # fixed step size\n",
        "    random_state=42\n",
        ")\n",
        "sgd_clf.fit(X, y)\n",
        "```\n",
        "\n",
        "### **2. Verify equality with another perceptron**\n",
        "\n",
        "```python\n",
        "assert (sgd_clf.coef_ == per_clf.coef_).all()\n",
        "assert (sgd_clf.intercept_ == per_clf.intercept_).all()\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "Both models learned **the exact same weights**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Plotting the Decision Boundary**\n",
        "\n",
        "### **3. Compute line equation from weights**\n",
        "\n",
        "Perceptron boundary:\n",
        "`w1*x1 + w2*x2 + b = 0`\n",
        "\n",
        "Solve for `x2`:\n",
        "\n",
        "```python\n",
        "a = -per_clf.coef_[0, 0] / per_clf.coef_[0, 1]  # slope\n",
        "b = -per_clf.intercept_ / per_clf.coef_[0, 1]   # intercept\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Create a grid to visualize predictions**\n",
        "\n",
        "```python\n",
        "x0, x1 = np.meshgrid(\n",
        "    np.linspace(0, 5, 500).reshape(-1, 1),   # petal length\n",
        "    np.linspace(0, 2, 200).reshape(-1, 1)    # petal width\n",
        ")\n",
        "X_new = np.c_[x0.ravel(), x1.ravel()]\n",
        "y_predict = per_clf.predict(X_new)\n",
        "zz = y_predict.reshape(x0.shape)\n",
        "```\n",
        "\n",
        "This creates 100,000 points and predicts class for each → gives a colored region.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Plot data, decision boundary, and background**\n",
        "\n",
        "```python\n",
        "plt.plot(X[y == 0, 0], X[y == 0, 1], \"bs\", label=\"Not Iris setosa\")\n",
        "plt.plot(X[y == 1, 0], X[y == 1, 1], \"yo\", label=\"Iris setosa\")\n",
        "plt.plot([0,5], [a*0 + b, a*5 + b], \"k-\", linewidth=3)\n",
        "plt.contourf(x0, x1, zz, cmap=custom_cmap)\n",
        "```\n",
        "\n",
        "* Blue squares = class 0\n",
        "* Yellow circles = class 1\n",
        "* Black line = decision boundary\n",
        "* Background shades = predicted regions\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "PG1Ttn3NCRay"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzK-5rwCMY34"
      },
      "source": [
        "**Activation functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxCG9zXmMY35"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 10–8\n",
        "\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "\n",
        "def derivative(f, z, eps=0.000001):\n",
        "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
        "\n",
        "max_z = 4.5\n",
        "z = np.linspace(-max_z, max_z, 200)\n",
        "\n",
        "plt.figure(figsize=(11, 3.1))\n",
        "\n",
        "plt.subplot(121)\n",
        "plt.plot([-max_z, 0], [0, 0], \"r-\", linewidth=2, label=\"Heaviside\")\n",
        "plt.plot(z, relu(z), \"m-.\", linewidth=2, label=\"ReLU\")\n",
        "plt.plot([0, 0], [0, 1], \"r-\", linewidth=0.5)\n",
        "plt.plot([0, max_z], [1, 1], \"r-\", linewidth=2)\n",
        "plt.plot(z, sigmoid(z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.plot(z, np.tanh(z), \"b-\", linewidth=1, label=\"Tanh\")\n",
        "plt.grid(True)\n",
        "plt.title(\"Activation functions\")\n",
        "plt.axis([-max_z, max_z, -1.65, 2.4])\n",
        "plt.gca().set_yticks([-1, 0, 1, 2])\n",
        "plt.legend(loc=\"lower right\", fontsize=13)\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(z, derivative(np.sign, z), \"r-\", linewidth=2, label=\"Heaviside\")\n",
        "plt.plot(0, 0, \"ro\", markersize=5)\n",
        "plt.plot(0, 0, \"rx\", markersize=10)\n",
        "plt.plot(z, derivative(sigmoid, z), \"g--\", linewidth=2, label=\"Sigmoid\")\n",
        "plt.plot(z, derivative(np.tanh, z), \"b-\", linewidth=1, label=\"Tanh\")\n",
        "plt.plot([-max_z, 0], [0, 0], \"m-.\", linewidth=2)\n",
        "plt.plot([0, max_z], [1, 1], \"m-.\", linewidth=2)\n",
        "plt.plot([0, 0], [0, 1], \"m-.\", linewidth=1.2)\n",
        "plt.plot(0, 1, \"mo\", markersize=5)\n",
        "plt.plot(0, 1, \"mx\", markersize=10)\n",
        "plt.grid(True)\n",
        "plt.title(\"Derivatives\")\n",
        "plt.axis([-max_z, max_z, -0.2, 1.2])\n",
        "\n",
        "save_fig(\"activation_functions_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding - Perceptron-3 </b></summary>\n",
        "Below is a **short, crisp AI-framework explanation + clear code breakdown** for your activation-functions plot.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅**AI Understanding Template — Activation Functions Code**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A visualization script that plots:\n",
        "\n",
        "* Activation functions (ReLU, Sigmoid, Tanh, Heaviside)\n",
        "* Their numerical derivatives\n",
        "\n",
        "Used to understand how neural networks **activate** and **backpropagate**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* Each activation transforms input **z** → output\n",
        "* Derivatives show how gradients flow\n",
        "* Smooth functions (sigmoid/tanh) → smooth gradients\n",
        "* ReLU → piecewise linear\n",
        "* Heaviside → step with no useful gradients\n",
        "\n",
        "The derivative function uses **finite differences** to approximate the true derivative.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* Numerical derivative is unstable near discontinuities (Heaviside, ReLU at 0)\n",
        "* Sigmoid saturates → vanishing gradients\n",
        "* Tanh also saturates\n",
        "* ReLU dies for negative z (grad = 0)\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use when learning or demonstrating:\n",
        "\n",
        "* Activation behavior\n",
        "* Vanishing gradient problem\n",
        "* Why modern networks prefer ReLU family\n",
        "* Backprop intuition\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Mental model**\n",
        "\n",
        "Think of activations as:\n",
        "\n",
        "> **“Gatekeepers that decide how much signal passes to the next layer.”**\n",
        "\n",
        "Derivatives = how much learning signal (gradient) flows backward.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(Not LLM prompting — *code usage guidance*.)\n",
        "You “prompt” the functions by:\n",
        "\n",
        "* Passing a vector `z`\n",
        "* Plotting outputs\n",
        "* Using `derivative(func, z)` to approximate slopes\n",
        "* Tweaking activation choices to see differences\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Alternatives**\n",
        "\n",
        "For activations:\n",
        "\n",
        "* **Leaky ReLU**\n",
        "* **ELU / GELU**\n",
        "* **Swish / Mish**\n",
        "\n",
        "For derivatives:\n",
        "\n",
        "* Analytical derivatives (instead of finite difference)\n",
        "* Autograd / TensorFlow / PyTorch automatic differentiation\n",
        "\n",
        "---\n",
        "\n",
        "## **Code Explanation (Short + Clear)**\n",
        "\n",
        "### **1. Import + activation definitions**\n",
        "\n",
        "```python\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "```\n",
        "\n",
        "* `sigmoid` imported from SciPy\n",
        "* `relu` implemented manually\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Numerical derivative function**\n",
        "\n",
        "```python\n",
        "def derivative(f, z, eps=0.000001):\n",
        "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
        "```\n",
        "\n",
        "* Central difference formula\n",
        "* Approximates f′(z)\n",
        "* Works for any function `f`\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Prepare input range**\n",
        "\n",
        "```python\n",
        "max_z = 4.5\n",
        "z = np.linspace(-max_z, max_z, 200)\n",
        "```\n",
        "\n",
        "* 200 points from –4.5 to +4.5\n",
        "* Used for smooth plots\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Plot activation functions**\n",
        "\n",
        "```python\n",
        "plt.subplot(121)\n",
        "plt.plot(z, relu(z), \"m-.\", label=\"ReLU\")\n",
        "plt.plot(z, sigmoid(z), \"g--\", label=\"Sigmoid\")\n",
        "plt.plot(z, np.tanh(z), \"b-\", label=\"Tanh\")\n",
        "```\n",
        "\n",
        "Also draws Heaviside step using simple line plots.\n",
        "\n",
        "Purpose: visualize shape.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Plot derivatives**\n",
        "\n",
        "```python\n",
        "plt.subplot(122)\n",
        "plt.plot(z, derivative(sigmoid, z), \"g--\", label=\"Sigmoid\")\n",
        "plt.plot(z, derivative(np.tanh, z), \"b-\", label=\"Tanh\")\n",
        "plt.plot(z, derivative(np.sign, z), \"r-\", label=\"Heaviside\")\n",
        "```\n",
        "\n",
        "* Shows slopes\n",
        "* Critical for understanding gradient flow\n",
        "\n",
        "ReLU derivative is drawn manually using straight lines, because derivative is undefined at 0.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Decoration**\n",
        "\n",
        "Grid, axis limits, legends, labels, titles.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Save and show**\n",
        "\n",
        "```python\n",
        "save_fig(\"activation_functions_plot\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Saves high-quality image + displays it.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can (short):\n",
        "✔ add GELU/Swish to the plot\n",
        "✔ rewrite in TensorFlow or PyTorch\n",
        "✔ summarize “when to use which activation” in 5 lines\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "aFsq44qnDjDk"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51HLXOkLMY36"
      },
      "source": [
        "## Regression MLPs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVVXRtNcMY36"
      },
      "source": [
        "**Warning**: In recent versions of Scikit-Learn, you must use `root_mean_squared_error()` to compute the RMSE, instead of `mean_squared_error(labels, predictions, squared=False)`. The following `try`/`except` block tries to import `root_mean_squared_error`, and if it fails it just defines it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcA1_kt0MY37"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from sklearn.metrics import root_mean_squared_error\n",
        "except ImportError:\n",
        "    from sklearn.metrics import mean_squared_error\n",
        "\n",
        "    def root_mean_squared_error(labels, predictions):\n",
        "        return mean_squared_error(labels, predictions, squared=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ags_bcLPMY37"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)\n",
        "\n",
        "mlp_reg = MLPRegressor(hidden_layer_sizes=[50, 50, 50], random_state=42)\n",
        "pipeline = make_pipeline(StandardScaler(), mlp_reg)\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_valid)\n",
        "rmse = root_mean_squared_error(y_valid, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVTJUdgsMY38"
      },
      "outputs": [],
      "source": [
        "rmse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding - Regression MLPs </b></summary>\n",
        "\n",
        "## ✅**AI Understanding— Activation Functions Code**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A visualization script that plots:\n",
        "\n",
        "* Activation functions (ReLU, Sigmoid, Tanh, Heaviside)\n",
        "* Their numerical derivatives\n",
        "\n",
        "Used to understand how neural networks **activate** and **backpropagate**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* Each activation transforms input **z** → output\n",
        "* Derivatives show how gradients flow\n",
        "* Smooth functions (sigmoid/tanh) → smooth gradients\n",
        "* ReLU → piecewise linear\n",
        "* Heaviside → step with no useful gradients\n",
        "\n",
        "The derivative function uses **finite differences** to approximate the true derivative.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* Numerical derivative is unstable near discontinuities (Heaviside, ReLU at 0)\n",
        "* Sigmoid saturates → vanishing gradients\n",
        "* Tanh also saturates\n",
        "* ReLU dies for negative z (grad = 0)\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use when learning or demonstrating:\n",
        "\n",
        "* Activation behavior\n",
        "* Vanishing gradient problem\n",
        "* Why modern networks prefer ReLU family\n",
        "* Backprop intuition\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Mental model**\n",
        "\n",
        "Think of activations as:\n",
        "\n",
        "> **“Gatekeepers that decide how much signal passes to the next layer.”**\n",
        "\n",
        "Derivatives = how much learning signal (gradient) flows backward.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(Not LLM prompting — *code usage guidance*.)\n",
        "You “prompt” the functions by:\n",
        "\n",
        "* Passing a vector `z`\n",
        "* Plotting outputs\n",
        "* Using `derivative(func, z)` to approximate slopes\n",
        "* Tweaking activation choices to see differences\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Alternatives**\n",
        "\n",
        "For activations:\n",
        "\n",
        "* **Leaky ReLU**\n",
        "* **ELU / GELU**\n",
        "* **Swish / Mish**\n",
        "\n",
        "For derivatives:\n",
        "\n",
        "* Analytical derivatives (instead of finite difference)\n",
        "* Autograd / TensorFlow / PyTorch automatic differentiation\n",
        "\n",
        "---\n",
        "\n",
        "# **Code Explanation (Short + Clear)**\n",
        "\n",
        "### **1. Import + activation definitions**\n",
        "\n",
        "```python\n",
        "from scipy.special import expit as sigmoid\n",
        "\n",
        "def relu(z):\n",
        "    return np.maximum(0, z)\n",
        "```\n",
        "\n",
        "* `sigmoid` imported from SciPy\n",
        "* `relu` implemented manually\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Numerical derivative function**\n",
        "\n",
        "```python\n",
        "def derivative(f, z, eps=0.000001):\n",
        "    return (f(z + eps) - f(z - eps))/(2 * eps)\n",
        "```\n",
        "\n",
        "* Central difference formula\n",
        "* Approximates f′(z)\n",
        "* Works for any function `f`\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Prepare input range**\n",
        "\n",
        "```python\n",
        "max_z = 4.5\n",
        "z = np.linspace(-max_z, max_z, 200)\n",
        "```\n",
        "\n",
        "* 200 points from –4.5 to +4.5\n",
        "* Used for smooth plots\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Plot activation functions**\n",
        "\n",
        "```python\n",
        "plt.subplot(121)\n",
        "plt.plot(z, relu(z), \"m-.\", label=\"ReLU\")\n",
        "plt.plot(z, sigmoid(z), \"g--\", label=\"Sigmoid\")\n",
        "plt.plot(z, np.tanh(z), \"b-\", label=\"Tanh\")\n",
        "```\n",
        "\n",
        "Also draws Heaviside step using simple line plots.\n",
        "\n",
        "Purpose: visualize shape.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Plot derivatives**\n",
        "\n",
        "```python\n",
        "plt.subplot(122)\n",
        "plt.plot(z, derivative(sigmoid, z), \"g--\", label=\"Sigmoid\")\n",
        "plt.plot(z, derivative(np.tanh, z), \"b-\", label=\"Tanh\")\n",
        "plt.plot(z, derivative(np.sign, z), \"r-\", label=\"Heaviside\")\n",
        "```\n",
        "\n",
        "* Shows slopes\n",
        "* Critical for understanding gradient flow\n",
        "\n",
        "ReLU derivative is drawn manually using straight lines, because derivative is undefined at 0.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Decoration**\n",
        "\n",
        "Grid, axis limits, legends, labels, titles.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Save and show**\n",
        "\n",
        "```python\n",
        "save_fig(\"activation_functions_plot\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Saves high-quality image + displays it.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can (short):\n",
        "✔ add GELU/Swish to the plot\n",
        "✔ rewrite in TensorFlow or PyTorch\n",
        "✔ summarize “when to use which activation” in 5 lines\n",
        "</details>"
      ],
      "metadata": {
        "id": "-dCd7F-wEln4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tQqJT_PMY38"
      },
      "source": [
        "## Classification MLPs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXBVtaSEMY39"
      },
      "outputs": [],
      "source": [
        "# extra code – this was left as an exercise for the reader\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.1, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
        "\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=[5], max_iter=10_000,\n",
        "                        random_state=42)\n",
        "pipeline = make_pipeline(StandardScaler(), mlp_clf)\n",
        "pipeline.fit(X_train, y_train)\n",
        "accuracy = pipeline.score(X_valid, y_valid)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding - Classification MLPs </b></summary>\n",
        "\n",
        "## ✅**AI Understanding Template (for this MLPClassifier code)**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A small **feed-forward neural network classifier** (MLP) trained on the **Iris dataset** using scikit-learn.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* Standardizes inputs (via **StandardScaler**)\n",
        "* Passes them through **one hidden layer of 5 neurons**\n",
        "* Learns non-linear decision boundaries\n",
        "* Uses backprop + gradient descent to minimize classification loss\n",
        "* Outputs class probabilities for the 3 Iris species\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* Very small network → may underfit complex datasets\n",
        "* Sensitive to scaling (but we use StandardScaler → good)\n",
        "* Doesn’t capture long-range structure (no attention, no convolution)\n",
        "* Not great for: images, text sequences, large tabular sets\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use MLPClassifier when:\n",
        "\n",
        "* You have **small/medium tabular numeric data**\n",
        "* Need a quick **baseline neural network**\n",
        "* Problem is **multi-class classification**\n",
        "* You want simple, fast, shallow neural nets (not deep architectures)\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> **A stack of weighted linear layers + nonlinear activations that bend the space so classes become separable.**\n",
        "\n",
        "It’s the simplest “neural network brain” for classification tasks.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(How to use/train it effectively)\n",
        "\n",
        "* Always **scale your features**\n",
        "* Tune:\n",
        "\n",
        "  * `hidden_layer_sizes`\n",
        "  * `max_iter`\n",
        "  * `learning_rate_init`\n",
        "* Feed numeric tabular features only\n",
        "* For small datasets → increase `max_iter`\n",
        "* For complex patterns → add more layers\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "| Method                     | When better                                  |\n",
        "| -------------------------- | -------------------------------------------- |\n",
        "| **LogisticRegression**     | Data mostly linear                           |\n",
        "| **RandomForestClassifier** | Tabular data with non-linear relations       |\n",
        "| **XGBoost/CatBoost**       | Best performance on structured data          |\n",
        "| **SVM**                    | Small datasets with clear margins            |\n",
        "| **Keras MLP**              | If you want deeper/more flexible neural nets |\n",
        "| **Transformers/CNNs**      | For text/vision problems                     |\n",
        "\n",
        "---\n",
        "\n",
        "# **Code Explanation (Short & Clear)**\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "```\n",
        "\n",
        "✔ Load dataset\n",
        "✔ Import train/test splitting\n",
        "✔ Import a neural network classifier\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "iris = load_iris()\n",
        "```\n",
        "\n",
        "Loads classic 150-sample Iris dataset with 4 numeric features.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.1, random_state=42)\n",
        "```\n",
        "\n",
        "* 10% test data\n",
        "* Remaining 90% kept for training/validation\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.1, random_state=42)\n",
        "```\n",
        "\n",
        "* Splits remaining 90% into:\n",
        "\n",
        "  * 90% training\n",
        "  * 10% validation\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "mlp_clf = MLPClassifier(hidden_layer_sizes=[5], max_iter=10_000,\n",
        "                        random_state=42)\n",
        "```\n",
        "\n",
        "Creates a neural network with:\n",
        "\n",
        "* 1 hidden layer of **5 neurons**\n",
        "* Allow up to **10,000 training iterations**\n",
        "* Same randomness for reproducibility\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "pipeline = make_pipeline(StandardScaler(), mlp_clf)\n",
        "```\n",
        "\n",
        "Builds a pipeline:\n",
        "\n",
        "1. **Standardize features**\n",
        "2. **Feed into neural network**\n",
        "\n",
        "(Scaling is crucial for MLP to converge.)\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "pipeline.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "Trains the neural network on training data.\n",
        "\n",
        "---\n",
        "\n",
        "```python\n",
        "accuracy = pipeline.score(X_valid, y_valid)\n",
        "accuracy\n",
        "```\n",
        "\n",
        "Evaluates accuracy on **validation set** and prints it.\n",
        "\n",
        "Expected accuracy ~ **0.93–1.0** (Iris is simple).\n",
        "</details>"
      ],
      "metadata": {
        "id": "wl2qz5qeNqRA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJVn92r6MY3-"
      },
      "source": [
        "# Implementing MLPs with Keras\n",
        "## Building an Image Classifier Using the Sequential API\n",
        "### Using Keras to load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WrLtU8wMY3-"
      },
      "source": [
        "Let's start by loading the fashion MNIST dataset. Keras has a number of functions to load popular datasets in `tf.keras.datasets`. The dataset is already split for you between a training set (60,000 images) and a test set (10,000 images), but it can be useful to split the training set further to have a validation set. We'll use 55,000 images for training, and 5,000 for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbfH6iSzMY3-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4W0c-fRMY3_"
      },
      "source": [
        "The training set contains 60,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDAkWbnjMY4A"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6zUJmXOMY4A"
      },
      "source": [
        "Each pixel intensity is represented as a byte (0 to 255):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz4OO7WqMY4B"
      },
      "outputs": [],
      "source": [
        "X_train.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npVvJ1EWMY4B"
      },
      "source": [
        "Let's scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPsYmxrAMY4C"
      },
      "outputs": [],
      "source": [
        "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0756zZSMY4I"
      },
      "source": [
        "You can plot an image using Matplotlib's `imshow()` function, with a `'binary'`\n",
        " color map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNRDn3qBMY4J"
      },
      "outputs": [],
      "source": [
        "# extra code\n",
        "\n",
        "plt.imshow(X_train[0], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiqqxkVGMY4K"
      },
      "source": [
        "The labels are the class IDs (represented as uint8), from 0 to 9:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkbQGj56MY4K"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjS6i5v5MY4L"
      },
      "source": [
        "Here are the corresponding class names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6BX1qCKMY4L"
      },
      "outputs": [],
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syy8LVKoMY4M"
      },
      "source": [
        "So the first image in the training set is an ankle boot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejD6FNepMY4M"
      },
      "outputs": [],
      "source": [
        "class_names[y_train[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d090dUUMY4N"
      },
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlxw_FVlMY4N"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 10–10\n",
        "\n",
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]])\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "\n",
        "save_fig(\"fashion_mnist_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding - Implementing MLPs with Keras </b></summary>\n",
        "\tBelow is a **short, crisp AI-style breakdown** followed by a **simple explanation of the code**.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **AI Understanding Template — Fashion-MNIST Loading & Visualization**\n",
        "\n",
        "### **1. What is it?**\n",
        "\n",
        "A machine learning pipeline step that:\n",
        "\n",
        "* Loads the **Fashion-MNIST** dataset\n",
        "* Splits it into train/validation/test\n",
        "* Normalizes pixel values\n",
        "* Visualizes sample images\n",
        "\n",
        "It prepares data for image-classification models (CNN, DNN, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How does it reason?**\n",
        "\n",
        "Not actual reasoning — but **data processing logic**:\n",
        "\n",
        "* Load → Split → Normalize → Visualize\n",
        "* Normalization helps models converge faster\n",
        "* Visualization lets you verify data sanity\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Where does it fail?**\n",
        "\n",
        "* If visualization is not checked → wrong labels or corrupted data.\n",
        "* If images are not normalized → training becomes unstable.\n",
        "* If shapes mismatch → model input errors.\n",
        "* If grayscale (1-channel) is used incorrectly with CNN expecting 3 channels.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. When should I use it?**\n",
        "\n",
        "* Before training **any image classifier**\n",
        "* To sanity-check data distribution\n",
        "* To confirm labels match images\n",
        "* When benchmarking new models on a standard dataset\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What is the mental model?**\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> **“Load clothes images → clean them → convert numbers → preview them → ready for model.”**\n",
        "\n",
        "---\n",
        "\n",
        "### **6. How do I prompt it?**\n",
        "\n",
        "(Here “prompt” = how to *use the code*.)\n",
        "\n",
        "* Ensure dataset loads correctly\n",
        "* Normalize values using `/ 255.`\n",
        "* Use `plt.imshow()` to confirm image quality\n",
        "* Prepare train/valid/test strictly separated\n",
        "\n",
        "---\n",
        "\n",
        "### **7. What are alternatives?**\n",
        "\n",
        "* **CIFAR-10** (color images)\n",
        "* **MNIST** (digits)\n",
        "* **Custom image datasets** via `tf.keras.utils.image_dataset_from_directory`\n",
        "* **Kaggle fashion datasets** for higher complexity\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Code Explanation (Short, Clear)**\n",
        "\n",
        "### **1. Load Dataset**\n",
        "\n",
        "```python\n",
        "import tensorflow as tf\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "```\n",
        "\n",
        "* Downloads Fashion-MNIST\n",
        "* Gives 60,000 training images + 10,000 test images\n",
        "* Each image is 28×28 grayscale\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Create Validation Set**\n",
        "\n",
        "```python\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
        "```\n",
        "\n",
        "* Last **5000 images** → validation\n",
        "* Remaining **55,000** → training\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Check shape & datatype**\n",
        "\n",
        "```python\n",
        "X_train.shape\n",
        "X_train.dtype\n",
        "```\n",
        "\n",
        "* Shape: `(55000, 28, 28)`\n",
        "* dtype: `uint8` (0–255 pixel values)\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Normalize Pixel Values**\n",
        "\n",
        "```python\n",
        "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.\n",
        "```\n",
        "\n",
        "* Converts pixels from **0–255 → 0–1**\n",
        "* Helps neural networks learn smoother\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Display First Image**\n",
        "\n",
        "```python\n",
        "plt.imshow(X_train[0], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "* Shows a grayscale image\n",
        "* `binary` = black/white colormap\n",
        "* Turn off axis for cleaner look\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Get Label Names**\n",
        "\n",
        "```python\n",
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "class_names[y_train[0]]\n",
        "```\n",
        "\n",
        "* Converts numeric label (0–9) → category name\n",
        "* Example: 9 → “Ankle boot”\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Plot Grid of Images**\n",
        "\n",
        "```python\n",
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "```\n",
        "\n",
        "* Creates grid: **4 rows × 10 columns**\n",
        "* Shows 40 sample images\n",
        "\n",
        "---\n",
        "\n",
        "### **Loop to Draw Each Image**\n",
        "\n",
        "```python\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(class_names[y_train[index]])\n",
        "```\n",
        "\n",
        "* Draws each image\n",
        "* Adds class name as title\n",
        "* Uses “nearest” to keep pixels sharp\n",
        "\n",
        "---\n",
        "\n",
        "### **Spacing + Save Figure**\n",
        "\n",
        "```python\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "save_fig(\"fashion_mnist_plot\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "* Adjust spacing\n",
        "* Saves as image file\n",
        "* Displays plot\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also provide:\n",
        "✔ **One-page cheat-sheet** for Fashion-MNIST\n",
        "✔ **Model code** (DNN or CNN)\n",
        "✔ **AI-style understanding for training the model**\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "e_8dYkp5Sw_C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-GJj6jFMY4O"
      },
      "source": [
        "### Creating the model using the Sequential API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjKcJu8rMY4O"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=[28, 28]))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSBKinx1MY4P"
      },
      "outputs": [],
      "source": [
        "# extra code – clear the session to reset the name counters\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1KGHfpmMY4P"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLgBS1khMY4Q"
      },
      "outputs": [],
      "source": [
        "# extra code – another way to display the model's architecture\n",
        "tf.keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYHXjWucMY4R"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLXB1nd5MY4R"
      },
      "outputs": [],
      "source": [
        "hidden1 = model.layers[1]\n",
        "hidden1.name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gq8bgM0KMY4T"
      },
      "outputs": [],
      "source": [
        "model.get_layer('dense') is hidden1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCRXJ0i-MY4T"
      },
      "outputs": [],
      "source": [
        "weights, biases = hidden1.get_weights()\n",
        "weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzQoM4IMMY4U"
      },
      "outputs": [],
      "source": [
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BJZ9wtJMY4V"
      },
      "outputs": [],
      "source": [
        "biases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6TYbC6rMY4W"
      },
      "outputs": [],
      "source": [
        "biases.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEdqMZcxMY4W"
      },
      "source": [
        "### Compiling the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re8HrEhzMY4X"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXT_ZjwVMY4X"
      },
      "source": [
        "This is equivalent to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P4gM2YRMY4X"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell is equivalent to the previous cell\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KTXMQ4VMY4Y"
      },
      "outputs": [],
      "source": [
        "# extra code – shows how to convert class ids to one-hot vectors\n",
        "tf.keras.utils.to_categorical([0, 5, 1, 0], num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRP0t1obMY4Z"
      },
      "source": [
        "Note: it's important to set `num_classes` when the number of classes is greater than the maximum class id in the sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXmpvYLrMY4Z"
      },
      "outputs": [],
      "source": [
        "# extra code – shows how to convert one-hot vectors to class ids\n",
        "np.argmax(\n",
        "    [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "     [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
        "     [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
        "     [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding MLPs with Keras - Creating and compiling </b></summary>\n",
        "\n",
        "Below is a **short, clean, AI-style explanation** *plus* a **code walkthrough** for the given Sequential MNIST model.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **AI Understanding Template — MNIST Sequential Model**\n",
        "\n",
        "### **1. What is it?**\n",
        "\n",
        "A simple **feed-forward neural network (MLP)** for classifying 28×28 images (e.g., Fashion-MNIST / MNIST) into 10 classes.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. How does it reason?**\n",
        "\n",
        "* **Flatten** turns the 2D image into a 1D vector.\n",
        "* **Dense layers (ReLU)** learn non-linear patterns such as shapes/edges.\n",
        "* **Softmax output** converts scores → probabilities.\n",
        "* Training uses **cross-entropy** to push probabilities toward the correct class.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Where does it fail?**\n",
        "\n",
        "* Works poorly on **complex images** (CNNs beat MLPs).\n",
        "* Does not use spatial structure → treats each pixel independently.\n",
        "* Prone to overfitting if network is large or data is small.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. When should I use it?**\n",
        "\n",
        "Use this when:\n",
        "\n",
        "* Data is **simple** (e.g., digit recognition).\n",
        "* You want a **baseline classifier** fast.\n",
        "* You want to teach beginners **how neural nets train**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. What is the mental model?**\n",
        "\n",
        "> **“Convert image → vector → pass through funnels of neurons → classify.”**\n",
        "\n",
        "It’s simply a stack of fully connected layers that gradually learn better representations.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. How do I prompt it?**\n",
        "\n",
        "(How to feed/use the model)\n",
        "\n",
        "* Input must be shape **[batch, 28, 28]**\n",
        "* Labels must be **integers 0–9** (sparse cross entropy).\n",
        "* Compile with **SGD** or **Adam**.\n",
        "* Train with `model.fit(X_train, y_train)`.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. What are the alternatives?**\n",
        "\n",
        "| Alternative             | Why use it?                              |\n",
        "| ----------------------- | ---------------------------------------- |\n",
        "| **CNN (Conv2D)**        | Best for images; uses spatial patterns.  |\n",
        "| **SimpleRNN/LSTM**      | If treating each row/column as sequence. |\n",
        "| **Vision Transformer**  | High-end accuracy on modern image tasks. |\n",
        "| **Logistic Regression** | Tiny baseline for comparison.            |\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 **Code Explanation (Short & Clear)**\n",
        "\n",
        "### **Set seed**\n",
        "\n",
        "```python\n",
        "tf.random.set_seed(42)\n",
        "```\n",
        "\n",
        "Ensures reproducible weight initialization.\n",
        "\n",
        "---\n",
        "\n",
        "## **Model Creation – Method 1**\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=[28, 28]))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "```\n",
        "\n",
        "**Explanation:**\n",
        "\n",
        "* `InputLayer`: expects 28×28 images\n",
        "* `Flatten`: reshape into 784-length vector\n",
        "* Dense-300 → Dense-100 → Dense-10 (classification)\n",
        "\n",
        "---\n",
        "\n",
        "### **Clear session (reset counters)**\n",
        "\n",
        "```python\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Model Creation – Method 2 (cleaner)**\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Model summary**\n",
        "\n",
        "```python\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "Shows layer shapes + parameters.\n",
        "\n",
        "---\n",
        "\n",
        "### **Plot the model**\n",
        "\n",
        "```python\n",
        "tf.keras.utils.plot_model(model, \"my_fashion_mnist_model.png\", show_shapes=True)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Accessing Layers**\n",
        "\n",
        "```python\n",
        "hidden1 = model.layers[1]\n",
        "hidden1.name\n",
        "model.get_layer('dense') is hidden1\n",
        "```\n",
        "\n",
        "* `layers[1]` → first Dense(300) layer\n",
        "* `get_layer()` fetches by name\n",
        "* You can inspect how layers are stored.\n",
        "\n",
        "---\n",
        "\n",
        "### **Weights of a layer**\n",
        "\n",
        "```python\n",
        "weights, biases = hidden1.get_weights()\n",
        "weights\n",
        "```\n",
        "\n",
        "* Shows the big `[784 x 300]` weight matrix\n",
        "* And bias vector of length 300\n",
        "\n",
        "---\n",
        "\n",
        "## **Compile Model**\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "Why sparse?\n",
        "\n",
        "* Labels are integers (0–9), not one-hot vectors.\n",
        "\n",
        "Equivalent version:\n",
        "\n",
        "```python\n",
        "model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=[tf.keras.metrics.sparse_categorical_accuracy])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **One-hot utilities**\n",
        "\n",
        "Convert ids → one-hot:\n",
        "\n",
        "```python\n",
        "tf.keras.utils.to_categorical([0, 5, 1, 0], num_classes=10)\n",
        "```\n",
        "\n",
        "Convert one-hot → ids:\n",
        "\n",
        "```python\n",
        "np.argmax([...], axis=1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also create:\n",
        "✔ A 20-word ultra-short version\n",
        "✔ A diagram showing how data flows through each layer\n",
        "✔ A version comparing this model to a CNN in table form\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "Y01NMKb3UcFT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2C_bjWiYMY4a"
      },
      "source": [
        "### Training and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlqdaKwEMY4a"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHWmEW0YMY4a"
      },
      "outputs": [],
      "source": [
        "history.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVBGNQmzMY4b"
      },
      "outputs": [],
      "source": [
        "print(history.epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S64uiMTMY4b"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(\n",
        "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1], grid=True, xlabel=\"Epoch\",\n",
        "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
        "plt.legend(loc=\"lower left\")  # extra code\n",
        "save_fig(\"keras_learning_curves_plot\")  # extra code\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6cxiRiFMY4c"
      },
      "outputs": [],
      "source": [
        "# extra code – shows how to shift the training curve by -1/2 epoch\n",
        "plt.figure(figsize=(8, 5))\n",
        "for key, style in zip(history.history, [\"r--\", \"r--.\", \"b-\", \"b-*\"]):\n",
        "    epochs = np.array(history.epoch) + (0 if key.startswith(\"val_\") else -0.5)\n",
        "    plt.plot(epochs, history.history[key], style, label=key)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.axis([-0.5, 29, 0., 1])\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWUxy62NMY4c"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOyWFtvcMY4d"
      },
      "source": [
        "### Using the model to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqgDjVLBMY4d"
      },
      "outputs": [],
      "source": [
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYxvTLolMY4d"
      },
      "outputs": [],
      "source": [
        "y_pred = y_proba.argmax(axis=-1)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1pJfoj6MY4e"
      },
      "outputs": [],
      "source": [
        "np.array(class_names)[y_pred]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4bthq2cMY4e"
      },
      "outputs": [],
      "source": [
        "y_new = y_test[:3]\n",
        "y_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jTL2bPRMY4f"
      },
      "outputs": [],
      "source": [
        "# extra code – this cell generates and saves Figure 10–12\n",
        "plt.figure(figsize=(7.2, 2.4))\n",
        "for index, image in enumerate(X_new):\n",
        "    plt.subplot(1, 3, index + 1)\n",
        "    plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    plt.title(class_names[y_test[index]])\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "save_fig('fashion_mnist_images_plot', tight_layout=False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding - Evaluation & Predictions </b></summary>\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **AI Understanding Template — Keras Training/Evaluation Code**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A standard **model training → tracking → evaluation → prediction → visualization** pipeline using Keras + Matplotlib.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* During `fit()`, the model updates weights using backprop (per epoch).\n",
        "* Stores all metrics in `history.history`.\n",
        "* Plots curves to help “reason” about learning trends (overfit/underfit).\n",
        "* Uses `model.predict()` to produce probabilities → picks highest class.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* If epochs too high → **overfitting**.\n",
        "* If data not normalized → **unstable loss curves**.\n",
        "* If classes imbalanced → **misleading accuracy**.\n",
        "* If model is too small/large → **underfitting/overfitting**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use this pipeline when you want:\n",
        "\n",
        "* Quick training + validation monitoring.\n",
        "* To diagnose model behavior via loss/accuracy curves.\n",
        "* Simple prediction flow (e.g., MNIST, tabular, binary classifiers).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> **“A training diary that logs each epoch’s progress and lets you visualize learning behavior.”**\n",
        "\n",
        "`fit()` = training engine\n",
        "`history.history` = logbook\n",
        "Plots = health check of your neural network\n",
        "`evaluate()` = exam\n",
        "`predict()` = final output\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(How to use the API correctly)\n",
        "\n",
        "* Provide training + validation data.\n",
        "* Choose number of epochs (20–50 early).\n",
        "* Read `history.history['loss']` etc. for diagnostics.\n",
        "* Use `plot()` to see underfit/overfit.\n",
        "* Use `predict()` → `argmax` to convert probabilities → classes.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "* **TensorBoard** → real-time training dashboard\n",
        "* **Scikit-learn fit/score APIs**\n",
        "* **PyTorch Lightning Trainer**\n",
        "* **Weights & Biases / MLflow** → experiment tracking\n",
        "* **FastAI** → automated training cycles\n",
        "\n",
        "---\n",
        "\n",
        "##  **CODE EXPLANATION (Short + Clear)**\n",
        "\n",
        "---\n",
        "\n",
        "## **Training the model**\n",
        "\n",
        "```python\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "```\n",
        "\n",
        "* Trains for **30 epochs**\n",
        "* Logs **loss + metrics** for train/val\n",
        "* Returns `history` object containing all logs.\n",
        "\n",
        "---\n",
        "\n",
        "## **Inspecting training metadata**\n",
        "\n",
        "```python\n",
        "history.params         # training parameters (epochs, samples)\n",
        "print(history.epoch)   # list of epochs [0..29]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Plot the learning curves**\n",
        "\n",
        "```python\n",
        "pd.DataFrame(history.history).plot(\n",
        "    figsize=(8, 5), xlim=[0, 29], ylim=[0, 1],\n",
        "    grid=True, xlabel=\"Epoch\",\n",
        "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Explanation:\n",
        "\n",
        "* Converts history to DataFrame.\n",
        "* Plots **training loss**, **validation loss**, **training metric**, **validation metric**.\n",
        "* Styles define line shapes (red dashed, blue solid, etc.).\n",
        "* Helps detect **overfitting** (val curves diverging).\n",
        "\n",
        "---\n",
        "\n",
        "## **Shift training curve by -0.5 epochs (optional trick)**\n",
        "\n",
        "Useful to visually align train vs validation curves.\n",
        "\n",
        "```python\n",
        "for key, style in zip(history.history, [\"r--\", \"r--.\", \"b-\", \"b-*\"]):\n",
        "    epochs = np.array(history.epoch) + (0 if key.startswith(\"val_\") else -0.5)\n",
        "    plt.plot(epochs, history.history[key], style, label=key)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Evaluate on test set**\n",
        "\n",
        "```python\n",
        "model.evaluate(X_test, y_test)\n",
        "```\n",
        "\n",
        "Gives final loss + metrics on unseen data.\n",
        "\n",
        "---\n",
        "\n",
        "## **Make predictions**\n",
        "\n",
        "```python\n",
        "X_new = X_test[:3]\n",
        "y_proba = model.predict(X_new)\n",
        "y_proba.round(2)\n",
        "```\n",
        "\n",
        "* Returns probability distribution per class.\n",
        "* Rounded to 2 decimals.\n",
        "\n",
        "---\n",
        "\n",
        "## **Convert probabilities to class labels**\n",
        "\n",
        "```python\n",
        "y_pred = y_proba.argmax(axis=-1)\n",
        "np.array(class_names)[y_pred]\n",
        "```\n",
        "\n",
        "`argmax` picks the class with highest probability.\n",
        "\n",
        "---\n",
        "\n",
        "## **Ground truth comparison**\n",
        "\n",
        "```python\n",
        "y_test[:3]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Plot images + true labels**\n",
        "\n",
        "```python\n",
        "plt.imshow(image, cmap=\"binary\", interpolation=\"nearest\")\n",
        "plt.title(class_names[y_test[index]])\n",
        "```\n",
        "\n",
        "Shows the test images with their *actual* class names.\n",
        "\n",
        "---\n",
        "</details>"
      ],
      "metadata": {
        "id": "bfbKkW13WZmZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSdO_0SxMY4f"
      },
      "source": [
        "## Building a Regression MLP Using the Sequential API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wASyQj_8MY4f"
      },
      "source": [
        "Let's load, split and scale the California housing dataset (the original one, not the modified one as in chapter 2):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ9ltHrUMY4g"
      },
      "outputs": [],
      "source": [
        "# extra code – load and split the California housing dataset, like earlier\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    housing.data, housing.target, random_state=42)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train_full, y_train_full, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xL4unKiMY4g"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
        "model = tf.keras.Sequential([\n",
        "    norm_layer,\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(50, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
        "norm_layer.adapt(X_train)\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
        "X_new = X_test[:3]\n",
        "y_pred = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yu-OBeK6MY4h"
      },
      "outputs": [],
      "source": [
        "rmse_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuzGuKHqMY4h"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRhdxCQlMY4i"
      },
      "source": [
        "## Building Complex Models Using the Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WdwtFz3MY4i"
      },
      "source": [
        "Not all neural network models are simply sequential. Some may have complex topologies. Some may have multiple inputs and/or multiple outputs. For example, a Wide & Deep neural network (see [paper](https://ai.google/research/pubs/pub45413)) connects all or part of the inputs directly to the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXr4ZSIxMY4i"
      },
      "outputs": [],
      "source": [
        "# extra code – reset the name counters and make the code reproducible\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzmTKdBxMY4i"
      },
      "outputs": [],
      "source": [
        "normalization_layer = tf.keras.layers.Normalization()\n",
        "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
        "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
        "concat_layer = tf.keras.layers.Concatenate()\n",
        "output_layer = tf.keras.layers.Dense(1)\n",
        "\n",
        "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
        "normalized = normalization_layer(input_)\n",
        "hidden1 = hidden_layer1(normalized)\n",
        "hidden2 = hidden_layer2(hidden1)\n",
        "concat = concat_layer([normalized, hidden2])\n",
        "output = output_layer(concat)\n",
        "\n",
        "model = tf.keras.Model(inputs=[input_], outputs=[output])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRtvW2K_MY4j"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "figEecafMY4j"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
        "normalization_layer.adapt(X_train)\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "mse_test = model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbJwodTPMY4k"
      },
      "source": [
        "What if you want to send different subsets of input features through the wide or deep paths? We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). Note that 3 features will go through both (features 2, 3 and 4)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzXSINnkMY4k"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIU3CM-wMY4k"
      },
      "outputs": [],
      "source": [
        "input_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\n",
        "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
        "norm_layer_wide = tf.keras.layers.Normalization()\n",
        "norm_layer_deep = tf.keras.layers.Normalization()\n",
        "norm_wide = norm_layer_wide(input_wide)\n",
        "norm_deep = norm_layer_deep(input_deep)\n",
        "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
        "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
        "output = tf.keras.layers.Dense(1)(concat)\n",
        "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IUgqDA59MY4l"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
        "\n",
        "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
        "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
        "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
        "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]\n",
        "\n",
        "norm_layer_wide.adapt(X_train_wide)\n",
        "norm_layer_deep.adapt(X_train_deep)\n",
        "history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,\n",
        "                    validation_data=((X_valid_wide, X_valid_deep), y_valid))\n",
        "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
        "y_pred = model.predict((X_new_wide, X_new_deep))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxUWTJ3JMY4l"
      },
      "source": [
        "Adding an auxiliary output for regularization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtrEP893MY4l"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_YGn1HWMY4m"
      },
      "outputs": [],
      "source": [
        "input_wide = tf.keras.layers.Input(shape=[5])  # features 0 to 4\n",
        "input_deep = tf.keras.layers.Input(shape=[6])  # features 2 to 7\n",
        "norm_layer_wide = tf.keras.layers.Normalization()\n",
        "norm_layer_deep = tf.keras.layers.Normalization()\n",
        "norm_wide = norm_layer_wide(input_wide)\n",
        "norm_deep = norm_layer_deep(input_deep)\n",
        "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
        "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
        "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
        "output = tf.keras.layers.Dense(1)(concat)\n",
        "aux_output = tf.keras.layers.Dense(1)(hidden2)\n",
        "model = tf.keras.Model(inputs=[input_wide, input_deep],\n",
        "                       outputs=[output, aux_output])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzCJaJd8MY4m"
      },
      "source": [
        "**Warning**: in recent versions, Keras requires one metric per output, so I replaced `metrics=[\"RootMeanSquaredError\"]` with `metrics=[\"RootMeanSquaredError\", \"RootMeanSquaredError\"]` in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFcoOv1fMY4m"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss=(\"mse\", \"mse\"), loss_weights=(0.9, 0.1), optimizer=optimizer,\n",
        "              metrics=[\"RootMeanSquaredError\", \"RootMeanSquaredError\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ou0w6cg9MY4n"
      },
      "outputs": [],
      "source": [
        "norm_layer_wide.adapt(X_train_wide)\n",
        "norm_layer_deep.adapt(X_train_deep)\n",
        "history = model.fit(\n",
        "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=20,\n",
        "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QexUNAzvMY4n"
      },
      "source": [
        "**Warning**: in recent TF version, `evaluate()` also returns the main metric and the aux metric. To ensure the code works in both old and new versions, we only look at the first 3 elements of `eval_results` (i.e., just the losses):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BYGTEUB7MY4n"
      },
      "outputs": [],
      "source": [
        "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
        "weighted_sum_of_losses, main_loss, aux_loss = eval_results[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST0FMzeXMY4o"
      },
      "outputs": [],
      "source": [
        "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNfqf6UKMY4o"
      },
      "outputs": [],
      "source": [
        "y_pred_tuple = model.predict((X_new_wide, X_new_deep))\n",
        "y_pred = dict(zip(model.output_names, y_pred_tuple))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Understanding - Regression & Complex Model </b></summary>\n",
        "\n",
        "Below is a **short, crisp AI-style breakdown** followed by a **clean explanation of each code block**.\n",
        "\n",
        "---\n",
        "\n",
        "# ✅ **AI Understanding Template – Applied to This Code**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A set of TensorFlow models for **tabular regression** (California Housing), progressing from:\n",
        "\n",
        "1. **Simple DNN**\n",
        "2. **Functional API DNN**\n",
        "3. **Wide & Deep model**\n",
        "4. **Wide & Deep with auxiliary outputs (multi-task style)**\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* **Normalization** helps all features to same scale.\n",
        "* **Deep layers** learn non-linear patterns (interactions).\n",
        "* **Wide branch** learns memorization patterns (direct linear effects).\n",
        "* **Concatenation** merges shallow + deep knowledge.\n",
        "* **Auxiliary outputs** stabilize training by forcing hidden layers to learn strong representations.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* Small datasets → overfitting.\n",
        "* Highly categorical data → better with embeddings / trees.\n",
        "* Highly linear tasks → simpler linear models are enough.\n",
        "* Poor feature splitting between wide & deep → suboptimal.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use these models for:\n",
        "\n",
        "* **Tabular data regression**\n",
        "* **Mixed linear + non-linear patterns**\n",
        "* **Real estate, pricing, risk scoring, health tabular datasets**\n",
        "* **When feature engineering matters**\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think of it like:\n",
        "\n",
        "> **“A neural network that listens to two sources: simple rules (wide) + complex patterns (deep) and blends both.”**\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?** *(How to use it)*\n",
        "\n",
        "* Feed **X_train**, **y_train** cleanly.\n",
        "* Always run `.adapt()` before training.\n",
        "* For wide+deep → split features logically.\n",
        "* For predictions → pass inputs as tuples:\n",
        "  `model.predict((wide, deep))`\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "* **XGBoost / CatBoost / LightGBM** → best for tabular data.\n",
        "* **Linear Regression / ElasticNet** → for purely linear tasks.\n",
        "* **Random Forest / Extra Trees** → quick baselines.\n",
        "* **TabTransformer** → deep learning for categorical-heavy data.\n",
        "\n",
        "---\n",
        "\n",
        "## ----------------------------------------------------------\n",
        "\n",
        "# ✅ **Code Explanation (Short + Clear)**\n",
        "\n",
        "## ----------------------------------------------------------\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **1) Basic DNN Regression (Sequential model)**\n",
        "\n",
        "### **Load + split data**\n",
        "\n",
        "```python\n",
        "housing = fetch_california_housing()\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(...)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(...)\n",
        "```\n",
        "\n",
        "→ Classic 70/15/15 splitting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Normalization + DNN**\n",
        "\n",
        "```python\n",
        "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
        "model = tf.keras.Sequential([\n",
        "    norm_layer,\n",
        "    Dense(50, relu),\n",
        "    Dense(50, relu),\n",
        "    Dense(50, relu),\n",
        "    Dense(1)\n",
        "])\n",
        "```\n",
        "\n",
        "* First layer scales all features.\n",
        "* Three hidden layers learn non-linear interactions.\n",
        "\n",
        "---\n",
        "\n",
        "### **Compile + Train**\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"mse\", optimizer=Adam(1e-3), metrics=[\"RMSE\"])\n",
        "norm_layer.adapt(X_train)\n",
        "history = model.fit(...)\n",
        "```\n",
        "\n",
        "* Adam optimizer\n",
        "* RMSE metric\n",
        "* `adapt()` learns mean & variance\n",
        "\n",
        "---\n",
        "\n",
        "### **Evaluate + Predict**\n",
        "\n",
        "```python\n",
        "model.evaluate(X_test, y_test)\n",
        "y_pred = model.predict(X_new)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **2) Functional API Model**\n",
        "\n",
        "Functional API gives more control than Sequential.\n",
        "\n",
        "### **Define layers explicitly**\n",
        "\n",
        "```python\n",
        "input_ = Input(shape=X_train.shape[1:])\n",
        "normalized = normalization_layer(input_)\n",
        "hidden1 = Dense(30, relu)(normalized)\n",
        "hidden2 = Dense(30, relu)(hidden1)\n",
        "concat = Concatenate()([normalized, hidden2])\n",
        "output = Dense(1)(concat)\n",
        "```\n",
        "\n",
        "* Input flows through two hidden layers.\n",
        "* The normalized input is concatenated back → forms a **wide + deep hybrid**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Build, compile & train**\n",
        "\n",
        "```python\n",
        "model = Model(inputs=[input_], outputs=[output])\n",
        "normalization_layer.adapt(X_train)\n",
        "model.fit(...)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔹 **3) Wide + Deep Model (Manually Split Features)**\n",
        "\n",
        "### **Split features manually**\n",
        "\n",
        "```python\n",
        "X_train_wide = X_train[:, :5]   # simple features\n",
        "X_train_deep = X_train[:, 2:]   # richer feature set\n",
        "```\n",
        "\n",
        "Wide: first 5\n",
        "Deep: last 6 (with overlap—OK in W&D)\n",
        "\n",
        "---\n",
        "\n",
        "### **Inputs + normalization**\n",
        "\n",
        "```python\n",
        "input_wide = Input(shape=[5])\n",
        "input_deep = Input(shape=[6])\n",
        "norm_layer_wide = Normalization()\n",
        "norm_layer_deep = Normalization()\n",
        "norm_wide = norm_layer_wide(input_wide)\n",
        "norm_deep = norm_layer_deep(input_deep)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Deep branch**\n",
        "\n",
        "```python\n",
        "hidden1 = Dense(30, relu)(norm_deep)\n",
        "hidden2 = Dense(30, relu)(hidden1)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Concatenate wide + deep**\n",
        "\n",
        "```python\n",
        "concat = concatenate([norm_wide, hidden2])\n",
        "output = Dense(1)(concat)\n",
        "model = Model(inputs=[input_wide, input_deep], outputs=[output])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Train**\n",
        "\n",
        "```python\n",
        "model.compile(...)\n",
        "norm_layer_wide.adapt(...)\n",
        "norm_layer_deep.adapt(...)\n",
        "model.fit((X_train_wide, X_train_deep), y_train, ...)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Evaluation**\n",
        "\n",
        "```python\n",
        "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Prediction**\n",
        "\n",
        "```python\n",
        "y_pred = model.predict((X_new_wide, X_new_deep))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 🔹 **4) Wide + Deep With Auxiliary Outputs**\n",
        "\n",
        "*(Your last lines refer to a model with two outputs.)*\n",
        "\n",
        "### **Evaluate**\n",
        "\n",
        "```python\n",
        "eval_results = model.evaluate(...)\n",
        "weighted_sum_of_losses, main_loss, aux_loss = eval_results[:3]\n",
        "```\n",
        "\n",
        "* Weighted losses reflect multi-task training.\n",
        "\n",
        "### **Predict**\n",
        "\n",
        "```python\n",
        "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\n",
        "```\n",
        "\n",
        "### **Convert tuple → dictionary**\n",
        "\n",
        "```python\n",
        "y_pred = dict(zip(model.output_names, y_pred_tuple))\n",
        "```\n",
        "\n",
        "* Useful for named outputs.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "GQTUF1ZKX4a1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM30o2VmMY4p"
      },
      "source": [
        "## Using the Subclassing API to Build Dynamic Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9js6J7IkMY4p"
      },
      "outputs": [],
      "source": [
        "class WideAndDeepModel(tf.keras.Model):\n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)  # needed to support naming the model\n",
        "        self.norm_layer_wide = tf.keras.layers.Normalization()\n",
        "        self.norm_layer_deep = tf.keras.layers.Normalization()\n",
        "        self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n",
        "        self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n",
        "        self.main_output = tf.keras.layers.Dense(1)\n",
        "        self.aux_output = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_wide, input_deep = inputs\n",
        "        norm_wide = self.norm_layer_wide(input_wide)\n",
        "        norm_deep = self.norm_layer_deep(input_deep)\n",
        "        hidden1 = self.hidden1(norm_deep)\n",
        "        hidden2 = self.hidden2(hidden1)\n",
        "        concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
        "        output = self.main_output(concat)\n",
        "        aux_output = self.aux_output(hidden2)\n",
        "        return output, aux_output\n",
        "\n",
        "tf.random.set_seed(42)  # extra code – just for reproducibility\n",
        "model = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkizrNEWMY4p"
      },
      "source": [
        "**Warning**: as explained above, Keras now requires one loss and one metric per output, so I replaced `loss=\"mse\"` with `loss=[\"mse\", \"mse\"]` and I also replaced `metrics=[\"RootMeanSquaredError\"]` with `metrics=[\"RootMeanSquaredError\", \"RootMeanSquaredError\"]` in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUJq11P5MY4q"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=optimizer,\n",
        "              metrics=[\"RootMeanSquaredError\", \"RootMeanSquaredError\"])\n",
        "model.norm_layer_wide.adapt(X_train_wide)\n",
        "model.norm_layer_deep.adapt(X_train_deep)\n",
        "history = model.fit(\n",
        "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\n",
        "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)))\n",
        "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
        "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ **AI – Quick Answers**\n",
        "\n",
        "### **1. What is it?**\n",
        "\n",
        "AI is software that learns patterns from data and uses them to make predictions, decisions, or generate outputs.\n",
        "\n",
        "### **2. How does it reason?**\n",
        "\n",
        "It doesn’t “think” like humans.\n",
        "It **matches patterns**, **optimizes probabilities**, and **predicts the most likely output** based on training data.\n",
        "\n",
        "### **3. Where does it fail?**\n",
        "\n",
        "* Unseen edge cases\n",
        "* Ambiguous/incomplete prompts\n",
        "* Wrong or biased training data\n",
        "* Logical reasoning that requires real-world grounding\n",
        "* Multi-step planning without guidance\n",
        "\n",
        "### **4. When should I use it?**\n",
        "\n",
        "Use AI when:\n",
        "\n",
        "* Rules cannot be hard-coded\n",
        "* Data is large\n",
        "* Problem is pattern-driven (image, text, time-series, recommendations)\n",
        "\n",
        "Not ideal for:\n",
        "\n",
        "* Exact logic (bank ledger)\n",
        "* Safety-critical tasks without supervision\n",
        "\n",
        "### **5. What is the mental model?**\n",
        "\n",
        "Think of AI as:\n",
        "👉 **Pattern autocomplete**\n",
        "Whatever you give, it tries to autocomplete based on learned examples.\n",
        "\n",
        "### **6. How do I prompt it?**\n",
        "\n",
        "* Be explicit (“Do X, explain Y, give Z format”)\n",
        "* Provide context and constraints\n",
        "* Break tasks into steps\n",
        "* Give examples\n",
        "* State output format\n",
        "\n",
        "### **7. Alternatives?**\n",
        "\n",
        "* Rule-based systems\n",
        "* Statistical models (regression, ARIMA)\n",
        "* Optimization algorithms\n",
        "* Search algorithms (A*, DFS, heuristic search)\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Code Explanation – `WideAndDeepModel`**\n",
        "\n",
        "This is a **Wide & Deep neural network** combining:\n",
        "✔️ **Wide part** → memorization\n",
        "✔️ **Deep part** → generalization\n",
        "\n",
        "Google uses it in recommender systems.\n",
        "\n",
        "---\n",
        "\n",
        "## **Class Definition**\n",
        "\n",
        "### **`__init__()`**\n",
        "\n",
        "```python\n",
        "class WideAndDeepModel(tf.keras.Model):\n",
        "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "```\n",
        "\n",
        "* Inherits from `tf.keras.Model`\n",
        "* Allows custom naming\n",
        "\n",
        "### **Layers Created**\n",
        "\n",
        "#### **Normalization layers**\n",
        "\n",
        "```python\n",
        "self.norm_layer_wide = tf.keras.layers.Normalization()\n",
        "self.norm_layer_deep = tf.keras.layers.Normalization()\n",
        "```\n",
        "\n",
        "* Each input branch (wide / deep) is normalized separately.\n",
        "\n",
        "#### **Deep network layers**\n",
        "\n",
        "```python\n",
        "self.hidden1 = tf.keras.layers.Dense(units, activation=activation)\n",
        "self.hidden2 = tf.keras.layers.Dense(units, activation=activation)\n",
        "```\n",
        "\n",
        "* Two dense layers for the *deep* pathway (non-linear learning)\n",
        "\n",
        "#### **Outputs**\n",
        "\n",
        "```python\n",
        "self.main_output = tf.keras.layers.Dense(1)\n",
        "self.aux_output = tf.keras.layers.Dense(1)\n",
        "```\n",
        "\n",
        "* Main output: used for final prediction\n",
        "* Aux output: regularizes the model (helps early layers)\n",
        "\n",
        "---\n",
        "\n",
        "## **`call()` method**\n",
        "\n",
        "This defines the forward pass.\n",
        "\n",
        "```python\n",
        "input_wide, input_deep = inputs\n",
        "```\n",
        "\n",
        "Two inputs:\n",
        "\n",
        "* Wide features\n",
        "* Deep features\n",
        "\n",
        "### **1. Normalize**\n",
        "\n",
        "```python\n",
        "norm_wide = self.norm_layer_wide(input_wide)\n",
        "norm_deep = self.norm_layer_deep(input_deep)\n",
        "```\n",
        "\n",
        "### **2. Deep network**\n",
        "\n",
        "```python\n",
        "hidden1 = self.hidden1(norm_deep)\n",
        "hidden2 = self.hidden2(hidden1)\n",
        "```\n",
        "\n",
        "### **3. Concatenate deep output + wide input**\n",
        "\n",
        "```python\n",
        "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
        "```\n",
        "\n",
        "This merges memorization + generalization.\n",
        "\n",
        "### **4. Two outputs**\n",
        "\n",
        "```python\n",
        "output = self.main_output(concat)\n",
        "aux_output = self.aux_output(hidden2)\n",
        "```\n",
        "\n",
        "* `main_output` → final task\n",
        "* `aux_output` → helps deep branch learn better\n",
        "\n",
        "Return both:\n",
        "\n",
        "```python\n",
        "return output, aux_output\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Training Explanation**\n",
        "\n",
        "### **Optimizer**\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "```\n",
        "\n",
        "### **Compile with two losses**\n",
        "\n",
        "```python\n",
        "model.compile(\n",
        "    loss=[\"mse\", \"mse\"],\n",
        "    loss_weights=[0.9, 0.1],\n",
        "    optimizer=optimizer,\n",
        "    metrics=[\"RootMeanSquaredError\", \"RootMeanSquaredError\"]\n",
        ")\n",
        "```\n",
        "\n",
        "Why two losses?\n",
        "\n",
        "* `0.9` → main output (important)\n",
        "* `0.1` → aux output (regularization)\n",
        "\n",
        "---\n",
        "\n",
        "### **Adapt normalization layers**\n",
        "\n",
        "```python\n",
        "model.norm_layer_wide.adapt(X_train_wide)\n",
        "model.norm_layer_deep.adapt(X_train_deep)\n",
        "```\n",
        "\n",
        "Learns the mean & variance for scaling.\n",
        "\n",
        "---\n",
        "\n",
        "### **Fit the model**\n",
        "\n",
        "```python\n",
        "history = model.fit((X_train_wide, X_train_deep), (y_train, y_train), epochs=10)\n",
        "```\n",
        "\n",
        "Inputs:\n",
        "\n",
        "* `(X_train_wide, X_train_deep)`\n",
        "\n",
        "Outputs:\n",
        "\n",
        "* `(y_train, y_train)` → main + aux get the same target\n",
        "\n",
        "---\n",
        "\n",
        "### **Evaluate**\n",
        "\n",
        "```python\n",
        "eval_results = model.evaluate((X_test_wide, X_test_deep), (y_test, y_test))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Predict**\n",
        "\n",
        "```python\n",
        "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))\n",
        "```\n",
        "\n",
        "* `y_pred_main` → final prediction\n",
        "* `y_pred_aux` → not used in real-world inference, but available\n"
      ],
      "metadata": {
        "id": "WBC8I0kkSOCo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC3JeZ1QMY4q"
      },
      "source": [
        "## Saving and Restoring a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5RIeTVIMY4r"
      },
      "source": [
        "**Warning**: Keras now recommends using the `.keras` format to save models, and the `h5` format for weights. Therefore I have updated the code in this section to first show what you need to change if you still want to use TensorFlow's `SavedModel` format, and then how you can use the recommended formats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p67kZgb6MY4r"
      },
      "outputs": [],
      "source": [
        "# extra code – delete the directory, in case it already exists\n",
        "\n",
        "import shutil\n",
        "\n",
        "shutil.rmtree(\"my_keras_model\", ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FsYzoc9MY4r"
      },
      "source": [
        "**Warning**: Keras's `model.save()` method no longer supports TensorFlow's `SavedModel` format. However, you can still export models to the `SavedModel` format using `model.export()` like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko9Pb1w-MY4s"
      },
      "outputs": [],
      "source": [
        "model.export(\"my_keras_model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_aHqQriVMY4s"
      },
      "outputs": [],
      "source": [
        "# extra code – show the contents of the my_keras_model/ directory\n",
        "for path in sorted(Path(\"my_keras_model\").glob(\"**/*\")):\n",
        "    print(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpMsfyHPMY4t"
      },
      "source": [
        "**Warning**: In Keras 3, it is no longer possible to load a TensorFlow `SavedModel` as a Keras model. However, you can load a `SavedModel` as a `tf.keras.layers.TFSMLayer` layer, but be aware that this layer can only be used for inference: no training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSL17CImMY4t"
      },
      "outputs": [],
      "source": [
        "tfsm_layer = tf.keras.layers.TFSMLayer(\"my_keras_model\")\n",
        "y_pred_main, y_pred_aux = tfsm_layer((X_new_wide, X_new_deep))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MTxeErXMY4t"
      },
      "source": [
        "**Warning**: Keras now requires the saved weights to have the `.weights.h5` extension. There are no longer saved using the `SavedModel` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inViQrSrMY4u"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"my_weights.weights.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BXMOLYbMY4u"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"my_weights.weights.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMKKtezNMY4u"
      },
      "source": [
        "To save a model using the `.keras` format, simply use `model.save()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHKS9E23MY4u"
      },
      "outputs": [],
      "source": [
        "model.save(\"my_model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-7qSHLBMY4v"
      },
      "source": [
        "To load a `.keras` model, use the `tf.keras.models.load_model()` function. If the model uses any custom object, you must pass them to the function via the `custom_objects` argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35DobRStMY4v"
      },
      "outputs": [],
      "source": [
        "loaded_model = tf.keras.models.load_model(\n",
        "    \"my_model.keras\",\n",
        "    custom_objects={\"WideAndDeepModel\": WideAndDeepModel}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ **AI — Quick Answers**\n",
        "\n",
        "### **1) What is it?**\n",
        "\n",
        "A system that learns patterns from data and makes predictions or generates outputs without explicit rules.\n",
        "\n",
        "### **2) How does it reason?**\n",
        "\n",
        "By mapping inputs → outputs using learned internal weights; not true “thinking.”\n",
        "Reasoning = pattern completion + probability.\n",
        "\n",
        "### **3) Where does it fail?**\n",
        "\n",
        "* Out-of-distribution data\n",
        "* Ambiguous instructions\n",
        "* Small or biased training data\n",
        "* Tasks requiring common sense or real-world knowledge\n",
        "\n",
        "### **4) When should I use it?**\n",
        "\n",
        "* When rules are unclear\n",
        "* When patterns are complex\n",
        "* When data is large\n",
        "* When automation requires prediction, summarization, classification, generation\n",
        "\n",
        "### **5) What is the mental model?**\n",
        "\n",
        "Think of AI as:\n",
        "**“A statistical function that predicts the most likely next step based on training.”**\n",
        "Not a calculator. Not logic-first. Pattern-first.\n",
        "\n",
        "### **6) How do I prompt it?**\n",
        "\n",
        "Use **CLEAR** instructions:\n",
        "\n",
        "* **Role:** “Act as an expert…”\n",
        "* **Task:** “Do X clearly…”\n",
        "* **Constraints:** “Short. Bullet points.”\n",
        "* **Context:** “Here is the data…”\n",
        "* **Output format:** “Give JSON, table, steps…”\n",
        "\n",
        "### **7) What are alternatives?**\n",
        "\n",
        "* Rule-based systems\n",
        "* Classical ML (trees, SVM, regressions)\n",
        "* Optimization algorithms\n",
        "* Statistical modeling\n",
        "* Simple scripts if rules are fixed\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Code Explanation (Short & Clear)**\n",
        "\n",
        "### **1. Delete old model directory**\n",
        "\n",
        "```python\n",
        "import shutil\n",
        "shutil.rmtree(\"my_keras_model\", ignore_errors=True)\n",
        "```\n",
        "\n",
        "Deletes the folder **if it exists**, so you export fresh.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Export the model in TF SavedModel format**\n",
        "\n",
        "```python\n",
        "model.export(\"my_keras_model\")\n",
        "```\n",
        "\n",
        "Creates a TF-SavedModel directory containing:\n",
        "\n",
        "* model graph\n",
        "* variables\n",
        "* assets\n",
        "* signatures\n",
        "\n",
        "---\n",
        "\n",
        "### **3. List exported files**\n",
        "\n",
        "```python\n",
        "for path in sorted(Path(\"my_keras_model\").glob(\"**/*\")):\n",
        "    print(path)\n",
        "```\n",
        "\n",
        "Shows the structure.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Load exported SavedModel into a TFSMLayer**\n",
        "\n",
        "```python\n",
        "tfsm_layer = tf.keras.layers.TFSMLayer(\"my_keras_model\")\n",
        "y_pred_main, y_pred_aux = tfsm_layer((X_new_wide, X_new_deep))\n",
        "```\n",
        "\n",
        "* Wraps the SavedModel as a layer\n",
        "* Allows predictions inside another model\n",
        "* Useful for serving pipelines, modular models\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Save weights only**\n",
        "\n",
        "```python\n",
        "model.save_weights(\"my_weights.weights.h5\")\n",
        "```\n",
        "\n",
        "Stores **only the layer weights**, no architecture.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Load weights**\n",
        "\n",
        "```python\n",
        "model.load_weights(\"my_weights.weights.h5\")\n",
        "```\n",
        "\n",
        "Requires the **same model architecture** to already exist in code.\n",
        "\n",
        "---\n",
        "\n",
        "### **7. Save full model (.keras format)**\n",
        "\n",
        "```python\n",
        "model.save(\"my_model.keras\")\n",
        "```\n",
        "\n",
        "Saves:\n",
        "\n",
        "* architecture\n",
        "* weights\n",
        "* compile settings\n",
        "\n",
        "Better than using `.h5`.\n",
        "\n",
        "---\n",
        "\n",
        "### **8. Load full model with custom layer/class**\n",
        "\n",
        "```python\n",
        "loaded_model = tf.keras.models.load_model(\n",
        "    \"my_model.keras\",\n",
        "    custom_objects={\"WideAndDeepModel\": WideAndDeepModel}\n",
        ")\n",
        "```\n",
        "\n",
        "If your model uses custom classes, you must provide them during load.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "d-HlvK_fRXMN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HwfD-RdMY4v"
      },
      "source": [
        "## Using Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7D_F1q2MY4w"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(\"my_checkpoints\", ignore_errors=True)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJqQ8rWaMY4w"
      },
      "source": [
        "**Warning**: as explained earlier, Keras now requires the checkpoint files to have a `.weights.h5` extension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "glfPxReWMY4w"
      },
      "outputs": [],
      "source": [
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_checkpoints.weights.h5\",\n",
        "                                                   save_weights_only=True)\n",
        "history = model.fit(\n",
        "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\n",
        "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),\n",
        "    callbacks=[checkpoint_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMnqPx3fMY4x"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                     restore_best_weights=True)\n",
        "history = model.fit(\n",
        "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=100,\n",
        "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6yy6QBYMY4x"
      },
      "outputs": [],
      "source": [
        "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n",
        "        print(f\"Epoch={epoch}, val/train={ratio:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "furIGR0bMY4x"
      },
      "outputs": [],
      "source": [
        "val_train_ratio_cb = PrintValTrainRatioCallback()\n",
        "history = model.fit(\n",
        "    (X_train_wide, X_train_deep), (y_train, y_train), epochs=10,\n",
        "    validation_data=((X_valid_wide, X_valid_deep), (y_valid, y_valid)),\n",
        "    callbacks=[val_train_ratio_cb], verbose=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is a **short, crisp answer** for each AI question + a **simple explanation** of your Keras callback code.\n",
        "\n",
        "---\n",
        "\n",
        "## **AI — The Fast Template**\n",
        "\n",
        "### **1. What is it?**\n",
        "\n",
        "A system that learns patterns from data and produces outputs (predictions, text, images) without explicit rules.\n",
        "\n",
        "### **2. How does it reason?**\n",
        "\n",
        "By **detecting patterns** in huge datasets, using weights + math (neural networks).\n",
        "It does **statistical reasoning**, not human logical reasoning.\n",
        "\n",
        "### **3. Where does it fail?**\n",
        "\n",
        "* When data is missing or biased\n",
        "* When asked for exact truth beyond patterns\n",
        "* In completely new situations\n",
        "* With unclear or contradictory prompts\n",
        "\n",
        "### **4. When should I use it?**\n",
        "\n",
        "When you need:\n",
        "\n",
        "* classification\n",
        "* prediction\n",
        "* summarization\n",
        "* pattern detection\n",
        "* automation of repetitive cognitive tasks\n",
        "\n",
        "Not useful for strict rule-based logic (e.g., tax rules).\n",
        "\n",
        "### **5. What is the mental model?**\n",
        "\n",
        "Think of AI as a **probabilistic pattern matcher**:\n",
        "“Given X, what is the statistically likely Y?”\n",
        "\n",
        "### **6. How do I prompt it?**\n",
        "\n",
        "* Be **clear**, **specific**, and **bounded**\n",
        "* Give **context**, **examples**, **format**\n",
        "* Mention **constraints** (short, table, JSON, steps etc.)\n",
        "\n",
        "### **7. What are alternatives?**\n",
        "\n",
        "* **Rule-based systems**\n",
        "* **Databases + SQL**\n",
        "* **Classical ML** (SVM, RF, XGBoost)\n",
        "* **Optimization algorithms**\n",
        "* **Symbolic AI**\n",
        "\n",
        "---\n",
        "\n",
        "# **Code Explanation (Short & Clear)**\n",
        "\n",
        "## **1. shutil.rmtree(\"my_checkpoints\", ignore_errors=True)**\n",
        "\n",
        "Deletes the folder **my_checkpoints** if it exists.\n",
        "`ignore_errors=True` = **don’t crash** if the folder is missing.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. ModelCheckpoint**\n",
        "\n",
        "```python\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_checkpoints.weights.h5\",\n",
        "    save_weights_only=True\n",
        ")\n",
        "```\n",
        "\n",
        "This callback:\n",
        "\n",
        "* Saves **model weights** after every epoch\n",
        "* File: `my_checkpoints.weights.h5`\n",
        "* Useful for restoring good weights later\n",
        "\n",
        "---\n",
        "\n",
        "## **3. First model.fit()**\n",
        "\n",
        "```python\n",
        "history = model.fit(..., epochs=10, callbacks=[checkpoint_cb])\n",
        "```\n",
        "\n",
        "Runs 10 epochs and **saves weights** after each epoch.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. EarlyStopping Callback**\n",
        "\n",
        "```python\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "```\n",
        "\n",
        "Stops training **early** if validation loss doesn’t improve for **10 epochs**.\n",
        "\n",
        "`restore_best_weights=True` = after stopping, return to the **best model** seen.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Second model.fit() (with early stopping)**\n",
        "\n",
        "```python\n",
        "history = model.fit(\n",
        "    ... ,\n",
        "    epochs=100,\n",
        "    callbacks=[checkpoint_cb, early_stopping_cb]\n",
        ")\n",
        "```\n",
        "\n",
        "* Trains up to 100 epochs\n",
        "* Stops early\n",
        "* Still saves checkpoints\n",
        "* Ends with best weights restored\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Custom Callback**\n",
        "\n",
        "```python\n",
        "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        ratio = logs[\"val_loss\"] / logs[\"loss\"]\n",
        "        print(f\"Epoch={epoch}, val/train={ratio:.2f}\")\n",
        "```\n",
        "\n",
        "What it does:\n",
        "\n",
        "* After each epoch\n",
        "* Reads loss and val_loss\n",
        "* Prints **val_loss / loss** ratio\n",
        "\n",
        "  * Ratio > 1.3 indicates overfitting\n",
        "  * Ratio ≈ 1 means balanced training\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Third model.fit() using the custom callback**\n",
        "\n",
        "```python\n",
        "history = model.fit(..., callbacks=[val_train_ratio_cb], verbose=0)\n",
        "```\n",
        "\n",
        "Runs silently (verbose=0) and prints only the ratio.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "s-3m0vthPTxD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9IAKbKnMY4y"
      },
      "source": [
        "## Using TensorBoard for Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxH0-_xKMY4y"
      },
      "source": [
        "TensorBoard is preinstalled on Colab, but not the `tensorboard-plugin-profile`, so let's install it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlQG-xn7MY4y"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:  # extra code\n",
        "    %pip install -q -U tensorboard-plugin-profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvHLr-_GMY4z"
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(\"my_logs\", ignore_errors=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1pg5hOAMY4z"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from time import strftime\n",
        "\n",
        "def get_run_logdir(root_logdir=\"my_logs\"):\n",
        "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
        "\n",
        "run_logdir = get_run_logdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCAXopuhMY4z"
      },
      "outputs": [],
      "source": [
        "# extra code – builds the first regression model we used earlier\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
        "model = tf.keras.Sequential([\n",
        "    norm_layer,\n",
        "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RootMeanSquaredError\"])\n",
        "norm_layer.adapt(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GSoTZWrMY40"
      },
      "outputs": [],
      "source": [
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,\n",
        "                                                profile_batch=(100, 200))\n",
        "history = model.fit(X_train, y_train, epochs=20,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[tensorboard_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT5YFurPMY40"
      },
      "outputs": [],
      "source": [
        "print(\"my_logs\")\n",
        "for path in sorted(Path(\"my_logs\").glob(\"**/*\")):\n",
        "    print(\"  \" * (len(path.parts) - 1) + path.parts[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1pNt238MY41"
      },
      "source": [
        "Let's load the `tensorboard` Jupyter extension and start the TensorBoard server:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNEFlc9OMY41"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eAwsbmPMY42"
      },
      "source": [
        "**Note**: if you prefer to access TensorBoard in a separate tab, click the \"localhost:6006\" link below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayfe6itwMY42"
      },
      "outputs": [],
      "source": [
        "# extra code\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import output\n",
        "\n",
        "    output.serve_kernel_port_as_window(6006)\n",
        "else:\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    display(HTML('<a href=\"http://localhost:6006/\">http://localhost:6006/</a>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdFG0yAhMY42"
      },
      "source": [
        "You can use also visualize histograms, images, text, and even listen to audio using TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CyOCTe_MY42"
      },
      "outputs": [],
      "source": [
        "test_logdir = get_run_logdir()\n",
        "writer = tf.summary.create_file_writer(str(test_logdir))\n",
        "with writer.as_default():\n",
        "    for step in range(1, 1000 + 1):\n",
        "        tf.summary.scalar(\"my_scalar\", np.sin(step / 10), step=step)\n",
        "\n",
        "        data = (np.random.randn(100) + 2) * step / 100  # gets larger\n",
        "        tf.summary.histogram(\"my_hist\", data, buckets=50, step=step)\n",
        "\n",
        "        images = np.random.rand(2, 32, 32, 3) * step / 1000  # gets brighter\n",
        "        tf.summary.image(\"my_images\", images, step=step)\n",
        "\n",
        "        texts = [\"The step is \" + str(step), \"Its square is \" + str(step ** 2)]\n",
        "        tf.summary.text(\"my_text\", texts, step=step)\n",
        "\n",
        "        sine_wave = tf.math.sin(tf.range(12000) / 48000 * 2 * np.pi * step)\n",
        "        audio = tf.reshape(tf.cast(sine_wave, tf.float32), [1, -1, 1])\n",
        "        tf.summary.audio(\"my_audio\", audio, sample_rate=48000, step=step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5zSMvcYMY43"
      },
      "source": [
        "**Note**: it used to be possible to easily share your TensorBoard logs with the world by uploading them to https://tensorboard.dev/. Sadly, this service will shut down in December 2023, so I have removed the corresponding code examples from this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5I5kv5RMY43"
      },
      "source": [
        "When you stop this Jupyter kernel (a.k.a. Runtime), it will automatically stop the TensorBoard server as well. Another way to stop the TensorBoard server is to kill it, if you are running on Linux or MacOSX. First, you need to find its process ID:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uik-wPU9MY43"
      },
      "outputs": [],
      "source": [
        "# extra code – lists all running TensorBoard server instances\n",
        "\n",
        "from tensorboard import notebook\n",
        "\n",
        "notebook.list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjHKBKg7MY44"
      },
      "source": [
        "Next you can use the following command on Linux or MacOSX, replacing `<pid>` with the pid listed above:\n",
        "\n",
        "    !kill <pid>\n",
        "\n",
        "On Windows:\n",
        "\n",
        "    !taskkill /F /PID <pid>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ✅ **AI – Quick Answers**\n",
        "\n",
        "### **• What is it?**\n",
        "\n",
        "A system that learns patterns from data and produces predictions, decisions, or generated content.\n",
        "\n",
        "### **• How does it reason?**\n",
        "\n",
        "By matching patterns → computing probabilities → choosing the most likely outcome.\n",
        "(LLMs: token-by-token prediction).\n",
        "\n",
        "### **• Where does it fail?**\n",
        "\n",
        "When data is missing, ambiguous, biased, or when tasks require true understanding, memory, or reasoning beyond patterns.\n",
        "\n",
        "### **• When should I use it?**\n",
        "\n",
        "For pattern-heavy tasks: classification, summarization, prediction, recommendation, automation, chatbot, vision, etc.\n",
        "\n",
        "### **• What is the mental model?**\n",
        "\n",
        "Treat AI like a very smart autocomplete:\n",
        "*“It continues patterns from huge training data, not by understanding like humans.”*\n",
        "\n",
        "### **• How do I prompt it?**\n",
        "\n",
        "Clear intent → role → constraints → examples → output style.\n",
        "Template:\n",
        "**You are X → Do Y → Under Z constraints → In this format → Using these examples.**\n",
        "\n",
        "### **• What are alternatives?**\n",
        "\n",
        "Rules, algorithms, search, statistics, databases, heuristics, automation scripts.\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ **Code Explanation (Simple)**\n",
        "\n",
        "Below is a **short, section-wise explanation**.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Colab Check + Plugin Install**\n",
        "\n",
        "```python\n",
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install -q -U tensorboard-plugin-profile\n",
        "```\n",
        "\n",
        "If running in Google Colab → install extra TensorBoard profiling plugin.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Clear old logs**\n",
        "\n",
        "```python\n",
        "shutil.rmtree(\"my_logs\", ignore_errors=True)\n",
        "```\n",
        "\n",
        "Deletes previous TensorBoard logs (fresh run).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Helper to create timestamped log folders**\n",
        "\n",
        "```python\n",
        "def get_run_logdir(root_logdir=\"my_logs\"):\n",
        "    return Path(root_logdir) / strftime(\"run_%Y_%m_%d_%H_%M_%S\")\n",
        "```\n",
        "\n",
        "Every training session goes into a new folder like:\n",
        "\n",
        "```\n",
        "my_logs/run_2025_11_18_08_45_31\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Prepare model**\n",
        "\n",
        "```python\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "norm_layer = tf.keras.layers.Normalization(...)\n",
        "model = tf.keras.Sequential([...])\n",
        "```\n",
        "\n",
        "* Clears previous graphs\n",
        "* Fixes randomness for reproducibility\n",
        "* Adds a normalization layer\n",
        "* Builds a dense neural network\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Compile + Adapt Normalization**\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"RMSE\"])\n",
        "norm_layer.adapt(X_train)\n",
        "```\n",
        "\n",
        "* Prepares model for regression\n",
        "* Learns normalization statistics from training data\n",
        "\n",
        "---\n",
        "\n",
        "## **6. TensorBoard callback**\n",
        "\n",
        "```python\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir,\n",
        "                                                profile_batch=(100, 200))\n",
        "```\n",
        "\n",
        "Creates logs + profiling between batch 100–200.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Train model**\n",
        "\n",
        "```python\n",
        "history = model.fit(..., callbacks=[tensorboard_cb])\n",
        "```\n",
        "\n",
        "Stores training curves (loss, RMSE) in TensorBoard logs.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. Print log folder tree**\n",
        "\n",
        "Useful for checking file structure.\n",
        "\n",
        "---\n",
        "\n",
        "## **9. Start TensorBoard**\n",
        "\n",
        "```python\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_logs\n",
        "```\n",
        "\n",
        "Launches TensorBoard UI.\n",
        "\n",
        "---\n",
        "\n",
        "## **10. Colab vs Local Browser**\n",
        "\n",
        "Opens TensorBoard in:\n",
        "\n",
        "* A new Colab window (Colab)\n",
        "* A local link (Jupyter)\n",
        "\n",
        "---\n",
        "\n",
        "## **11. Write custom summaries**\n",
        "\n",
        "```python\n",
        "with writer.as_default():\n",
        "    tf.summary.scalar(...)\n",
        "    tf.summary.histogram(...)\n",
        "    tf.summary.image(...)\n",
        "    tf.summary.text(...)\n",
        "    tf.summary.audio(...)\n",
        "```\n",
        "\n",
        "This section manually generates data for TensorBoard demos:\n",
        "\n",
        "| Summary     | Meaning                             |\n",
        "| ----------- | ----------------------------------- |\n",
        "| `scalar`    | Line chart (sin wave)               |\n",
        "| `histogram` | Distribution plot (values increase) |\n",
        "| `image`     | Random images getting brighter      |\n",
        "| `text`      | Text logs                           |\n",
        "| `audio`     | Synthetic tone (sine wave)          |\n",
        "\n",
        "You’ll see all these inside TensorBoard.\n",
        "\n",
        "---\n",
        "\n",
        "## **12. Show TensorBoard sessions**\n",
        "\n",
        "```python\n",
        "notebook.list()\n",
        "```\n",
        "\n",
        "Lists running TensorBoard instances.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ABxuOOjAOkAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCIxnzATMY44"
      },
      "source": [
        "# Fine-Tuning Neural Network Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLLSeOCGMY44"
      },
      "source": [
        "In this section we'll use the Fashion MNIST dataset again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra-xSX2lMY44"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yTWVQ2vMY45"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyL_2E1FMY45"
      },
      "outputs": [],
      "source": [
        "if \"google.colab\" in sys.modules:\n",
        "    %pip install -q -U keras_tuner~=1.4.6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYcYgcOkMY45"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt\n",
        "\n",
        "def build_model(hp):\n",
        "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
        "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
        "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
        "                             sampling=\"log\")\n",
        "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
        "    if optimizer == \"sgd\":\n",
        "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "    else:\n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    for _ in range(n_hidden):\n",
        "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha5Ihp07MY46"
      },
      "outputs": [],
      "source": [
        "random_search_tuner = kt.RandomSearch(\n",
        "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
        "    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n",
        "random_search_tuner.search(X_train, y_train, epochs=10,\n",
        "                           validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDbW76kJMY46"
      },
      "outputs": [],
      "source": [
        "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
        "best_model = top3_models[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWFfdj7cMY47"
      },
      "outputs": [],
      "source": [
        "top3_params = random_search_tuner.get_best_hyperparameters(num_trials=3)\n",
        "top3_params[0].values  # best hyperparameter values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuovnotaMY47"
      },
      "outputs": [],
      "source": [
        "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
        "best_trial.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuJ6ydaWMY48"
      },
      "outputs": [],
      "source": [
        "best_trial.metrics.get_last_value(\"val_accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ7NZ1p5MY48"
      },
      "outputs": [],
      "source": [
        "best_model.fit(X_train_full, y_train_full, epochs=10)\n",
        "test_loss, test_accuracy = best_model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGRphyenMY49"
      },
      "outputs": [],
      "source": [
        "class MyClassificationHyperModel(kt.HyperModel):\n",
        "    def build(self, hp):\n",
        "        return build_model(hp)\n",
        "\n",
        "    def fit(self, hp, model, X, y, **kwargs):\n",
        "        if hp.Boolean(\"normalize\"):\n",
        "            norm_layer = tf.keras.layers.Normalization()\n",
        "            norm_layer.adapt(X)\n",
        "            X = norm_layer(X)\n",
        "        return model.fit(X, y, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8wu6OSCMY49"
      },
      "outputs": [],
      "source": [
        "hyperband_tuner = kt.Hyperband(\n",
        "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
        "    max_epochs=10, factor=3, hyperband_iterations=2,\n",
        "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"hyperband\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_w9GX_OMY4-"
      },
      "outputs": [],
      "source": [
        "root_logdir = Path(hyperband_tuner.project_dir) / \"tensorboard\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(root_logdir)\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2)\n",
        "hyperband_tuner.search(X_train, y_train, epochs=10,\n",
        "                       validation_data=(X_valid, y_valid),\n",
        "                       callbacks=[early_stopping_cb, tensorboard_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCMM7YNQMY4-"
      },
      "outputs": [],
      "source": [
        "bayesian_opt_tuner = kt.BayesianOptimization(\n",
        "    MyClassificationHyperModel(), objective=\"val_accuracy\", seed=42,\n",
        "    max_trials=10, alpha=1e-4, beta=2.6,\n",
        "    overwrite=True, directory=\"my_fashion_mnist\", project_name=\"bayesian_opt\")\n",
        "bayesian_opt_tuner.search(X_train, y_train, epochs=10,\n",
        "                          validation_data=(X_valid, y_valid),\n",
        "                          callbacks=[early_stopping_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_WRgWs3MY4_"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir {root_logdir}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bTR5nh3MY4_"
      },
      "source": [
        "# Exercise solutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaWD18FNMY5A"
      },
      "source": [
        "## 1. to 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Lj5v4ZMY5A"
      },
      "source": [
        "1. Visit the [TensorFlow Playground](https://playground.tensorflow.org/) and play around with it, as described in this exercise.\n",
        "2. Here is a neural network based on the original artificial neurons that computes _A_ ⊕ _B_ (where ⊕ represents the exclusive OR), using the fact that _A_ ⊕ _B_ = (_A_ ∧ ¬ _B_) ∨ (¬ _A_ ∧ _B_). There are other solutions—for example, using the fact that _A_ ⊕ _B_ = (_A_ ∨ _B_) ∧ ¬(_A_ ∧ _B_), or the fact that _A_ ⊕ _B_ = (_A_ ∨ _B_) ∧ (¬ _A_ ∨ ¬ _B_), and so on.<br /><img width=\"70%\" src=\"https://github.com/macsrc/mac-handson-ml3/blob/handson-ml-241025/images/ann/exercise2.png?raw=1\" />\n",
        "3. A classical Perceptron will converge only if the dataset is linearly separable, and it won't be able to estimate class probabilities. In contrast, a Logistic Regression classifier will generally converge to a reasonably good solution even if the dataset is not linearly separable, and it will output class probabilities. If you change the Perceptron's activation function to the sigmoid activation function (or the softmax activation function if there are multiple neurons), and if you train it using Gradient Descent (or some other optimization algorithm minimizing the cost function, typically cross entropy), then it becomes equivalent to a Logistic Regression classifier.\n",
        "4. The sigmoid activation function was a key ingredient in training the first MLPs because its derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all.\n",
        "5. Popular activation functions include the step function, the sigmoid function, the hyperbolic tangent (tanh) function, and the Rectified Linear Unit (ReLU) function (see Figure 10-8). See Chapter 11 for other examples, such as ELU and variants of the ReLU function.\n",
        "6. Considering the MLP described in the question, composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons, where all artificial neurons use the ReLU activation function:\n",
        "    * The shape of the input matrix **X** is _m_ × 10, where _m_ represents the training batch size.\n",
        "    * The shape of the hidden layer's weight matrix **W**<sub>_h_</sub> is 10 × 50, and the length of its bias vector **b**<sub>_h_</sub> is 50.\n",
        "    * The shape of the output layer's weight matrix **W**<sub>_o_</sub> is 50 × 3, and the length of its bias vector **b**<sub>_o_</sub> is 3.\n",
        "    * The shape of the network's output matrix **Y** is _m_ × 3.\n",
        "    * **Y** = ReLU(ReLU(**X** **W**<sub>_h_</sub> + **b**<sub>_h_</sub>) **W**<sub>_o_</sub> + **b**<sub>_o_</sub>). Recall that the ReLU function just sets every negative number in the matrix to zero. Also note that when you are adding a bias vector to a matrix, it is added to every single row in the matrix, which is called _broadcasting_.\n",
        "7. To classify email into spam or ham, you just need one neuron in the output layer of a neural network—for example, indicating the probability that the email is spam. You would typically use the sigmoid activation function in the output layer when estimating a probability. If instead you want to tackle MNIST, you need 10 neurons in the output layer, and you must replace the sigmoid function with the softmax activation function, which can handle multiple classes, outputting one probability per class. If you want your neural network to predict housing prices like in Chapter 2, then you need one output neuron, using no activation function at all in the output layer. Note: when the values to predict can vary by many orders of magnitude, you may want to predict the logarithm of the target value rather than the target value directly. Simply computing the exponential of the neural network's output will give you the estimated value (since exp(log _v_) = _v_).\n",
        "8. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regard to every model parameter (all the weights and biases), then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that (hopefully) minimize the cost function. To compute the gradients, backpropagation uses reverse-mode autodiff (although it wasn't called that when backpropagation was invented, and it has been reinvented several times). Reverse-mode autodiff performs a forward pass through a computation graph, computing every node's value for the current training batch, and then it performs a reverse pass, computing all the gradients at once (see Appendix B for more details). So what's the difference? Well, backpropagation refers to the whole process of training an artificial neural network using multiple backpropagation steps, each of which computes gradients and uses them to perform a Gradient Descent step. In contrast, reverse-mode autodiff is just a technique to compute gradients efficiently, and it happens to be used by backpropagation.\n",
        "9. Here is a list of all the hyperparameters you can tweak in a basic MLP: the number of hidden layers, the number of neurons in each hidden layer, and the activation function used in each hidden layer and in the output layer. In general, the ReLU activation function (or one of its variants; see Chapter 11) is a good default for the hidden layers. For the output layer, in general you will want the sigmoid activation function for binary classification, the softmax activation function for multiclass classification, or no activation function for regression. If the MLP overfits the training data, you can try reducing the number of hidden layers and reducing the number of neurons per hidden layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c-cysmGMY5B"
      },
      "source": [
        "## 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHJcYx3lMY5B"
      },
      "source": [
        "*Exercise: Train a deep MLP on the MNIST dataset (you can load it using `tf.keras.datasets.mnist.load_data()`. See if you can get over 98% accuracy by manually tuning the hyperparameters. Try searching for the optimal learning rate by using the approach presented in this chapter (i.e., by growing the learning rate exponentially, plotting the loss, and finding the point where the loss shoots up). Next, try tuning the hyperparameters using Keras Tuner with all the bells and whistles—save checkpoints, use early stopping, and plot learning curves using TensorBoard.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIFcWh_LMY5C"
      },
      "source": [
        "**TODO**: update this solution to use Keras Tuner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivjTWZ9CMY5C"
      },
      "source": [
        "Let's load the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzQ8HXNRMY5C"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZA3x_wpMY5C"
      },
      "source": [
        "Just like for the Fashion MNIST dataset, the MNIST training set contains 60,000 grayscale images, each 28x28 pixels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T6vsky9MY5D"
      },
      "outputs": [],
      "source": [
        "X_train_full.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q22VmHSKMY5D"
      },
      "source": [
        "Each pixel intensity is also represented as a byte (0 to 255):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SGpl7d2MY5D"
      },
      "outputs": [],
      "source": [
        "X_train_full.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7CuX5ODMY5E"
      },
      "source": [
        "Let's split the full training set into a validation set and a (smaller) training set. We also scale the pixel intensities down to the 0-1 range and convert them to floats, by dividing by 255, just like we did for Fashion MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XG7ZQt5HMY5E"
      },
      "outputs": [],
      "source": [
        "X_valid, X_train = X_train_full[:5000] / 255., X_train_full[5000:] / 255.\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "X_test = X_test / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyW1JYPpMY5E"
      },
      "source": [
        "Let's plot an image using Matplotlib's `imshow()` function, with a `'binary'`\n",
        " color map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zMFFTtQ0MY5F"
      },
      "outputs": [],
      "source": [
        "plt.imshow(X_train[0], cmap=\"binary\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK650rBSMY5F"
      },
      "source": [
        "The labels are the class IDs (represented as uint8), from 0 to 9. Conveniently, the class IDs correspond to the digits represented in the images, so we don't need a `class_names` array:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oTRrqOpMY5F"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGDFiTTkMY5F"
      },
      "source": [
        "The validation set contains 5,000 images, and the test set contains 10,000 images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8RmT-f6NMY5G"
      },
      "outputs": [],
      "source": [
        "X_valid.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2elVYkpwMY5G"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kZSUyECMY5H"
      },
      "source": [
        "Let's take a look at a sample of the images in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upMzA1yzMY5H"
      },
      "outputs": [],
      "source": [
        "n_rows = 4\n",
        "n_cols = 10\n",
        "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
        "for row in range(n_rows):\n",
        "    for col in range(n_cols):\n",
        "        index = n_cols * row + col\n",
        "        plt.subplot(n_rows, n_cols, index + 1)\n",
        "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
        "        plt.axis('off')\n",
        "        plt.title(y_train[index])\n",
        "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R597rG4aMY5H"
      },
      "source": [
        "Let's build a simple dense network and find the optimal learning rate. We will need a callback to grow the learning rate at each iteration. It will also record the learning rate and the loss at each iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkDLFi9xMY5H"
      },
      "outputs": [],
      "source": [
        "K = tf.keras.backend\n",
        "\n",
        "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        lr = self.model.optimizer.learning_rate.numpy() * self.factor\n",
        "        self.model.optimizer.learning_rate = lr\n",
        "        self.rates.append(lr)\n",
        "        self.losses.append(logs[\"loss\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyYlH-ESMY5I"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sKiB13eLMY5I"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNTkyWDBMY5I"
      },
      "source": [
        "We will start with a small learning rate of 1e-3, and grow it by 0.5% at each iteration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVhKrZfnMY5I"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "expon_lr = ExponentialLearningRate(factor=1.005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmzfNyK_MY5J"
      },
      "source": [
        "Now let's train the model for just 1 epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG8kjKL7MY5J"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train, y_train, epochs=1,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[expon_lr])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwmxFK0WMY5K"
      },
      "source": [
        "We can now plot the loss as a functionof the learning rate:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9zzquwJMY5K"
      },
      "outputs": [],
      "source": [
        "plt.plot(expon_lr.rates, expon_lr.losses)\n",
        "plt.gca().set_xscale('log')\n",
        "plt.hlines(min(expon_lr.losses), min(expon_lr.rates), max(expon_lr.rates))\n",
        "plt.axis([min(expon_lr.rates), max(expon_lr.rates), 0, expon_lr.losses[0]])\n",
        "plt.grid()\n",
        "plt.xlabel(\"Learning rate\")\n",
        "plt.ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shRBWDPrMY5L"
      },
      "source": [
        "The loss starts shooting back up violently when the learning rate goes over 6e-1, so let's try using half of that, at 3e-1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy3H12trMY5L"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqcVMWLiMY5L"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yf99hbPdMY5M"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=3e-1)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buhgYXsKMY5M"
      },
      "outputs": [],
      "source": [
        "run_index = 1 # increment this at every run\n",
        "run_logdir = Path() / \"my_mnist_logs\" / \"run_{:03d}\".format(run_index)\n",
        "run_logdir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtZIuwF9MY5M"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20)\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_mnist_model.keras\", save_best_only=True)\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[checkpoint_cb, early_stopping_cb, tensorboard_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRLKMYbAMY5N"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\"my_mnist_model.keras\") # rollback to best model\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AENl6teqMY5O"
      },
      "source": [
        "We got over 98% accuracy. Finally, let's look at the learning curves using TensorBoard:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Snh4KhwuMY5P"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=./my_mnist_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fnO4fLCMY5P"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}