{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9YiVUNL0uhm"
      },
      "source": [
        "**Chapter 11 ‚Äì Training Deep Neural Networks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH8Eyqa10uh1"
      },
      "source": [
        "_This notebook contains all the sample code and solutions to the exercises in chapter 11._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTpLRok60uh3"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/ageron/handson-ml3/blob/main/11_training_deep_neural_networks.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "xmKkp0Jp0uh7"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VMfck8q0uh8"
      },
      "source": [
        "This project requires Python 3.7 or above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySOSN7650uh-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "assert sys.version_info >= (3, 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph9sBrKC0uiE"
      },
      "source": [
        "And TensorFlow ‚â• 2.8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiVoZHA10uiG"
      },
      "outputs": [],
      "source": [
        "from packaging import version\n",
        "import tensorflow as tf\n",
        "\n",
        "assert version.parse(tf.__version__) >= version.parse(\"2.8.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaVpb98w0uiI"
      },
      "source": [
        "As we did in previous chapters, let's define the default font sizes to make the figures prettier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtL_HSCu0uiJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rc('font', size=14)\n",
        "plt.rc('axes', labelsize=14, titlesize=14)\n",
        "plt.rc('legend', fontsize=14)\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWOLp6sI0uiL"
      },
      "source": [
        "And let's create the `images/deep` folder (if it doesn't already exist), and define the `save_fig()` function which is used through this notebook to save the figures in high-res for the book:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbGOIOCr0uiM"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "IMAGES_PATH = Path() / \"images\" / \"deep\"\n",
        "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODy8eDJs0uiN"
      },
      "source": [
        "# Vanishing/Exploding Gradients Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SIj2uG_X0uiO"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell generates and saves Figure 11‚Äì1\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [1, 1], 'k--')\n",
        "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
        "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
        "plt.plot(z, sigmoid(z), \"b-\", linewidth=2,\n",
        "         label=r\"$\\sigma(z) = \\dfrac{1}{1+e^{-z}}$\")\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props,\n",
        "             fontsize=14, ha=\"center\")\n",
        "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props,\n",
        "             fontsize=14, ha=\"center\")\n",
        "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props,\n",
        "             fontsize=14, ha=\"center\")\n",
        "plt.grid(True)\n",
        "plt.axis([-5, 5, -0.2, 1.2])\n",
        "plt.xlabel(\"$z$\")\n",
        "plt.legend(loc=\"upper left\", fontsize=16)\n",
        "\n",
        "save_fig(\"sigmoid_saturation_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSOw7dmB0uiS"
      },
      "source": [
        "## Xavier and He Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UqDBXrY0uiT"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.layers.Dense(50, activation=\"relu\",\n",
        "                              kernel_initializer=\"he_normal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8JAX72t0uiT"
      },
      "outputs": [],
      "source": [
        "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\",\n",
        "                                                    distribution=\"uniform\")\n",
        "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\",\n",
        "                              kernel_initializer=he_avg_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5SeNo1Y0uiU"
      },
      "source": [
        "## Nonsaturating Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApTnwC3t0uiV"
      },
      "source": [
        "### Leaky ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvckTjgE0uiW"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell generates and saves Figure 11‚Äì2\n",
        "\n",
        "def leaky_relu(z, alpha):\n",
        "    return np.maximum(alpha * z, z)\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "plt.plot(z, leaky_relu(z, 0.1), \"b-\", linewidth=2, label=r\"$LeakyReLU(z) = max(\\alpha z, z)$\")\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-1, 3.7], 'k-')\n",
        "plt.grid(True)\n",
        "props = dict(facecolor='black', shrink=0.1)\n",
        "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.3), arrowprops=props,\n",
        "             fontsize=14, ha=\"center\")\n",
        "plt.xlabel(\"$z$\")\n",
        "plt.axis([-5, 5, -1, 3.7])\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.legend()\n",
        "\n",
        "save_fig(\"leaky_relu_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FQSNVVL0uiX"
      },
      "outputs": [],
      "source": [
        "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)  # defaults to alpha=0.3\n",
        "dense = tf.keras.layers.Dense(50, activation=leaky_relu,\n",
        "                              kernel_initializer=\"he_normal\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7syhgFm0uiY"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # [...]  # more layers\n",
        "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  # no activation\n",
        "    tf.keras.layers.LeakyReLU(alpha=0.2),  # activation as a separate layer\n",
        "    # [...]  # more layers\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU9SLdsg0uiZ"
      },
      "source": [
        "### ELU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14LHb5f-0uia"
      },
      "source": [
        "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer, and use He initialization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDTlXNdr0uia"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.layers.Dense(50, activation=\"elu\",\n",
        "                              kernel_initializer=\"he_normal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>AI Template - Deep NN-01</b></summary>\n",
        "\n",
        "Below is the **short + structured AI-style explanation**, followed by a **simple code explanation** for all parts (Sigmoid, Leaky ReLU, Dense layers, initializers).\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ **AI Understanding Template ‚Äì Activation Function Code (Sigmoid, LeakyReLU, ELU)**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A set of **activation functions** and **layers** used in neural networks to learn non-linear patterns:\n",
        "\n",
        "* **Sigmoid** ‚Üí squashes values into (0,1)\n",
        "* **LeakyReLU** ‚Üí fixes dying ReLU problem\n",
        "* **ELU** ‚Üí smoother and faster alternative to ReLU\n",
        "\n",
        "The code **visualizes** these activations and **builds Keras Dense layers** using them.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "The model uses activation functions as ‚Äúdecision curves‚Äù:\n",
        "\n",
        "* Sigmoid:\n",
        "\n",
        "  * Small/large inputs ‚Üí saturated ‚Üí gradient almost zero\n",
        "* LeakyReLU:\n",
        "\n",
        "  * Small inputs ‚Üí slight negative slope ‚Üí never fully dies\n",
        "* ELU:\n",
        "\n",
        "  * Negative inputs ‚Üí smooth exponential decay ‚Üí better gradient flow\n",
        "\n",
        "Dense layers apply:\n",
        "\n",
        "> **output = activation(Wx + b)**\n",
        "> Initializers (He Normal/He Avg) ensure good weight scale for activations.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* **Sigmoid** ‚Üí vanishing gradients, slow training\n",
        "* **ReLU** ‚Üí neurons can ‚Äúdie‚Äù (always output 0)\n",
        "* **LeakyReLU** ‚Üí still piecewise linear; sometimes too simple\n",
        "* **ELU** ‚Üí slightly slower to compute\n",
        "* **Bad initialization** ‚Üí exploding/vanishing gradients\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "* Sigmoid ‚Üí only for binary output layers\n",
        "* LeakyReLU ‚Üí when ReLU is unstable or dead neurons appear\n",
        "* ELU ‚Üí when training is slow or gradients vanish\n",
        "* He initialization ‚Üí always for ReLU-family activations\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think in simple shapes:\n",
        "\n",
        "| Activation    | Mental Model                             |\n",
        "| ------------- | ---------------------------------------- |\n",
        "| **Sigmoid**   | S-shaped curve ‚Üí ‚Äúsoft decision‚Äù         |\n",
        "| **ReLU**      | Half line that cuts negatives            |\n",
        "| **LeakyReLU** | ReLU with a leak ‚Üí never fully dead      |\n",
        "| **ELU**       | Smooth curve ‚Üí good gradients everywhere |\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(Here: how to use it in code)\n",
        "\n",
        "* Use `activation=\"relu\"` or `activation=LeakyReLU()` in Dense\n",
        "* Pair LeakyReLU/ELU with `\"he_normal\"` initialization\n",
        "* Use sigmoid only at the last layer of binary classification\n",
        "* Use separate activation layers if you want more flexibility\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "* **GELU** (used in transformers)\n",
        "* **Swish** (Google ‚Üí smoother than ReLU)\n",
        "* **Mish** (variant of Swish)\n",
        "* **PReLU** (learnable negative slope)\n",
        "* **SELU** (for self-normalizing networks)\n",
        "\n",
        "---\n",
        "\n",
        "## -----------------------------------------------\n",
        "\n",
        "## ‚úÖ **CODE EXPLANATION (Simple + Short)**\n",
        "\n",
        "## -----------------------------------------------\n",
        "\n",
        "## **1. Sigmoid Curve Plotting**\n",
        "\n",
        "```python\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "```\n",
        "\n",
        "Computes sigmoid function.\n",
        "\n",
        "```python\n",
        "z = np.linspace(-5, 5, 200)\n",
        "plt.plot(z, sigmoid(z))\n",
        "```\n",
        "\n",
        "Creates x values from ‚Äì5 to 5 and plots the sigmoid curve.\n",
        "\n",
        "```python\n",
        "plt.plot([-5, 5], [0, 0], 'k-')   # horizontal line\n",
        "plt.plot([-5, 5], [1, 1], 'k--') # y=1 saturation line\n",
        "plt.plot([0, 0], [-0.2, 1.2], 'k-') # y-axis\n",
        "plt.plot([-5, 5], [-3/4, 7/4], 'g--') # linear region reference\n",
        "```\n",
        "\n",
        "Adds reference lines for understanding where sigmoid is linear or saturated.\n",
        "\n",
        "```python\n",
        "plt.annotate('Saturating', ...)\n",
        "```\n",
        "\n",
        "Labels the flat ends of the sigmoid curve.\n",
        "\n",
        "```python\n",
        "save_fig(\"sigmoid_saturation_plot\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "Saves and displays the plot.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Dense Layer With Sigmoid + Custom Initializer**\n",
        "\n",
        "```python\n",
        "he_avg_init = tf.keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\",\n",
        "                                                    distribution=\"uniform\")\n",
        "\n",
        "dense = tf.keras.layers.Dense(50, activation=\"sigmoid\",\n",
        "                              kernel_initializer=he_avg_init)\n",
        "```\n",
        "\n",
        "Creates:\n",
        "\n",
        "* Dense layer with **50 neurons**\n",
        "* **Sigmoid activation**\n",
        "* **He Avg initializer** (good for sigmoid / ReLU family)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Leaky ReLU Plotting**\n",
        "\n",
        "```python\n",
        "def leaky_relu(z, alpha):\n",
        "    return np.maximum(alpha * z, z)\n",
        "```\n",
        "\n",
        "Implements LeakyReLU manually.\n",
        "\n",
        "```python\n",
        "plt.plot(z, leaky_relu(z, 0.1))\n",
        "```\n",
        "\n",
        "Plots the curve showing a small slope for negative values.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Using LeakyReLU in Keras**\n",
        "\n",
        "### Option 1: Inline activation\n",
        "\n",
        "```python\n",
        "leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "\n",
        "dense = tf.keras.layers.Dense(50, activation=leaky_relu,\n",
        "                              kernel_initializer=\"he_normal\")\n",
        "```\n",
        "\n",
        "Activation included inside the layer.\n",
        "\n",
        "### Option 2: Separate layer\n",
        "\n",
        "```python\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(50, kernel_initializer=\"he_normal\"),  \n",
        "    tf.keras.layers.LeakyReLU(alpha=0.2),  \n",
        "])\n",
        "```\n",
        "\n",
        "Gives flexibility (easier to swap activations).\n",
        "\n",
        "---\n",
        "\n",
        "## **5. ELU Activation**\n",
        "\n",
        "```python\n",
        "dense = tf.keras.layers.Dense(50, activation=\"elu\",\n",
        "                              kernel_initializer=\"he_normal\")\n",
        "```\n",
        "\n",
        "ELU gives smooth negative outputs ‚Üí better gradient flow.\n",
        "\n",
        "---\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "ilCgivfi2lNU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ZOmwBn1v0uib"
      },
      "source": [
        "### SELU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKJ1uRzP0uib"
      },
      "source": [
        "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too, and other constraints are respected, as explained in the book). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3Pky6q30uic"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell generates and saves Figure 11‚Äì3\n",
        "\n",
        "from scipy.special import erfc\n",
        "\n",
        "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
        "# (see equation 14 in the paper):\n",
        "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1 / np.sqrt(2)) * np.exp(1 / 2) - 1)\n",
        "scale_0_1 = (\n",
        "    (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e))\n",
        "    * np.sqrt(2 * np.pi)\n",
        "    * (\n",
        "        2 * erfc(np.sqrt(2)) * np.e ** 2\n",
        "        + np.pi * erfc(1 / np.sqrt(2)) ** 2 * np.e\n",
        "        - 2 * (2 + np.pi) * erfc(1 / np.sqrt(2)) * np.sqrt(np.e)\n",
        "        + np.pi\n",
        "        + 2\n",
        "    ) ** (-1 / 2)\n",
        ")\n",
        "\n",
        "def elu(z, alpha=1):\n",
        "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)\n",
        "\n",
        "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
        "    return scale * elu(z, alpha)\n",
        "\n",
        "z = np.linspace(-5, 5, 200)\n",
        "plt.plot(z, elu(z), \"b-\", linewidth=2, label=r\"ELU$_\\alpha(z) = \\alpha (e^z - 1)$ if $z < 0$, else $z$\")\n",
        "plt.plot(z, selu(z), \"r--\", linewidth=2, label=r\"SELU$(z) = 1.05 \\, $ELU$_{1.67}(z)$\")\n",
        "plt.plot([-5, 5], [0, 0], 'k-')\n",
        "plt.plot([-5, 5], [-1, -1], 'k:', linewidth=2)\n",
        "plt.plot([-5, 5], [-1.758, -1.758], 'k:', linewidth=2)\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.axis([-5, 5, -2.2, 3.2])\n",
        "plt.xlabel(\"$z$\")\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.legend()\n",
        "\n",
        "save_fig(\"elu_selu_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrUoBmB50uid"
      },
      "source": [
        "Using SELU is straightforward:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUBQ45Zq0uie"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.layers.Dense(50, activation=\"selu\",\n",
        "                              kernel_initializer=\"lecun_normal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_34qi_BE0uie"
      },
      "source": [
        "**Extra material ‚Äì an example of a self-regularized network using SELU**\n",
        "\n",
        "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLZZLXAc0uif"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n",
        "for layer in range(100):\n",
        "    model.add(tf.keras.layers.Dense(100, activation=\"selu\",\n",
        "                                    kernel_initializer=\"lecun_normal\"))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQRYN6qJ0uig"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMe_XzgE0uih"
      },
      "source": [
        "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDPZRQV80uii"
      },
      "outputs": [],
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
        "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
        "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
        "X_train, X_valid, X_test = X_train / 255, X_valid / 255, X_test / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8sEh0jP0uij"
      },
      "outputs": [],
      "source": [
        "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uLCevsx0uij"
      },
      "outputs": [],
      "source": [
        "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
        "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
        "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
        "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
        "X_test_scaled = (X_test - pixel_means) / pixel_stds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2luGUht30uik"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3Hdm68M0uil"
      },
      "source": [
        "The network managed to learn, despite how deep it is. Now look at what happens if we try to use the ReLU activation function instead:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duW6HkAi0ujU"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgE5CB4p0ujV"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[28, 28]))\n",
        "for layer in range(100):\n",
        "    model.add(tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                                    kernel_initializer=\"he_normal\"))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zxp1ok4j0ujW"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnf909Tz0ujX"
      },
      "outputs": [],
      "source": [
        "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
        "                    validation_data=(X_valid_scaled, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW5vQCgI0ujY"
      },
      "source": [
        "Not great at all, we suffered from the vanishing/exploding gradients problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1dpcSr20ujY"
      },
      "source": [
        "### GELU, Swish and Mish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxMez6ga0ujZ"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell generates and saves Figure 11‚Äì4\n",
        "\n",
        "def swish(z, beta=1):\n",
        "    return z * sigmoid(beta * z)\n",
        "\n",
        "def approx_gelu(z):\n",
        "    return swish(z, beta=1.702)\n",
        "\n",
        "def softplus(z):\n",
        "    return np.log(1 + np.exp(z))\n",
        "\n",
        "def mish(z):\n",
        "    return z * np.tanh(softplus(z))\n",
        "\n",
        "z = np.linspace(-4, 2, 200)\n",
        "\n",
        "beta = 0.6\n",
        "plt.plot(z, approx_gelu(z), \"b-\", linewidth=2,\n",
        "         label=r\"GELU$(z) = z\\,\\Phi(z)$\")\n",
        "plt.plot(z, swish(z), \"r--\", linewidth=2,\n",
        "         label=r\"Swish$(z) = z\\,\\sigma(z)$\")\n",
        "plt.plot(z, swish(z, beta), \"r:\", linewidth=2,\n",
        "         label=fr\"Swish$_{{\\beta={beta}}}(z)=z\\,\\sigma({beta}\\,z)$\")\n",
        "plt.plot(z, mish(z), \"g:\", linewidth=3,\n",
        "         label=fr\"Mish$(z) = z\\,\\tanh($softplus$(z))$\")\n",
        "plt.plot([-4, 2], [0, 0], 'k-')\n",
        "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
        "plt.grid(True)\n",
        "plt.axis([-4, 2, -1, 2])\n",
        "plt.gca().set_aspect(\"equal\")\n",
        "plt.xlabel(\"$z$\")\n",
        "plt.legend(loc=\"upper left\")\n",
        "\n",
        "save_fig(\"gelu_swish_mish_plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b> ‚úÖAI Understanding Template-Vanishing/Exploding Gradient -02 </b></summary>\n",
        "Below is the **short, crisp AI-style explanation + code explanation** exactly in the 7-question format you asked for.\n",
        "(Simple, practical, interview-ready.)\n",
        "\n",
        "---\n",
        "\n",
        "## **AI Understanding Template ‚Äî SELU / ELU / Self-Normalizing Network Code**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A **Self-Normalizing Neural Network (SNN)** using **SELU activation + LeCun normal initialization** so that activations naturally push themselves toward mean=0 and std=1 during training.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* ELU/SELU introduce **smooth negative values** ‚Üí reduce dead neurons.\n",
        "* SELU automatically **normalizes activations** ‚Üí avoids exploding/vanishing.\n",
        "* Deep stack (100 layers) becomes trainable without batch norm.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* When you use **dropout** ‚Üí must use **AlphaDropout**, not regular dropout.\n",
        "* When inputs are **not scaled** ‚Üí breaks self-normalization.\n",
        "* When architecture has **residuals, skip-connections, CNNs** ‚Üí SELU advantage reduces.\n",
        "* Works mainly for **fully-connected MLPs**.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use SELU when:\n",
        "\n",
        "* You build **very deep MLPs** (50‚Äì200 dense layers).\n",
        "* You want **fast convergence without BatchNorm**.\n",
        "* You want **stable gradients** and **simple architecture**.\n",
        "\n",
        "Use ReLU/He when:\n",
        "\n",
        "* CNNs, transformers, or residual networks.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "> **‚ÄúSELU is BatchNorm without layers.‚Äù**\n",
        "> It keeps the network automatically centered and scaled.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it? (How to use it)**\n",
        "\n",
        "‚úî Use: `activation=\"selu\"`\n",
        "‚úî Use: `kernel_initializer=\"lecun_normal\"`\n",
        "‚úî Use: **scaled input** (mean=0, std=1)\n",
        "‚úò Do NOT use standard dropout (use AlphaDropout)\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "| Activation           | When to use                      |\n",
        "| -------------------- | -------------------------------- |\n",
        "| **ReLU + He init**   | Default for CNNs, deep nets      |\n",
        "| **GELU**             | Transformers, NLP                |\n",
        "| **Swish / Mish**     | Slightly better than ReLU        |\n",
        "| **ELU**              | Older smooth alternative to ReLU |\n",
        "| **BatchNorm + ReLU** | Stable deep networks             |\n",
        "\n",
        "---\n",
        "\n",
        "# **CODE EXPLANATION (Short, Simple)**\n",
        "\n",
        "---\n",
        "\n",
        "## **#1 ‚Äî Compute & Plot ELU vs SELU**\n",
        "\n",
        "```python\n",
        "alpha_0_1, scale_0_1 = ...  # SELU constants from the paper\n",
        "```\n",
        "\n",
        "* These constants make SELU self-normalizing (mean=0, std=1).\n",
        "* `elu(z)` defines ELU activation.\n",
        "* `selu(z)` = `scale √ó ELU(alpha¬∑z)` ‚Üí self-normalizing version.\n",
        "* Plot compares ELU (blue) vs SELU (red dashed).\n",
        "\n",
        "Purpose ‚Üí visualize why SELU stays stable.\n",
        "\n",
        "---\n",
        "\n",
        "## **#2 ‚Äî Create a SELU Layer**\n",
        "\n",
        "```python\n",
        "dense = tf.keras.layers.Dense(50, activation=\"selu\",\n",
        "                              kernel_initializer=\"lecun_normal\")\n",
        "```\n",
        "\n",
        "* SELU **must** use `lecun_normal` to maintain stable variance.\n",
        "* This layer becomes self-normalizing.\n",
        "\n",
        "---\n",
        "\n",
        "## **#3 ‚Äî Build a 100-layer Deep SELU Network**\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential()\n",
        "model.add(Flatten())\n",
        "for layer in range(100):\n",
        "    model.add(Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
        "model.add(Dense(10, softmax))\n",
        "```\n",
        "\n",
        "This extremely deep MLP is possible because SELU avoids:\n",
        "\n",
        "* vanishing gradients\n",
        "* exploding activations\n",
        "* need for batch normalization\n",
        "\n",
        "---\n",
        "\n",
        "## **#4 ‚Äî Compile**\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=SGD(0.001),\n",
        "              metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "Standard classification setup ‚Üí 10 classes.\n",
        "\n",
        "---\n",
        "\n",
        "## **#5 ‚Äî Load & Scale Fashion-MNIST**\n",
        "\n",
        "```python\n",
        "X_train = X_train / 255\n",
        "```\n",
        "\n",
        "Normalize 0‚Äì255 pixel values to 0‚Äì1.\n",
        "\n",
        "---\n",
        "\n",
        "## **#6 ‚Äî Class Names**\n",
        "\n",
        "Just a lookup table for the 10 class labels.\n",
        "\n",
        "---\n",
        "\n",
        "## **#7 ‚Äî Standardize Inputs (important for SELU)**\n",
        "\n",
        "```python\n",
        "X_train_scaled = (X_train - mean) / std\n",
        "```\n",
        "\n",
        "SELU works best when input has **mean=0, std=1**.\n",
        "\n",
        "---\n",
        "\n",
        "## **#8 ‚Äî Train SELU Model, Then Train ReLU Model**\n",
        "\n",
        "First training:\n",
        "\n",
        "```python\n",
        "model.fit(X_train_scaled, y_train)\n",
        "```\n",
        "\n",
        "Then rebuild a **ReLU+He** network for comparison:\n",
        "\n",
        "```python\n",
        "activation=\"relu\"\n",
        "kernel_initializer=\"he_normal\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **Bonus: Activation Function Plots**\n",
        "\n",
        "GELU, Swish, Mish are computed:\n",
        "\n",
        "```python\n",
        "approx_gelu(z)\n",
        "swish(z)\n",
        "mish(z)\n",
        "```\n",
        "\n",
        "Plots help visually compare modern alternatives.\n",
        "\n",
        "---\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "4b1hEiUg5xEQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Ci92nB0uja"
      },
      "source": [
        "# Batch Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsr4pP8e0ujb"
      },
      "outputs": [],
      "source": [
        "# extra code - clear the name counters and set the random seed\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjkqB8kJ0ujc"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSkbDsCP0ujc"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEb_pKQG0ujd"
      },
      "outputs": [],
      "source": [
        "[(var.name, var.trainable) for var in model.layers[1].variables]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YMfO5JI0uje"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì just show that the model works! üòä\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mQh9wN30ujf"
      },
      "source": [
        "Sometimes applying BN before the activation function works better (there's a debate on this topic). Moreover, the layer before a `BatchNormalization` layer does not need to have bias terms, since the `BatchNormalization` layer some as well, it would be a waste of parameters, so you can set `use_bias=False` when creating those layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5SOmGWu0ujg"
      },
      "outputs": [],
      "source": [
        "# extra code - clear the name counters and set the random seed\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJnvE1ZQ0ujh"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "    tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Activation(\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jsl4yV5B0uji"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì just show that the model works! üòä\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=2, validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPL3q9Hd0ujj"
      },
      "source": [
        "## Gradient Clipping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMcuyx2Y0ujk"
      },
      "source": [
        "All `tf.keras.optimizers` accept `clipnorm` or `clipvalue` arguments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6asGAtp0ujl"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDj_NU5F0ujl"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b> ‚úÖAI Understanding Template-Vanishing/Batch Normalization-01 </b></summary>\n",
        "\n",
        "\n",
        "Below is a short, crisp AI-style explanation (like you requested) + code explanation for the BatchNorm + Dense MNIST model.\n",
        "\n",
        "‚úÖ AI Understanding Template ‚Äì BatchNorm + Dense MNIST Model\n",
        "1. What is it?\n",
        "A feed-forward neural network for image classification (MNIST), enhanced with:\n",
        "\n",
        "\n",
        "Batch Normalization (stabilizes training)\n",
        "\n",
        "\n",
        "He initialization (good for ReLU)\n",
        "\n",
        "\n",
        "Gradient clipping (prevents exploding gradients)\n",
        "\n",
        "\n",
        "\n",
        "2. How does it reason?\n",
        "\n",
        "\n",
        "Flatten layer converts 2D images ‚Üí 1D vector.\n",
        "\n",
        "\n",
        "Dense layers learn patterns (edges ‚Üí shapes ‚Üí digits).\n",
        "\n",
        "\n",
        "BatchNorm normalizes intermediate activations ‚Üí faster + smoother learning.\n",
        "\n",
        "\n",
        "ReLU extracts non-linear patterns.\n",
        "\n",
        "\n",
        "Softmax produces class probabilities.\n",
        "\n",
        "\n",
        "\n",
        "3. Where does it fail?\n",
        "\n",
        "\n",
        "When images have complex spatial patterns ‚Üí CNNs perform better.\n",
        "\n",
        "\n",
        "If BatchNorm is misused (tiny batch size).\n",
        "\n",
        "\n",
        "With too-deep networks ‚Üí vanishing gradients (unless using skip connections).\n",
        "\n",
        "\n",
        "When digits are noisy, rotated, or very small.\n",
        "\n",
        "\n",
        "\n",
        "4. When should I use it?\n",
        "Use this model when:\n",
        "\n",
        "\n",
        "You want a simple but robust baseline for tabular/flattened image data.\n",
        "\n",
        "\n",
        "You need fast training with stable gradients.\n",
        "\n",
        "\n",
        "You want to test BatchNorm vs no BatchNorm behavior.\n",
        "\n",
        "\n",
        "\n",
        "5. What is the mental model?\n",
        "Think of the network as:\n",
        "\n",
        "‚ÄúA stack of linear layers that get cleaned/reset after each step (BatchNorm), then activated to learn patterns.‚Äù\n",
        "\n",
        "BatchNorm = cleaning + stabilizing\n",
        "ReLU = nonlinear thinking\n",
        "Dense = feature mixing\n",
        "Softmax = decision making\n",
        "\n",
        "6. How do I prompt it? (i.e., how to use it correctly)\n",
        "\n",
        "\n",
        "Always scale inputs to 0‚Äì1 or ‚àí1 to 1.\n",
        "\n",
        "\n",
        "Use BatchNorm before or after activation‚Äîboth work (two versions shown).\n",
        "\n",
        "\n",
        "Use He initialization with ReLU.\n",
        "\n",
        "\n",
        "For stable training:\n",
        "\n",
        "\n",
        "Large batch ‚Üí good for BatchNorm\n",
        "\n",
        "\n",
        "Use gradient clipping if gradients explode\n",
        "\n",
        "7. What are alternatives?\n",
        "TechniqueWhen to preferCNNs (Conv2D)Better accuracy for images (spatial structure)DropoutStronger regularization than BatchNormLayerNorm / GroupNormVery small batchesResidual NetworksDeep networks with stable gradientsTransformers (ViT)Larger datasets and modern architectures\n",
        "\n",
        "üß† Code Explanation (Short & Clear)\n",
        "\n",
        "‚úî First Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "\n",
        "üëâ Converts 28√ó28 image ‚Üí 784 vector.\n",
        "\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "\n",
        "üëâ BatchNorm before Dense ensures normalized input.\n",
        "üëâ Dense(300) learns 300 hidden features using ReLU.\n",
        "üëâ He initialization avoids vanishing/exploding activations.\n",
        "\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
        "\n",
        "üëâ Repeat: normalization + nonlinear layer.\n",
        "\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "\n",
        "üëâ Final classification into 10 digits.\n",
        "\n",
        "‚úî Trainable variables check\n",
        "[(var.name, var.trainable) for var in model.layers[1].variables]\n",
        "\n",
        "üëâ Shows BatchNorm variables (gamma, beta, moving mean, moving variance).\n",
        "\n",
        "‚úî Training\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(X_train, y_train, epochs=2)\n",
        "\n",
        "üëâ SGD optimizer + cross-entropy for classification.\n",
        "\n",
        "\n",
        "‚úî Second Model (BN after Dense, no bias)\n",
        "tf.keras.layers.Dense(300, kernel_initializer=\"he_normal\", use_bias=False),\n",
        "tf.keras.layers.BatchNormalization(),\n",
        "tf.keras.layers.Activation(\"relu\"),\n",
        "\n",
        "Why use_bias=False?\n",
        "‚Üí BatchNorm already has a learnable shift (beta), so bias is unnecessary.\n",
        "Order = Dense ‚Üí BatchNorm ‚Üí Activation\n",
        "This is the recommended modern pattern.\n",
        "\n",
        "‚úî Gradient Clipping\n",
        "1Ô∏è‚É£ Clip by value\n",
        "optimizer = tf.keras.optimizers.SGD(clipvalue=1.0)\n",
        "\n",
        "üëâ If gradient > 1, force it to 1.\n",
        "2Ô∏è‚É£ Clip by global norm\n",
        "optimizer = tf.keras.optimizers.SGD(clipnorm=1.0)\n",
        "\n",
        "üëâ Scale all gradients so their norm ‚â§ 1.\n",
        "(Better for deep models.)\n",
        "\n",
        "‚úÖ Summary (one-line)\n",
        "A stabilized, well-initialized Dense network for MNIST using BatchNorm, ReLU, and gradient clipping to ensure smooth, fast training.\n",
        "\n",
        "\n",
        "<details>"
      ],
      "metadata": {
        "id": "fnkEYuy08o9k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bbgTVLm0ujm"
      },
      "source": [
        "## Reusing Pretrained Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLRGg3Lx0ujn"
      },
      "source": [
        "### Reusing a Keras model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7-e0Wy90ujn"
      },
      "source": [
        "Let's split the fashion MNIST training set in two:\n",
        "* `X_train_A`: all images of all items except for T-shirts/tops and pullovers (classes 0 and 2).\n",
        "* `X_train_B`: a much smaller training set of just the first 200 images of T-shirts/tops and pullovers.\n",
        "\n",
        "The validation set and the test set are also split this way, but without restricting the number of images.\n",
        "\n",
        "We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (trousers, dresses, coats, sandals, shirts, sneakers, bags, and ankle boots) are somewhat similar to classes in set B (T-shirts/tops and pullovers). However, since we are using `Dense` layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the chapter 14)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1FMn72Z0ujo"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì split Fashion MNIST into tasks A and B, then train and save\n",
        "#              model A to \"my_model_A\".\n",
        "\n",
        "pos_class_id = class_names.index(\"Pullover\")\n",
        "neg_class_id = class_names.index(\"T-shirt/top\")\n",
        "\n",
        "def split_dataset(X, y):\n",
        "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
        "    y_A = y[~y_for_B]\n",
        "    y_B = (y[y_for_B] == pos_class_id).astype(np.float32)\n",
        "    old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
        "    for old_class_id, new_class_id in zip(old_class_ids, range(8)):\n",
        "        y_A[y_A == old_class_id] = new_class_id  # reorder class ids for A\n",
        "    return ((X[~y_for_B], y_A), (X[y_for_B], y_B))\n",
        "\n",
        "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
        "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
        "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
        "X_train_B = X_train_B[:200]\n",
        "y_train_B = y_train_B[:200]\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "model_A = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(8, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model_A.fit(X_train_A, y_train_A, epochs=20,\n",
        "                      validation_data=(X_valid_A, y_valid_A))\n",
        "model_A.save(\"my_model_A.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2auPLX4O0ujp"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì train and evaluate model B, without reusing model A\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "model_B = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "\n",
        "model_B.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
        "                      validation_data=(X_valid_B, y_valid_B))\n",
        "model_B.evaluate(X_test_B, y_test_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0IX-dgN0ujq"
      },
      "source": [
        "Model B reaches 91.85% accuracy on the test set. Now let's try reusing the pretrained model A."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPmSLgIG0ujr"
      },
      "outputs": [],
      "source": [
        "model_A = tf.keras.models.load_model(\"my_model_A.keras\")\n",
        "model_B_on_A = tf.keras.Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z07ovDns0ujs"
      },
      "source": [
        "Note that `model_B_on_A` and `model_A` actually share layers now, so when we train one, it will update both models. If we want to avoid that, we need to build `model_B_on_A` on top of a *clone* of `model_A`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39Zj3-zt0ujs"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code ‚Äì ensure reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZkuIH3F0ujt"
      },
      "outputs": [],
      "source": [
        "model_A_clone = tf.keras.models.clone_model(model_A)\n",
        "model_A_clone.set_weights(model_A.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGBkm1O90uju"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì creating model_B_on_A just like in the previous cell\n",
        "model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-1])\n",
        "model_B_on_A.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hlfSngV0ujv"
      },
      "outputs": [],
      "source": [
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBhGKTB_0ujv"
      },
      "outputs": [],
      "source": [
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
        "                           validation_data=(X_valid_B, y_valid_B))\n",
        "\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model_B_on_A.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
        "                     metrics=[\"accuracy\"])\n",
        "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
        "                           validation_data=(X_valid_B, y_valid_B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-emECGAt0ujw"
      },
      "source": [
        "So, what's the final verdict?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEp0PvEf0ujx"
      },
      "outputs": [],
      "source": [
        "model_B_on_A.evaluate(X_test_B, y_test_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eee4po0N0ujy"
      },
      "source": [
        "Great! We got a bit of transfer: the model's accuracy went up 2 percentage points, from 91.85% to 93.85%. This means the error rate dropped by almost 25%:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Rqykx00ujz"
      },
      "outputs": [],
      "source": [
        "1 - (100 - 93.85) / (100 - 91.85)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b> ‚úÖAI Understanding Template-Deep NN-02 </b></summary>\n",
        "\n",
        "Below is a **short, clear AI-style explanation** followed by a **simple code explanation**.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **AI Understanding Template ‚Äì Two-Model System (Model A + Model B)**\n",
        "\n",
        "*(Used for transfer learning on Fashion-MNIST)*\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A two-stage AI pipeline:\n",
        "\n",
        "* **Model A** ‚Üí Multi-class classifier for 8 clothing categories.\n",
        "* **Model B** ‚Üí Binary classifier: \"*Is this a Pullover or T-shirt?*\"\n",
        "* **Model B_on_A** ‚Üí Transfer learning: reuse Model A‚Äôs learned features to improve Model B.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* Model A learns **general features** (edges, shapes, textures).\n",
        "* Model B learns **fine-grained difference** between two similar classes.\n",
        "* Model B_on_A uses Model A‚Äôs layers as a **feature extractor** and adds a binary head.\n",
        "* Training happens in two phases:\n",
        "\n",
        "  1. **Freeze** shared layers ‚Üí learn only the binary head\n",
        "  2. **Unfreeze** ‚Üí fine-tune all layers slightly\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* If initial model (A) is poorly trained ‚Üí transfer learning helps less\n",
        "* If binary classes are **too similar** ‚Üí needs more data\n",
        "* If Model B training data is tiny ‚Üí overfitting\n",
        "* If layers are **not frozen in the beginning** ‚Üí catastrophic forgetting\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use this setup when:\n",
        "\n",
        "* You have a **large dataset** for general classification\n",
        "* You have **small, specific dataset** for a focused binary problem\n",
        "* You want **fast training** + **high accuracy** with fewer samples\n",
        "* You‚Äôre doing **class specialization** (general model ‚Üí expert model)\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "> **Model A = Teacher (broad knowledge)**\n",
        ">\n",
        "> **Model B = Student (small dataset)**\n",
        ">\n",
        "> **Model B_on_A = Student who learns from the Teacher's brain.**\n",
        "\n",
        "The student borrows the teacher‚Äôs feature detector, then fine-tunes a bit.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?** *(meaning: how to use/train this architecture)*\n",
        "\n",
        "* Build **A**, train on big dataset\n",
        "* Build **B**, train on small dataset\n",
        "* Build **B_on_A**:\n",
        "\n",
        "  * Remove A‚Äôs last softmax layer\n",
        "  * Add binary sigmoid layer\n",
        "  * **Freeze ‚Üí train small head**\n",
        "  * **Unfreeze ‚Üí fine-tune entire network**\n",
        "* Evaluate on binary test set\n",
        "\n",
        "This is exactly what the code does.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "| Alternative                                                  | Use when                                 |\n",
        "| ------------------------------------------------------------ | ---------------------------------------- |\n",
        "| **Train B from scratch only**                                | You have enough binary data              |\n",
        "| **Use CNN like LeNet, ResNet**                               | Images need stronger feature extraction  |\n",
        "| **Use full transfer learning (MobileNet/ResNet pretrained)** | Small dataset + high accuracy            |\n",
        "| **One-vs-Rest or One-vs-One SVM**                            | Classical ML, small images               |\n",
        "| **Few-shot learning / Siamese networks**                     | Very small binary dataset (<100 samples) |\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **Code Explanation (Short + Simple) **\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 1: Pick positive & negative classes**\n",
        "\n",
        "```python\n",
        "pos_class_id = class_names.index(\"Pullover\")\n",
        "neg_class_id = class_names.index(\"T-shirt/top\")\n",
        "```\n",
        "\n",
        "You decide to classify only:\n",
        "**Pullover = 1**\n",
        "**T-shirt/top = 0**\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Split dataset into two parts**\n",
        "\n",
        "```python\n",
        "def split_dataset(X, y):\n",
        "    y_for_B = (y == pos_class_id) | (y == neg_class_id)\n",
        "```\n",
        "\n",
        "* `y_for_B`: samples used for **binary model B**\n",
        "* Remaining samples ‚Üí **multi-class model A**\n",
        "\n",
        "### Reorder class IDs for model A\n",
        "\n",
        "```python\n",
        "old_class_ids = list(set(range(10)) - set([neg_class_id, pos_class_id]))\n",
        "y_A[y_A == old] = new\n",
        "```\n",
        "\n",
        "Model A needs classes from **0 to 7**, so IDs are remapped.\n",
        "\n",
        "Function returns:\n",
        "\n",
        "* Data for **Model A (8 classes)**\n",
        "* Data for **Model B (binary)**\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 3: Train Model A (8-class classifier)**\n",
        "\n",
        "A simple 3-hidden-layer network with softmax:\n",
        "\n",
        "```python\n",
        "model_A = tf.keras.Sequential([... Dense(8, softmax) ...])\n",
        "```\n",
        "\n",
        "Train it:\n",
        "\n",
        "```python\n",
        "model_A.fit(...)\n",
        "model_A.save(\"my_model_A.keras\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 4: Train Model B (Binary classifier)**\n",
        "\n",
        "Same architecture but **sigmoid(1)** output:\n",
        "\n",
        "```python\n",
        "model_B = tf.keras.Sequential([... Dense(1, sigmoid) ...])\n",
        "```\n",
        "\n",
        "Uses only 200 images to simulate **few-shot learning**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 5: Build Model B_on_A (transfer learning)**\n",
        "\n",
        "### Load Model A\n",
        "\n",
        "```python\n",
        "model_A = tf.keras.models.load_model(...)\n",
        "```\n",
        "\n",
        "### Remove its final softmax layer\n",
        "\n",
        "Add a new binary layer:\n",
        "\n",
        "```python\n",
        "model_B_on_A = Sequential(model_A.layers[:-1])\n",
        "model_B_on_A.add(Dense(1, activation=\"sigmoid\"))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 6: Freeze base layers**\n",
        "\n",
        "```python\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "```\n",
        "\n",
        "Only new layer trains (like a linear classifier on pretrained features).\n",
        "\n",
        "Train for **4 epochs**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 7: Unfreeze & fine tune**\n",
        "\n",
        "```python\n",
        "for layer in model_B_on_A.layers[:-1]:\n",
        "    layer.trainable = True\n",
        "```\n",
        "\n",
        "Train everything together for **16 more epochs**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 8: Evaluate**\n",
        "\n",
        "```python\n",
        "model_B_on_A.evaluate(X_test_B, y_test_B)\n",
        "```\n",
        "\n",
        "---\n",
        "<details>"
      ],
      "metadata": {
        "id": "QtJ0xriX-iIq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0q2n9c30uj0"
      },
      "source": [
        "# Faster Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrmuEF5N0uj0"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì a little function to test an optimizer on Fashion MNIST\n",
        "\n",
        "def build_model(seed=42):\n",
        "    tf.random.set_seed(seed)\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                              kernel_initializer=\"he_normal\"),\n",
        "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                              kernel_initializer=\"he_normal\"),\n",
        "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                              kernel_initializer=\"he_normal\"),\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "\n",
        "def build_and_train_model(optimizer):\n",
        "    model = build_model()\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model.fit(X_train, y_train, epochs=10,\n",
        "                     validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuI5msYr0uj1"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnFDt52o0uj2"
      },
      "outputs": [],
      "source": [
        "history_sgd = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbDoFSOG0uj3"
      },
      "source": [
        "## Momentum optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFt3Giim0uj4"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDAH4luX0uj4"
      },
      "outputs": [],
      "source": [
        "history_momentum = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzhUhJnp0uj5"
      },
      "source": [
        "## Nesterov Accelerated Gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEQ603mn0uj6"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9,\n",
        "                                    nesterov=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eoYUth-0uj6"
      },
      "outputs": [],
      "source": [
        "history_nesterov = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hik0Z4_D0uj7"
      },
      "source": [
        "## AdaGrad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sF28GQO0uj8"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adagrad(learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob0f7ohG0uj8"
      },
      "outputs": [],
      "source": [
        "history_adagrad = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rfFbngH0uj9"
      },
      "source": [
        "## RMSProp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH4e5wjH0uj-"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qh_cuYhz0uj-"
      },
      "outputs": [],
      "source": [
        "history_rmsprop = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGV8HCeD0uj_"
      },
      "source": [
        "## Adam Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_gmOsVR0ukA"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9,\n",
        "                                     beta_2=0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lgpz8E70ukA"
      },
      "outputs": [],
      "source": [
        "history_adam = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NagHHZH0ukB"
      },
      "source": [
        "**Adamax Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91ekpeyR0ukB"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adamax(learning_rate=0.001, beta_1=0.9,\n",
        "                                       beta_2=0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mK_JNf20ukC"
      },
      "outputs": [],
      "source": [
        "history_adamax = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "mDICwc0B0ukD"
      },
      "source": [
        "**Nadam Optimization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4NxfpFc00ukE"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9,\n",
        "                                      beta_2=0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaVFtsnH0ukE"
      },
      "outputs": [],
      "source": [
        "history_nadam = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joxxsYlU0ukF"
      },
      "source": [
        "**AdamW Optimization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK9B9B7N0ukG"
      },
      "source": [
        "Note: Since TF 1.12, `AdamW` is no longer experimental. It is available at `tf.keras.optimizers.AdamW` instead of `tf.keras.optimizers.experimental.AdamW`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5jPi7qDO0ukG"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.AdamW(weight_decay=1e-5, learning_rate=0.001,\n",
        "                                      beta_1=0.9, beta_2=0.999)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVylDZqU0ukH"
      },
      "outputs": [],
      "source": [
        "history_adamw = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjXjrQip0ukI"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì visualize the learning curves of all the optimizers\n",
        "\n",
        "for loss in (\"loss\", \"val_loss\"):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    opt_names = \"SGD Momentum Nesterov AdaGrad RMSProp Adam Adamax Nadam AdamW\"\n",
        "    for history, opt_name in zip((history_sgd, history_momentum, history_nesterov,\n",
        "                                  history_adagrad, history_rmsprop, history_adam,\n",
        "                                  history_adamax, history_nadam, history_adamw),\n",
        "                                 opt_names.split()):\n",
        "        plt.plot(history.history[loss], label=f\"{opt_name}\", linewidth=3)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel({\"loss\": \"Training loss\", \"val_loss\": \"Validation loss\"}[loss])\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.axis([0, 9, 0.1, 0.7])\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b> ‚úÖAI Understanding Template-Deep NN-03 </b></summary>\n",
        "Below is a **short, crisp AI-style explanation**, followed by a **clear code walkthrough**.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **AI Understanding Template ‚Äî w.r.t. This Code**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A simple **fully-connected neural network** for MNIST-like 28√ó28 images.\n",
        "You are **training the same model with different optimizers** to compare learning behaviors.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "* **Flatten** turns the 28√ó28 image into a 784-dim vector.\n",
        "* **Dense layers (ReLU)** learn non-linear patterns (edges, shapes).\n",
        "* **Final softmax** produces class probabilities (0‚Äì9).\n",
        "* Different **optimizers** update weights differently (momentum, adaptive learning rates, decay).\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* On **complex images** (CNNs work better).\n",
        "* If learning rate is wrong ‚Üí slow learning or divergence.\n",
        "* Dense networks struggle with **spatial patterns** (location-specific features).\n",
        "* Overfits if trained too long without regularization.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use this design when:\n",
        "\n",
        "* Data is **simple and small** (MNIST).\n",
        "* You want to **compare optimizers**.\n",
        "* You need a **baseline model**.\n",
        "* You want to learn **optimizer effects on training curves**.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think of it like:\n",
        "\n",
        "> **‚ÄúA stack of 3 hidden layers learning pixel patterns, and the optimizers decide *how* to update the weights.‚Äù**\n",
        "\n",
        "Or simple:\n",
        "\n",
        "> **Model = Pattern learner\n",
        "> Optimizer = Weight-update strategy**\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "(Not prompting text ‚Äî prompting the *model building/training*.)\n",
        "\n",
        "To use it correctly:\n",
        "\n",
        "* Give input shape **[28, 28]**.\n",
        "* Use correct loss: **sparse_categorical_crossentropy**.\n",
        "* Pick optimizer based on need:\n",
        "\n",
        "  * `SGD` ‚Üí simple, slow\n",
        "  * `Momentum/Nesterov` ‚Üí faster\n",
        "  * `Adam/AdamW` ‚Üí best general-purpose\n",
        "  * `RMSprop` ‚Üí stabilizes noisy gradients\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "Better architectures:\n",
        "\n",
        "* **CNNs** ‚Üí Conv2D, MaxPool, much higher accuracy\n",
        "* **ResNet-style blocks**\n",
        "* **Vision Transformers** (advanced)\n",
        "\n",
        "Better training strategies:\n",
        "\n",
        "* Learning rate schedulers\n",
        "* Batch normalization\n",
        "* Dropout\n",
        "* Weight decay regularization\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **Code Explanation (Short & Clear)**\n",
        "\n",
        "---\n",
        "\n",
        "## **Function 1: build_model()**\n",
        "\n",
        "```python\n",
        "def build_model(seed=42):\n",
        "    tf.random.set_seed(seed)\n",
        "```\n",
        "\n",
        "‚û° Ensures reproducible weight initialization.\n",
        "\n",
        "```python\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "```\n",
        "\n",
        "‚û° Converts each 28√ó28 image ‚Üí 784 vector.\n",
        "\n",
        "```python\n",
        "        tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                              kernel_initializer=\"he_normal\"),\n",
        "```\n",
        "\n",
        "‚û° First hidden layer\n",
        "\n",
        "* 100 neurons\n",
        "* ReLU activation\n",
        "* `he_normal` ‚Üí ideal for ReLU networks\n",
        "\n",
        "(Repeated 3 times: three hidden layers of size 100.)\n",
        "\n",
        "```python\n",
        "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "```\n",
        "\n",
        "‚û° Output layer\n",
        "\n",
        "* 10 classes\n",
        "* Softmax ‚Üí probability distribution\n",
        "\n",
        "---\n",
        "\n",
        "## **Function 2: build_and_train_model(optimizer)**\n",
        "\n",
        "```python\n",
        "model = build_model()\n",
        "```\n",
        "\n",
        "‚û° Creates the same architecture every time.\n",
        "\n",
        "```python\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "```\n",
        "\n",
        "‚û° Sets loss + optimizer (passed externally).\n",
        "‚û° Uses label integers (0‚Äì9), so sparse loss is correct.\n",
        "\n",
        "```python\n",
        "return model.fit(X_train, y_train, epochs=10,\n",
        "                 validation_data=(X_valid, y_valid))\n",
        "```\n",
        "\n",
        "‚û° Runs training for 10 epochs and returns history.\n",
        "\n",
        "---\n",
        "\n",
        "## **Training with Different Optimizers**\n",
        "\n",
        "Each block builds a new model and trains it:\n",
        "\n",
        "### **SGD**\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "```\n",
        "\n",
        "‚û° Slow, pure gradient descent.\n",
        "\n",
        "### **Momentum**\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
        "```\n",
        "\n",
        "‚û° Adds velocity ‚Üí converges faster.\n",
        "\n",
        "### **Nesterov**\n",
        "\n",
        "‚û° Looks ahead before updating.\n",
        "\n",
        "### **Adagrad**\n",
        "\n",
        "‚û° Adapts learning rate per parameter.\n",
        "\n",
        "### **RMSprop**\n",
        "\n",
        "‚û° Good for noisy gradients.\n",
        "\n",
        "### **Adam**\n",
        "\n",
        "‚û° Most popular (momentum + RMSprop ideas).\n",
        "\n",
        "### **Adamax**\n",
        "\n",
        "‚û° Variant of Adam using infinity norm.\n",
        "\n",
        "### **Nadam**\n",
        "\n",
        "‚û° Adam + Nesterov momentum.\n",
        "\n",
        "### **AdamW**\n",
        "\n",
        "‚û° Adam + decoupled weight decay (improves generalization).\n",
        "\n",
        "---\n",
        "\n",
        "## **Visualization**\n",
        "\n",
        "The loop plots **training loss** and **validation loss** curves for each optimizer.\n",
        "\n",
        "```python\n",
        "for loss in (\"loss\", \"val_loss\"):\n",
        "    plt.figure(figsize=(12, 8))\n",
        "```\n",
        "\n",
        "‚û° Creates separate plots for training & validation loss.\n",
        "\n",
        "```python\n",
        "opt_names = \"SGD Momentum Nesterov AdaGrad RMSProp Adam Adamax Nadam AdamW\"\n",
        "```\n",
        "\n",
        "‚û° Optimizer labels.\n",
        "\n",
        "```python\n",
        "plt.plot(history.history[loss], label=f\"{opt_name}\", linewidth=3)\n",
        "```\n",
        "\n",
        "‚û° Plots learning curves for each optimizer.\n",
        "\n",
        "`plt.legend()` ‚Üí Shows optimizer names\n",
        "`plt.axis([0, 9, 0.1, 0.7])` ‚Üí Fixed axis range\n",
        "`plt.show()` ‚Üí Final graph\n",
        "\n",
        "---\n",
        "\n",
        "# Want a **comparison table** of all optimizers (strengths/weaknesses) in 5 lines?\n",
        "\n",
        "Or a **CNN version** of this experiment?\n",
        "\n",
        "<details>"
      ],
      "metadata": {
        "id": "XVO0PlC8BXtA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXTu4kWy0ukI"
      },
      "source": [
        "## Learning Rate Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPT0lve50ukJ"
      },
      "source": [
        "### Power Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bAYanuI0ukJ"
      },
      "source": [
        "```python\n",
        "learning_rate = initial_learning_rate / (1 + step / decay_steps)**power\n",
        "```\n",
        "\n",
        "Keras uses `power = 1`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yhhxr6m0ukK"
      },
      "source": [
        "**Note**: Optimizers used to have a `decay` argument for this, but it was deprecated. You must use the schedulers in `tf.keras.optimizers.schedules` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykQkEiPl0ukK"
      },
      "outputs": [],
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10_000,\n",
        "    decay_rate=1.0,\n",
        "    staircase=False\n",
        ")\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rC2mNkr0ukK"
      },
      "source": [
        "The `InverseTimeDecay` scheduler uses `learning_rate = initial_learning_rate / (1 + decay_rate * step / decay_steps)`. If you set `staircase=True`, then it replaces `step / decay_step` with `floor(step / decay_step)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZj3PE0e0ukL"
      },
      "outputs": [],
      "source": [
        "history_power_scheduling = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9XtLZX00ukL"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell plots power scheduling with staircase=True or False\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "decay_rate = 1.0\n",
        "decay_steps = 10_000\n",
        "\n",
        "steps = np.arange(100_000)\n",
        "lrs = initial_learning_rate / (1 + decay_rate * steps / decay_steps)\n",
        "lrs2 = initial_learning_rate / (1 + decay_rate * np.floor(steps / decay_steps))\n",
        "\n",
        "plt.plot(steps, lrs,  \"-\", label=\"staircase=False\")\n",
        "plt.plot(steps, lrs2,  \"-\", label=\"staircase=True\")\n",
        "plt.axis([0, steps.max(), 0, 0.0105])\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Power Scheduling\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAWk_WAO0ukM"
      },
      "source": [
        "### Exponential Scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P65-jJmm0ukM"
      },
      "source": [
        "```python\n",
        "learning_rate = initial_learning_rate * decay_rate ** (step / decay_steps)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k12h_1mb0ukN"
      },
      "outputs": [],
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=20_000,\n",
        "    decay_rate=0.1,\n",
        "    staircase=False\n",
        ")\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mm7U9ZY30ukN"
      },
      "outputs": [],
      "source": [
        "history_exponential_scheduling = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQYDSvN70ukO"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell plots exponential scheduling\n",
        "\n",
        "initial_learning_rate = 0.01\n",
        "decay_rate = 0.1\n",
        "decay_steps = 20_000\n",
        "\n",
        "steps = np.arange(100_000)\n",
        "lrs = initial_learning_rate * decay_rate ** (steps / decay_steps)\n",
        "lrs2 = initial_learning_rate * decay_rate ** np.floor(steps / decay_steps)\n",
        "\n",
        "plt.plot(steps, lrs,  \"-\", label=\"staircase=False\")\n",
        "plt.plot(steps, lrs2,  \"-\", label=\"staircase=True\")\n",
        "plt.axis([0, steps.max(), 0, 0.0105])\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAFpQO4J0ukO"
      },
      "source": [
        "Keras also provides a `LearningRateScheduler` callback class that lets you define your own scheduling function. Let's see how you could use it to implement exponential decay. Note that in this case the learning rate only changes at each epoch, not at each step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nhbf_8gc0ukP"
      },
      "outputs": [],
      "source": [
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1 ** (epoch / 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lApfUBAo0ukP"
      },
      "outputs": [],
      "source": [
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 ** (epoch / s)\n",
        "    return exponential_decay_fn\n",
        "\n",
        "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZc1MX770ukQ"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì build and compile a model for Fashion MNIST\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "model = build_model()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "urh-bOmm0ukQ"
      },
      "outputs": [],
      "source": [
        "n_epochs = 20\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "history = model.fit(X_train, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[lr_scheduler])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOiW_FTz0ukQ"
      },
      "source": [
        "Alternatively, the schedule function can take the current learning rate as a second argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kysfaQgp0ukR"
      },
      "outputs": [],
      "source": [
        "def exponential_decay_fn(epoch, lr):\n",
        "    return lr * 0.1 ** (1 / 20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uf6nP12T0ukR"
      },
      "source": [
        "**Extra material**: if you want to use a custom scheduling function that updates the learning rate at each iteration rather than at each epoch, you can write your own callback class like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZVSUjWn0ukS"
      },
      "outputs": [],
      "source": [
        "K = tf.keras.backend\n",
        "\n",
        "class ExponentialDecay(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, n_steps=40_000):\n",
        "        super().__init__()\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        # Note: the `batch` argument is reset at each epoch\n",
        "        lr = self.model.optimizer.learning_rate.numpy()\n",
        "        new_learning_rate = lr * 0.1 ** (1 / self.n_steps)\n",
        "        self.model.optimizer.learning_rate = new_learning_rate\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = self.model.optimizer.learning_rate.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtU4N5eC0ukS"
      },
      "outputs": [],
      "source": [
        "lr0 = 0.01\n",
        "model = build_model()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6kb6KEY0ukS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "batch_size = 32\n",
        "n_steps = n_epochs * math.ceil(len(X_train) / batch_size)\n",
        "exp_decay = ExponentialDecay(n_steps)\n",
        "history = model.fit(X_train, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[exp_decay])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b> ‚úÖAI Understanding Template-Deep NN-04 </b></summary>\n",
        "Below is a **short, crisp AI-style breakdown + full code explanation** for **Learning-Rate Schedules** used in your code.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **AI Understanding Template ‚Äî Learning Rate Scheduling**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A method to **automatically reduce the learning rate over time** while training a model.\n",
        "Your code demonstrates:\n",
        "\n",
        "* **Inverse Time Decay**\n",
        "* **Exponential Decay**\n",
        "* **Custom Callbacks** for per-epoch and per-batch decay\n",
        "* **Plots** comparing staircase vs smooth decay\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "LR schedules assume:\n",
        "\n",
        "> **‚ÄúHigh LR early = explore; low LR later = converge.‚Äù**\n",
        "\n",
        "So the optimizer takes:\n",
        "\n",
        "* **large steps at the beginning**\n",
        "* **tiny, stable steps near convergence**\n",
        "\n",
        "Inverse-time reasoning:\n",
        "\n",
        "> LR drops slowly ‚Üí stable finish\n",
        "\n",
        "Exponential reasoning:\n",
        "\n",
        "> LR shrinks by a constant factor ‚Üí fast drop\n",
        "\n",
        "Callbacks reason per-step or per-epoch by recomputing LR before applying gradients.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* If LR **drops too early** ‚Üí underfitting\n",
        "* If LR **drops too slowly** ‚Üí long training\n",
        "* If decay rate is wrong ‚Üí oscillation or stagnation\n",
        "* For very noisy datasets or adaptive optimizers (Adam) ‚Üí sometimes unnecessary\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "Use LR schedules when:\n",
        "\n",
        "* Training **SGD** ‚Üí schedules matter a lot\n",
        "* You want **smooth convergence**\n",
        "* You don‚Äôt want to hand-tune LR manually\n",
        "* The model is sensitive to step-size (e.g., CNNs, RNNs)\n",
        "\n",
        "Avoid when:\n",
        "\n",
        "* Using **AdamW / RMSProp** (already adaptive) unless fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "Think of LR scheduling as:\n",
        "\n",
        "> **‚ÄúTurn the volume down gradually so training becomes less noisy.‚Äù**\n",
        "\n",
        "Or:\n",
        "\n",
        "> **‚ÄúStart fast, finish slow.‚Äù**\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it?**\n",
        "\n",
        "In practical terms:\n",
        "\n",
        "* Provide **initial_learning_rate**\n",
        "* Choose **decay strategy**\n",
        "* Pass schedule to **optimizer**\n",
        "* Visualize LR curve to confirm\n",
        "* For callbacks ‚Üí define function(epoch) and plug into `LearningRateScheduler`\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "| Alternative                     | Notes                                       |\n",
        "| ------------------------------- | ------------------------------------------- |\n",
        "| **Step Decay**                  | Drop LR every N epochs by factor.           |\n",
        "| **Cosine Decay**                | Smooth cosine curve ‚Äì very popular.         |\n",
        "| **Warmup + Decay**              | Best for transformers.                      |\n",
        "| **Adaptive optimizers (AdamW)** | Often removes need for LR scheduling.       |\n",
        "| **ReduceLROnPlateau callback**  | Auto-decay when validation stops improving. |\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **Code Explanation (Simple & Clear)**\n",
        "\n",
        "---\n",
        "\n",
        "## **A) Inverse Time Decay**\n",
        "\n",
        "### **Formula**\n",
        "\n",
        "```python\n",
        "learning_rate = initial_lr / (1 + step / decay_steps)**power\n",
        "```\n",
        "\n",
        "### **TF implementation**\n",
        "\n",
        "```python\n",
        "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=10_000,\n",
        "    decay_rate=1.0,\n",
        "    staircase=False\n",
        ")\n",
        "```\n",
        "\n",
        "* **staircase=False** ‚Üí smooth curve\n",
        "* **staircase=True** ‚Üí drops in steps\n",
        "\n",
        "### **Optimizer**\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "```\n",
        "\n",
        "### **Plotting**\n",
        "\n",
        "`lrs` ‚Üí smooth decay\n",
        "`lrs2` ‚Üí staircase decay using `floor()`\n",
        "\n",
        "---\n",
        "\n",
        "## **B) Exponential Decay**\n",
        "\n",
        "### **Formula**\n",
        "\n",
        "```python\n",
        "learning_rate = initial_lr * decay_rate ** (step / decay_steps)\n",
        "```\n",
        "\n",
        "### **TF implementation**\n",
        "\n",
        "```python\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate=0.01,\n",
        "    decay_steps=20_000,\n",
        "    decay_rate=0.1,\n",
        "    staircase=False\n",
        ")\n",
        "```\n",
        "\n",
        "Same plot logic as inverse decay.\n",
        "\n",
        "---\n",
        "\n",
        "## **C) Exponential Decay (Custom Function ‚Äî per epoch)**\n",
        "\n",
        "### **Simple function**\n",
        "\n",
        "```python\n",
        "def exponential_decay_fn(epoch):\n",
        "    return 0.01 * 0.1 ** (epoch / 20)\n",
        "```\n",
        "\n",
        "### **Using a factory function**\n",
        "\n",
        "```python\n",
        "def exponential_decay(lr0, s):\n",
        "    def exponential_decay_fn(epoch):\n",
        "        return lr0 * 0.1 ** (epoch / s)\n",
        "    return exponential_decay_fn\n",
        "```\n",
        "\n",
        "### **Adding to callbacks**\n",
        "\n",
        "```python\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
        "```\n",
        "\n",
        "This updates LR **at each epoch**.\n",
        "\n",
        "---\n",
        "\n",
        "## **D) Exponential Decay (Per Batch using a Callback)**\n",
        "\n",
        "### **Custom Callback**\n",
        "\n",
        "```python\n",
        "class ExponentialDecay(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, n_steps):\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "    def on_batch_begin(self, batch, logs=None):\n",
        "        lr = self.model.optimizer.learning_rate.numpy()\n",
        "        new_lr = lr * 0.1 ** (1 / self.n_steps)\n",
        "        self.model.optimizer.learning_rate = new_lr\n",
        "```\n",
        "\n",
        "* Updates LR **every batch**\n",
        "* Produces extremely smooth exponential decay\n",
        "\n",
        "### **Using it**\n",
        "\n",
        "```python\n",
        "exp_decay = ExponentialDecay(n_steps)\n",
        "history = model.fit(..., callbacks=[exp_decay])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **Ultra-Short Summary (as per your default preference)**\n",
        "\n",
        "* These snippets show **Inverse Time** and **Exponential** LR decays.\n",
        "* TF schedules: automatic & clean.\n",
        "* Custom callbacks: more control (epoch/batch).\n",
        "* Helps SGD converge better.\n",
        "* Staircase vs smooth = stepwise vs continuous LR drop.\n",
        "* Alternatives: cosine decay, warmup, ReduceLROnPlateau, AdamW.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also give you:\n",
        "‚úì **One-page cheat sheet**\n",
        "‚úì **Visual intuition diagrams**\n",
        "‚úì **Comparison table: which LR schedule to use when**\n",
        "\n",
        "<details>"
      ],
      "metadata": {
        "id": "nHVFwKmgRmZs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCVN7JHP0ukT"
      },
      "source": [
        "### Piecewise Constant Scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jWPVL2lB0ukT"
      },
      "outputs": [],
      "source": [
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries = [50_000 * n_epochs // batch_size, 80_000 * n_epochs // batch_size] ,\n",
        "    values=[0.01, 0.005, 0.001]\n",
        ")\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zooj8Xlb0ukT"
      },
      "outputs": [],
      "source": [
        "history_piecewise_scheduling = build_and_train_model(optimizer)  # extra code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Qdp8PyQQ0ukU"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell plots piecewise constant scheduling\n",
        "\n",
        "boundaries = [50_000, 80_000]\n",
        "values = [0.01, 0.005, 0.001]\n",
        "\n",
        "steps = np.arange(100_000)\n",
        "\n",
        "lrs = np.full(len(steps), values[0])\n",
        "for boundary, value in zip(boundaries, values[1:]):\n",
        "    lrs[boundary:] = value\n",
        "\n",
        "plt.plot(steps, lrs, \"-\")\n",
        "plt.axis([0, steps.max(), 0, 0.0105])\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0Yg37ac0ukU"
      },
      "source": [
        "Just like we did with exponential scheduling, we could also implement piecewise constant scheduling manually:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzjSPwmD0ukU"
      },
      "outputs": [],
      "source": [
        "def piecewise_constant_fn(epoch):\n",
        "    if epoch < 5:\n",
        "        return 0.01\n",
        "    elif epoch < 15:\n",
        "        return 0.005\n",
        "    else:\n",
        "        return 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQFda6tj0ukV"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell demonstrates a more general way to define\n",
        "#              piecewise constant scheduling.\n",
        "\n",
        "def piecewise_constant(boundaries, values):\n",
        "    boundaries = np.array([0] + boundaries)\n",
        "    values = np.array(values)\n",
        "    def piecewise_constant_fn(epoch):\n",
        "        return values[(boundaries > epoch).argmax() - 1]\n",
        "    return piecewise_constant_fn\n",
        "\n",
        "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjvdQo-00ukV"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì use a tf.keras.callbacks.LearningRateScheduler like earlier\n",
        "\n",
        "n_epochs = 25\n",
        "\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
        "\n",
        "model = build_model()\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=lr0)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[lr_scheduler])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETPy-6Iu0ukV"
      },
      "source": [
        "We've looked at `InverseTimeDecay`, `ExponentialDecay`, and `PiecewiseConstantDecay`. A few more schedulers are available in `tf.keras.optimizers.schedules`, here is the full list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AEP7w-J0ukW"
      },
      "outputs": [],
      "source": [
        "for name in sorted(dir(tf.keras.optimizers.schedules)):\n",
        "    if name[0] == name[0].lower():  # must start with capital letter\n",
        "        continue\n",
        "    scheduler_class = getattr(tf.keras.optimizers.schedules, name)\n",
        "    print(f\"‚Ä¢ {name} ‚Äì {scheduler_class.__doc__.splitlines()[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMze9iKG0ukW"
      },
      "source": [
        "### Performance Scheduling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0ccErM90ukW"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì build and compile the model\n",
        "\n",
        "model = build_model()\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr0)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1ht8q6c0ukX"
      },
      "outputs": [],
      "source": [
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "history = model.fit(X_train, y_train, epochs=n_epochs,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[lr_scheduler])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm5Dc1uo0ukX"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì this cell plots performance scheduling\n",
        "\n",
        "plt.plot(history.epoch, history.history[\"learning_rate\"], \"bo-\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Learning Rate\", color='b')\n",
        "plt.tick_params('y', colors='b')\n",
        "plt.gca().set_xlim(0, n_epochs - 1)\n",
        "plt.grid(True)\n",
        "\n",
        "ax2 = plt.gca().twinx()\n",
        "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
        "ax2.set_ylabel('Validation Loss', color='r')\n",
        "ax2.tick_params('y', colors='r')\n",
        "\n",
        "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2ruja6z0ukY"
      },
      "source": [
        "### 1Cycle scheduling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qVr7zyA0ukY"
      },
      "source": [
        "The `ExponentialLearningRate` custom callback updates the learning rate during training, at the end of each batch. It multiplies it by a constant `factor`. It also saves the learning rate and loss at each batch. Since `logs[\"loss\"]` is actually the mean loss since the start of the epoch, and we want to save the batch loss instead, we must compute the mean times the number of batches since the beginning of the epoch to get the total loss so far, then we subtract the total loss at the previous batch to get the current batch's loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3ei5JGB0ukY"
      },
      "outputs": [],
      "source": [
        "K = tf.keras.backend\n",
        "\n",
        "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, factor):\n",
        "        self.factor = factor\n",
        "        self.rates = []\n",
        "        self.losses = []\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.sum_of_epoch_losses = 0\n",
        "\n",
        "    def on_batch_end(self, batch, logs=None):\n",
        "        mean_epoch_loss = logs[\"loss\"]  # the epoch's mean loss so far\n",
        "        new_sum_of_epoch_losses = mean_epoch_loss * (batch + 1)\n",
        "        batch_loss = new_sum_of_epoch_losses - self.sum_of_epoch_losses\n",
        "        self.sum_of_epoch_losses = new_sum_of_epoch_losses\n",
        "        lr = self.model.optimizer.learning_rate.numpy()\n",
        "        self.rates.append(lr)\n",
        "        self.losses.append(batch_loss)\n",
        "        self.model.optimizer.learning_rate = lr * self.factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHiEuGBv0ukZ"
      },
      "source": [
        "The `find_learning_rate()` function trains the model using the `ExponentialLearningRate` callback, and it returns the learning rates and corresponding batch losses. At the end, it restores the model and its optimizer to their initial state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0TPqZjF0ukZ"
      },
      "outputs": [],
      "source": [
        "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=1e-4,\n",
        "                       max_rate=1):\n",
        "    init_weights = model.get_weights()\n",
        "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
        "    factor = (max_rate / min_rate) ** (1 / iterations)\n",
        "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
        "    model.optimizer.learning_rate = min_rate\n",
        "    exp_lr = ExponentialLearningRate(factor)\n",
        "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
        "                        callbacks=[exp_lr])\n",
        "    model.optimizer.learning_rate = init_lr\n",
        "    model.set_weights(init_weights)\n",
        "    return exp_lr.rates, exp_lr.losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly-VGDYa0ukZ"
      },
      "source": [
        "The `plot_lr_vs_loss()` function plots the learning rates vs the losses. The optimal learning rate to use as the maximum learning rate in 1cycle is near the bottom of the curve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fL7aeNv50ukZ"
      },
      "outputs": [],
      "source": [
        "def plot_lr_vs_loss(rates, losses):\n",
        "    plt.plot(rates, losses, \"b\")\n",
        "    plt.gca().set_xscale('log')\n",
        "    max_loss = losses[0] + min(losses)\n",
        "    plt.hlines(min(losses), min(rates), max(rates), color=\"k\")\n",
        "    plt.axis([min(rates), max(rates), 0, max_loss])\n",
        "    plt.xlabel(\"Learning rate\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uo_ObJy70uka"
      },
      "source": [
        "Let's build a simple Fashion MNIST model and compile it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BdvWGY10uka"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rID01woa0uka"
      },
      "source": [
        "Now let's find the optimal max learning rate for 1cycle:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eSGL1RGB0uka"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "rates, losses = find_learning_rate(model, X_train, y_train, epochs=1,\n",
        "                                   batch_size=batch_size)\n",
        "plot_lr_vs_loss(rates, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeFlV6Fd0ukb"
      },
      "source": [
        "Looks like the max learning rate to use for 1cycle is around 10<sup>‚Äì1</sup>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXumj7w90ukb"
      },
      "source": [
        "The `OneCycleScheduler` custom callback updates the learning rate at the beginning of each batch. It applies the logic described in the book: increase the learning rate linearly during about half of training, then reduce it linearly back to the initial learning rate, and lastly reduce it down to close to zero linearly for the very last part of training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uYi4Smu0ukb"
      },
      "outputs": [],
      "source": [
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, iterations, max_lr=1e-3, start_lr=None,\n",
        "                 last_iterations=None, last_lr=None):\n",
        "        self.iterations = iterations\n",
        "        self.max_lr = max_lr\n",
        "        self.start_lr = start_lr or max_lr / 10\n",
        "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
        "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
        "        self.last_lr = last_lr or self.start_lr / 1000\n",
        "        self.iteration = 0\n",
        "\n",
        "    def _interpolate(self, iter1, iter2, lr1, lr2):\n",
        "        return (lr2 - lr1) * (self.iteration - iter1) / (iter2 - iter1) + lr1\n",
        "\n",
        "    def on_batch_begin(self, batch, logs):\n",
        "        if self.iteration < self.half_iteration:\n",
        "            lr = self._interpolate(0, self.half_iteration, self.start_lr,\n",
        "                                   self.max_lr)\n",
        "        elif self.iteration < 2 * self.half_iteration:\n",
        "            lr = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
        "                                   self.max_lr, self.start_lr)\n",
        "        else:\n",
        "            lr = self._interpolate(2 * self.half_iteration, self.iterations,\n",
        "                                   self.start_lr, self.last_lr)\n",
        "        self.iteration += 1\n",
        "        self.model.optimizer.learning_rate = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRsUer430ukb"
      },
      "source": [
        "Let's build and compile a simple Fashion MNIST model, then train it using the `OneCycleScheduler` callback:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdYkDUP-0ukc"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(),\n",
        "              metrics=[\"accuracy\"])\n",
        "n_epochs = 25\n",
        "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs,\n",
        "                             max_lr=0.1)\n",
        "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[onecycle])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b>#‚úÖAI Understanding Template-Deep NN-03 </b></summary>\n",
        "\n",
        "Below is the **short, crisp AI-style breakdown (HML style)** + **clear explanation of all code** in the requested format.\n",
        "\n",
        "---\n",
        "\n",
        "## **AI Understanding Template (for Learning-Rate Scheduling Code)**\n",
        "\n",
        "## **1. What is it?**\n",
        "\n",
        "A collection of **learning rate schedulers** in TensorFlow/Keras:\n",
        "\n",
        "* **PiecewiseConstantDecay**\n",
        "* **Manual piecewise functions**\n",
        "* **LearningRateScheduler callback**\n",
        "* **ReduceLROnPlateau**\n",
        "* **Learning Rate Finder**\n",
        "* **One-Cycle Policy**\n",
        "\n",
        "These control *how the learning rate changes during training*.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. How does it reason?**\n",
        "\n",
        "It adjusts the learning rate based on:\n",
        "\n",
        "* **Fixed boundaries** (piecewise)\n",
        "* **Epoch number** (callbacks)\n",
        "* **Validation loss behaviour** (plateau)\n",
        "* **Loss vs LR curve** (LR finder)\n",
        "* **Cyclic schedule** (1-cycle)\n",
        "\n",
        "Reasoning = ‚Äú**Use high LR to explore, low LR to converge**‚Äù.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Where does it fail?**\n",
        "\n",
        "* Wrong boundaries ‚Üí model may diverge\n",
        "* Too low LR ‚Üí slow learning\n",
        "* Too aggressive cycles ‚Üí unstable training\n",
        "* Small datasets ‚Üí LR finder becomes noisy\n",
        "* Plateau scheduler triggers too early on noisy val-loss\n",
        "\n",
        "---\n",
        "\n",
        "## **4. When should I use it?**\n",
        "\n",
        "* **Piecewise** ‚Üí when exact phases of training are known\n",
        "* **Scheduler callback** ‚Üí simple epoch-based schedules\n",
        "* **Plateau** ‚Üí unstable/real-world training where loss fluctuates\n",
        "* **LR finder** ‚Üí you don‚Äôt know the optimal LR\n",
        "* **OneCycle** ‚Üí best for SGD/CLR training (vision, NLP)\n",
        "\n",
        "---\n",
        "\n",
        "## **5. What is the mental model?**\n",
        "\n",
        "> **LR scheduling = thermostat for training stability.**\n",
        "> Too high = overheating (divergence)\n",
        "> Too low = cold engine (slow learning)\n",
        "\n",
        "Schedulers = automatic way to keep LR in the ideal zone.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. How do I prompt it (use it)?**\n",
        "\n",
        "* For **static schedule**, define boundaries + values\n",
        "* For **callbacks**, return LR based on epoch\n",
        "* For **plateau**, just set factor + patience\n",
        "* For **LR finder**, train once with exponential LR growth\n",
        "* For **OneCycle**, specify total iterations + max LR\n",
        "\n",
        "No natural-language prompting ‚Äî you ‚Äúprompt‚Äù via **functions & callbacks**.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. What are alternatives?**\n",
        "\n",
        "| Alternative       | When to use                              |\n",
        "| ----------------- | ---------------------------------------- |\n",
        "| Constant LR       | Small models, simple tasks               |\n",
        "| CosineDecay       | Vision, transformers                     |\n",
        "| PolynomialDecay   | Long schedule, fine control              |\n",
        "| AdamW/Lookahead   | When LR scheduling alone is insufficient |\n",
        "| Custom cyclic LRs | Time-series / noisy datasets             |\n",
        "\n",
        "---\n",
        "\n",
        "## **Now the Code Explanation (Simple + Sharp)**\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå PIECEWISE CONSTANT DECAY**\n",
        "\n",
        "### **Code**\n",
        "\n",
        "```python\n",
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=[50_000 * n_epochs // batch_size, 80_000 * n_epochs // batch_size],\n",
        "    values=[0.01, 0.005, 0.001]\n",
        ")\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule)\n",
        "```\n",
        "\n",
        "### **Meaning**\n",
        "\n",
        "* LR = **0.01** until boundary 1\n",
        "* then **0.005** until boundary 2\n",
        "* then **0.001**\n",
        "\n",
        "Boundaries = **training steps**, not epochs.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Manual plotting logic**\n",
        "\n",
        "```python\n",
        "steps = np.arange(100_000)\n",
        "lrs = np.full(len(steps), values[0])\n",
        "for boundary, value in zip(boundaries, values[1:]):\n",
        "    lrs[boundary:] = value\n",
        "```\n",
        "\n",
        "Simple NumPy simulation that:\n",
        "\n",
        "* Fills entire LR array with first LR\n",
        "* For each boundary ‚Üí override LR after that point\n",
        "\n",
        "Used only for graph visualization.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Manual piecewise function (epoch-based)**\n",
        "\n",
        "```python\n",
        "def piecewise_constant_fn(epoch):\n",
        "    if epoch < 5: return 0.01\n",
        "    elif epoch < 15: return 0.005\n",
        "    else: return 0.001\n",
        "```\n",
        "\n",
        "Used with:\n",
        "\n",
        "```python\n",
        "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
        "```\n",
        "\n",
        "Works per **epoch**, not per step.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Generalized piecewise function**\n",
        "\n",
        "```python\n",
        "def piecewise_constant(boundaries, values):\n",
        "    boundaries = np.array([0] + boundaries)\n",
        "    values = np.array(values)\n",
        "    def piecewise_constant_fn(epoch):\n",
        "        return values[(boundaries > epoch).argmax() - 1]\n",
        "    return piecewise_constant_fn\n",
        "```\n",
        "\n",
        "This builds a **generic** piecewise LR schedule:\n",
        "\n",
        "```python\n",
        "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå ReduceLROnPlateau**\n",
        "\n",
        "```python\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* If **val_loss** does not improve for **5 epochs** ‚Üí LR = LR √ó 0.5\n",
        "* Automatic & adaptive.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Learning Rate Finder**\n",
        "\n",
        "### **Key callback**\n",
        "\n",
        "```python\n",
        "class ExponentialLearningRate(tf.keras.callbacks.Callback):\n",
        "    ...\n",
        "    self.model.optimizer.learning_rate = lr * self.factor\n",
        "```\n",
        "\n",
        "* Increases LR **every batch** exponentially\n",
        "* Records loss + LR for plotting\n",
        "* Purpose: find ‚Äúsweet spot LR‚Äù\n",
        "\n",
        "### **Function**\n",
        "\n",
        "```python\n",
        "rates, losses = find_learning_rate(...)\n",
        "```\n",
        "\n",
        "### **Plot**\n",
        "\n",
        "```python\n",
        "plt.plot(rates, losses)\n",
        "plt.xscale('log')\n",
        "```\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "* LR where loss stops decreasing ‚Üí chosen LR\n",
        "* Best LR = slightly before loss spikes\n",
        "\n",
        "---\n",
        "\n",
        "## **üìå One-Cycle Learning Rate Policy**\n",
        "\n",
        "### **Class**\n",
        "\n",
        "```python\n",
        "class OneCycleScheduler(tf.keras.callbacks.Callback):\n",
        "```\n",
        "\n",
        "Logic:\n",
        "\n",
        "* Phase 1: LR ‚Üë from low ‚Üí high\n",
        "* Phase 2: LR ‚Üì back to low\n",
        "* Phase 3: LR drops ‚Üí very low for final convergence\n",
        "\n",
        "Used with:\n",
        "\n",
        "```python\n",
        "onecycle = OneCycleScheduler(total_iterations, max_lr=0.1)\n",
        "```\n",
        "\n",
        "This is currently state-of-the-art for **SGD training**.\n",
        "\n",
        "---\n",
        "\n",
        "## **Short Summary**\n",
        "\n",
        "**This entire block of code demonstrates**:\n",
        "‚úî multiple LR schedules (piecewise, manual, callbacks)\n",
        "‚úî adaptive LR (plateau)\n",
        "‚úî LR finder\n",
        "‚úî 1-cycle policy\n",
        "‚úî how LR impacts convergence, speed, stability\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also prepare:\n",
        "‚úÖ one-line memory hooks for each scheduler\n",
        "‚úÖ comparison table\n",
        "‚úÖ recommended LR strategy for your project\n",
        "‚úÖ integrate with your Wide-&-Deep model\n",
        "\n",
        "<details>"
      ],
      "metadata": {
        "id": "a6J3b1P5TBqJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j30cBm5P0ukc"
      },
      "source": [
        "# Avoiding Overfitting Through Regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbQvTTrX0ukc"
      },
      "source": [
        "## $\\ell_1$ and $\\ell_2$ regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQh23m0T0ukc"
      },
      "outputs": [],
      "source": [
        "layer = tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                              kernel_initializer=\"he_normal\",\n",
        "                              kernel_regularizer=tf.keras.regularizers.l2(0.01))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TmCidAo0ukd"
      },
      "source": [
        "Or use `l1(0.1)` for ‚Ñì<sub>1</sub> regularization with a factor of 0.1, or `l1_l2(0.1, 0.01)` for both ‚Ñì<sub>1</sub> and ‚Ñì<sub>2</sub> regularization, with factors 0.1 and 0.01 respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNe-vv5q0ukd"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code ‚Äì for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cMaSRjp0ukd"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "RegularizedDense = partial(tf.keras.layers.Dense,\n",
        "                           activation=\"relu\",\n",
        "                           kernel_initializer=\"he_normal\",\n",
        "                           kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\",\n",
        "                     kernel_initializer=\"glorot_normal\")  # not in the book\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFz_1sdF0ukd"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì compile and train the model\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=2,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0H67QjP0uke"
      },
      "source": [
        "## Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iaiuuk6l0uke"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code ‚Äì for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prr8Oeyc0uke"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\",\n",
        "                          kernel_initializer=\"he_normal\"),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_QV_67L0ukf"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì compile and train the model\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gDUK8Uc0ukf"
      },
      "source": [
        "The training accuracy looks like it's lower than the validation accuracy, but that's just because dropout is only active during training. If we evaluate the model on the training set after training (i.e., with dropout turned off), we get the \"real\" training accuracy, which is very slightly higher than the validation accuracy and the test accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsFhpnRO0ukf"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFlwK1-a0ukf"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZolCHYDL0ukg"
      },
      "source": [
        "**Note**: make sure to use `AlphaDropout` instead of `Dropout` if you want to build a self-normalizing neural net using SELU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lT-3Pjgz0ukg"
      },
      "source": [
        "## MC Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBW76WoV0ukg"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)  # extra code ‚Äì for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8NZ4IB70ukh"
      },
      "outputs": [],
      "source": [
        "y_probas = np.stack([model(X_test, training=True)\n",
        "                     for sample in range(100)])\n",
        "y_proba = y_probas.mean(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "en230feS0ukh"
      },
      "outputs": [],
      "source": [
        "model.predict(X_test[:1]).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo-Ts8NX0ukh"
      },
      "outputs": [],
      "source": [
        "y_proba[0].round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nG-jkqNf0ukh"
      },
      "outputs": [],
      "source": [
        "y_std = y_probas.std(axis=0)\n",
        "y_std[0].round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Hkf36I30uki"
      },
      "outputs": [],
      "source": [
        "y_pred = y_proba.argmax(axis=1)\n",
        "accuracy = (y_pred == y_test).sum() / len(y_test)\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_mZRSpu40uki"
      },
      "outputs": [],
      "source": [
        "class MCDropout(tf.keras.layers.Dropout):\n",
        "    def call(self, inputs, training=None):\n",
        "        return super().call(inputs, training=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KR_IT4iP0uki"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì shows how to convert Dropout to MCDropout in a Sequential model\n",
        "Dropout = tf.keras.layers.Dropout\n",
        "mc_model = tf.keras.Sequential([\n",
        "    MCDropout(layer.rate) if isinstance(layer, Dropout) else layer\n",
        "    for layer in model.layers\n",
        "])\n",
        "mc_model.set_weights(model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWSuqGYB0uki"
      },
      "outputs": [],
      "source": [
        "mc_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eRY84C80ukj"
      },
      "source": [
        "Now we can use the model with MC Dropout:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Be8sLTZ0ukj"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì shows that the model works without retraining\n",
        "tf.random.set_seed(42)\n",
        "np.mean([mc_model.predict(X_test[:1])\n",
        "         for sample in range(100)], axis=0).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ-0yYOu0ukj"
      },
      "source": [
        "## Max norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJLqjOfR0ukj"
      },
      "outputs": [],
      "source": [
        "dense = tf.keras.layers.Dense(\n",
        "    100, activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "    kernel_constraint=tf.keras.constraints.max_norm(1.))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfW28ckq0ukk"
      },
      "outputs": [],
      "source": [
        "# extra code ‚Äì shows how to apply max norm to every hidden layer in a model\n",
        "\n",
        "MaxNormDense = partial(tf.keras.layers.Dense,\n",
        "                       activation=\"relu\", kernel_initializer=\"he_normal\",\n",
        "                       kernel_constraint=tf.keras.constraints.max_norm(1.))\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    MaxNormDense(100),\n",
        "    MaxNormDense(100),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary><b> ‚úÖAI Understanding Template-Deep NN-05 </b></summary>\n",
        "\t\t\t<p>Avoiding Overfitting Through Regularization</p>\n",
        "Below is a **short, crisp AI-style explanation** (your preferred style) plus a **clear explanation of each code block**.\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **AI: Wide Explanation Template (for this code)**\n",
        "\n",
        "(Answering the 7 questions exactly as requested)\n",
        "\n",
        "---\n",
        "\n",
        "## **1) What is it?**\n",
        "\n",
        "A set of **regularized neural-network techniques** for MNIST-style image classification:\n",
        "\n",
        "* L2 regularization\n",
        "* Dropout\n",
        "* MC Dropout (Bayesian uncertainty)\n",
        "* Max-Norm regularization\n",
        "\n",
        "Used to **reduce overfitting** and **estimate uncertainty**.\n",
        "\n",
        "---\n",
        "\n",
        "## **2) How does it reason?**\n",
        "\n",
        "The model learns patterns using:\n",
        "\n",
        "* **Dense layers** ‚Üí detect abstract features\n",
        "* **L2** ‚Üí penalizes large weights (shrinks them)\n",
        "* **Dropout** ‚Üí randomly silences neurons ‚Üí forces redundancy\n",
        "* **MC Dropout** ‚Üí runs dropout at inference ‚Üí produces uncertainty\n",
        "* **MaxNorm** ‚Üí caps weight magnitude after each update\n",
        "\n",
        "Reasoning = pattern recognition via gradients while controlling model complexity.\n",
        "\n",
        "---\n",
        "\n",
        "## **3) Where does it fail?**\n",
        "\n",
        "* On **high-resolution images** (CNN works better)\n",
        "* If dropout rate is too high ‚Üí underfitting\n",
        "* If regularization is too strong ‚Üí slow/poor learning\n",
        "* For sequential data (use RNN/Transformers instead)\n",
        "\n",
        "---\n",
        "\n",
        "## **4) When should I use it?**\n",
        "\n",
        "Use when:\n",
        "\n",
        "* Dataset is **small/medium**\n",
        "* Model **overfits**\n",
        "* You need **uncertainty in predictions** (medical, finance)\n",
        "* You want fast training without CNNs\n",
        "\n",
        "---\n",
        "\n",
        "## **5) What is the mental model?**\n",
        "\n",
        "Think of it as:\n",
        "\n",
        "> **‚ÄúA simple dense neural network wearing multiple safety belts.‚Äù**\n",
        ">\n",
        "> * L2 shrinks weights\n",
        "> * Dropout hides neurons randomly\n",
        "> * MC Dropout measures uncertainty\n",
        "> * Max-Norm prevents exploding weights\n",
        "\n",
        "---\n",
        "\n",
        "## **6) How do I prompt it?**\n",
        "\n",
        "(*How to configure/guide the model via hyperparameters*)\n",
        "\n",
        "* Choose **activation** = relu\n",
        "* Choose **kernel_initializer** = he_normal (best for relu)\n",
        "* Choose **L2 / dropout / max-norm** based on overfitting\n",
        "* Train with SGD(+momentum) or Adam\n",
        "* Use many forward passes for uncertainty\n",
        "\n",
        "---\n",
        "\n",
        "## **7) What are alternatives?**\n",
        "\n",
        "| Alternative                  | When to use                         |\n",
        "| ---------------------------- | ----------------------------------- |\n",
        "| **CNN**                      | For images (better accuracy)        |\n",
        "| **BatchNorm + No Dropout**   | Faster convergence                  |\n",
        "| **Early Stopping**           | Instead of strong regularization    |\n",
        "| **Bayesian Neural Networks** | If uncertainty estimation is needed |\n",
        "| **Data Augmentation**        | Instead of heavy L2/dropout         |\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **Now the Code Explanation (Block by Block)**\n",
        "\n",
        "---\n",
        "\n",
        "# **üîπ 1. Dense layer with L2 regularization**\n",
        "\n",
        "```python\n",
        "layer = tf.keras.layers.Dense(\n",
        "    100, activation=\"relu\",\n",
        "    kernel_initializer=\"he_normal\",\n",
        "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        ")\n",
        "```\n",
        "\n",
        "### **Meaning**\n",
        "\n",
        "* `Dense(100)` ‚Üí 100 neurons\n",
        "* `relu` ‚Üí for non-linearity\n",
        "* `he_normal` ‚Üí best initializer for relu\n",
        "* `l2(0.01)` ‚Üí penalizes large weights (reduces overfitting)\n",
        "\n",
        "---\n",
        "\n",
        "# **üîπ 2. Using `partial` to reuse the same layer configuration**\n",
        "\n",
        "```python\n",
        "RegularizedDense = partial(\n",
        "    tf.keras.layers.Dense,\n",
        "    activation=\"relu\",\n",
        "    kernel_initializer=\"he_normal\",\n",
        "    kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        ")\n",
        "```\n",
        "\n",
        "### **Meaning**\n",
        "\n",
        "`RegularizedDense(100)` automatically creates a Dense layer with all regularization settings.\n",
        "Usefulness ‚Üí **reduces repetition**.\n",
        "\n",
        "---\n",
        "\n",
        "# **üîπ 3. Building a regularized Sequential model**\n",
        "\n",
        "```python\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(100),\n",
        "    RegularizedDense(10, activation=\"softmax\", kernel_initializer=\"glorot_normal\")\n",
        "])\n",
        "```\n",
        "\n",
        "### **Meaning**\n",
        "\n",
        "* Flatten 28√ó28 ‚Üí vector\n",
        "* Two hidden Dense layers with L2\n",
        "* Output layer: 10 classes (softmax)\n",
        "\n",
        "---\n",
        "\n",
        "# **üîπ 4. Compiling and training**\n",
        "\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.02)\n",
        "model.compile(...)\n",
        "model.fit(...)\n",
        "```\n",
        "\n",
        "### Purpose:\n",
        "\n",
        "Train with **SGD**, using **cross-entropy** for classification.\n",
        "\n",
        "---\n",
        "\n",
        "# **üîπ 5. Dropout Model**\n",
        "\n",
        "```python\n",
        "tf.keras.layers.Dropout(rate=0.2)\n",
        "```\n",
        "\n",
        "### Meaning\n",
        "\n",
        "Randomly drops 20% of units. Reduces overfitting.\n",
        "\n",
        "Model architecture:\n",
        "Flatten ‚Üí Dropout ‚Üí Dense ‚Üí Dropout ‚Üí Dense ‚Üí Dropout ‚Üí Output\n",
        "\n",
        "---\n",
        "\n",
        "# **üîπ 6. MC Dropout (Bayesian inference)**\n",
        "\n",
        "### **Forward passes with dropout enabled at inference**\n",
        "\n",
        "```python\n",
        "y_probas = np.stack([model(X_test, training=True) for _ in range(100)])\n",
        "y_proba = y_probas.mean(axis=0)\n",
        "```\n",
        "\n",
        "Meaning:\n",
        "\n",
        "* Each prediction is slightly different\n",
        "* Average gives **mean prediction**\n",
        "* Std gives **uncertainty**\n",
        "\n",
        "---\n",
        "\n",
        "### Convert normal Dropout ‚Üí always-on Dropout\n",
        "\n",
        "```python\n",
        "class MCDropout(tf.keras.layers.Dropout):\n",
        "    def call(self, inputs, training=None):\n",
        "        return super().call(inputs, training=True)\n",
        "```\n",
        "\n",
        "Wrap whole model:\n",
        "\n",
        "```python\n",
        "mc_model = tf.keras.Sequential([\n",
        "    MCDropout(layer.rate) if isinstance(layer, Dropout) else layer\n",
        "    for layer in model.layers\n",
        "])\n",
        "```\n",
        "\n",
        "### Use\n",
        "\n",
        "This gives **Bayesian-like uncertainty** *without retraining*.\n",
        "\n",
        "---\n",
        "\n",
        "# **üîπ 7. Max-Norm Regularization**\n",
        "\n",
        "```python\n",
        "dense = tf.keras.layers.Dense(\n",
        "    100, activation=\"relu\",\n",
        "    kernel_initializer=\"he_normal\",\n",
        "    kernel_constraint=tf.keras.constraints.max_norm(1.)\n",
        ")\n",
        "```\n",
        "\n",
        "### Meaning\n",
        "\n",
        "After each gradient update:\n",
        "\n",
        "> **weights are clipped to max norm ‚â§ 1**\n",
        "\n",
        "Prevents exploding weights.\n",
        "\n",
        "Then using `partial`:\n",
        "\n",
        "```python\n",
        "MaxNormDense = partial(\n",
        "    tf.keras.layers.Dense,\n",
        "    activation=\"relu\",\n",
        "    kernel_initializer=\"he_normal\",\n",
        "    kernel_constraint=tf.keras.constraints.max_norm(1.)\n",
        ")\n",
        "```\n",
        "\n",
        "Model:\n",
        "Flatten ‚Üí MaxNormDense ‚Üí MaxNormDense ‚Üí Softmax\n",
        "\n",
        "---\n",
        "\n",
        "# ‚úÖ **Final Summary (super short)**\n",
        "\n",
        "This code shows **multiple regularization strategies** in dense neural networks:\n",
        "\n",
        "* L2 regularization\n",
        "* Dropout\n",
        "* Monte-Carlo Dropout (uncertainty)\n",
        "* Max-Norm constraints\n",
        "\n",
        "And uses reusable layer patterns (`partial`) for cleaner code.\n",
        "\n",
        "---\n",
        "\n",
        "If you want, I can also give:\n",
        "‚úî **One-page cheat sheet for all regularization types**\n",
        "‚úî **Architectural diagrams**\n",
        "‚úî **Side-by-side comparison table**\n",
        "‚úî **Simplified version for your notes**\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "YkDRtasOXCVo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxwXP7tT0ukk"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjg6QQ6o0ukk"
      },
      "source": [
        "## 1. to 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFdMgwFd0ukk"
      },
      "source": [
        "1. Glorot initialization and He initialization were designed to make the output standard deviation as close as possible to the input standard deviation, at least at the beginning of training. This reduces the vanishing/exploding gradients problem.\n",
        "2. No, all weights should be sampled independently; they should not all have the same initial value. One important goal of sampling weights randomly is to break symmetry: if all the weights have the same initial value, even if that value is not zero, then symmetry is not broken (i.e., all neurons in a given layer are equivalent), and backpropagation will be unable to break it. Concretely, this means that all the neurons in any given layer will always have the same weights. It's like having just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution.\n",
        "3. It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that's OK too; it does not make much difference.\n",
        "4. ReLU is usually a good default for the hidden layers, as it is fast and yields good results. Its ability to output precisely zero can also be useful in some cases (e.g., see Chapter 17). Moreover, it can sometimes benefit from optimized implementations as well as from hardware acceleration. The leaky ReLU variants of ReLU can improve the model's quality without hindering its speed too much compared to ReLU. For large neural nets and more complex problems, GLU, Swish and Mish can give you a slightly higher quality model, but they have a computational cost. The hyperbolic tangent (tanh) can be useful in the output layer if you need to output a number in a fixed range (by default between ‚Äì1 and 1), but nowadays it is not used much in hidden layers, except in recurrent nets. The sigmoid activation function is also useful in the output layer when you need to estimate a probability (e.g., for binary classification), but it is rarely used in hidden layers (there are exceptions‚Äîfor example, for the coding layer of variational autoencoders; see Chapter 17). The softplus activation function is useful in the output layer when you need to ensure that the output will always be positive. The softmax activation function is useful in the output layer to estimate probabilities for mutually exclusive classes, but it is rarely (if ever) used in hidden layers.\n",
        "5. If you set the `momentum` hyperparameter too close to 1 (e.g., 0.99999) when using an `SGD` optimizer, then the algorithm will likely pick up a lot of speed, hopefully moving roughly toward the global minimum, but its momentum will carry it right past the minimum. Then it will slow down and come back, accelerate again, overshoot again, and so on. It may oscillate this way many times before converging, so overall it will take much longer to converge than with a smaller `momentum` value.\n",
        "6. One way to produce a sparse model (i.e., with most weights equal to zero) is to train the model normally, then zero out tiny weights. For more sparsity, you can apply ‚Ñì<sub>1</sub> regularization during training, which pushes the optimizer toward sparsity. A third option is to use the TensorFlow Model Optimization Toolkit.\n",
        "7. Yes, dropout does slow down training, in general roughly by a factor of two. However, it has no impact on inference speed since it is only turned on during training. MC Dropout is exactly like dropout during training, but it is still active during inference, so each inference is slowed down slightly. More importantly, when using MC Dropout you generally want to run inference 10 times or more to get better predictions. This means that making predictions is slowed down by a factor of 10 or more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7c96LHL0ukl"
      },
      "source": [
        "## 8. Deep Learning on CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwHeP5zd0ukl"
      },
      "source": [
        "### a.\n",
        "*Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the Swish activation function.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nq6TlKQP0ukl"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(100,\n",
        "                                    activation=\"swish\",\n",
        "                                    kernel_initializer=\"he_normal\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SILvcSLI0ukm"
      },
      "source": [
        "### b.\n",
        "*Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with `tf.keras.datasets.cifar10.load_data()`. The dataset is composed of 60,000 32 √ó 32‚Äìpixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APreBpEf0ukm"
      },
      "source": [
        "Let's add the output layer to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKk5hgW-0ukn"
      },
      "outputs": [],
      "source": [
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D07V7VhU0ukn"
      },
      "source": [
        "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwhGz5wI0ukn"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-5)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsfanotc0ukn"
      },
      "source": [
        "Let's load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so9P2TA_0ukn"
      },
      "outputs": [],
      "source": [
        "cifar10 = tf.keras.datasets.cifar10.load_data()\n",
        "(X_train_full, y_train_full), (X_test, y_test) = cifar10\n",
        "\n",
        "X_train = X_train_full[5000:]\n",
        "y_train = y_train_full[5000:]\n",
        "X_valid = X_train_full[:5000]\n",
        "y_valid = y_train_full[:5000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2YLa4G10uko"
      },
      "source": [
        "Now we can create the callbacks we need and train the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTvqrcFO0uko"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True)\n",
        "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_model.keras\",\n",
        "                                                         save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_{run_index:03d}\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1AJdpKz0uko"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./my_cifar10_logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnsG2PPI0uko"
      },
      "outputs": [],
      "source": [
        "model.fit(X_train, y_train, epochs=100,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRMqyGpl0ukp"
      },
      "outputs": [],
      "source": [
        "model.evaluate(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3dparug0ukp"
      },
      "source": [
        "The model with the lowest validation loss gets about 46.8% accuracy on the validation set. It took 29 epochs to reach the lowest validation loss, with roughly 10 seconds per epoch on my laptop (without a GPU). Let's see if we can improve the model using Batch Normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvRqtBv_0ukp"
      },
      "source": [
        "### c.\n",
        "*Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za-UmMTG0ukp"
      },
      "source": [
        "The code below is very similar to the code above, with a few changes:\n",
        "\n",
        "* I added a BN layer after every Dense layer (before the activation function), except for the output layer.\n",
        "* I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
        "* I renamed the run directories to run_bn_* and the model file name to `my_cifar10_bn_model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNuwGCuC0ukq"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
        "    model.add(tf.keras.layers.BatchNormalization())\n",
        "    model.add(tf.keras.layers.Activation(\"swish\"))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=20,\n",
        "                                                     restore_best_weights=True)\n",
        "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.keras\",\n",
        "                                                         save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_bn_{run_index:03d}\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "model.fit(X_train, y_train, epochs=100,\n",
        "          validation_data=(X_valid, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.evaluate(X_valid, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8vGwlBO0ukq"
      },
      "source": [
        "* *Is the model converging faster than before?* Much faster! The previous model took 29 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 12 epochs and continued to make progress until the 17th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
        "* *Does BN produce a better model?* Yes! The final model is also much better, with 50.7% validation accuracy instead of 46.7%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
        "* *How does BN affect training speed?* Although the model converged much faster, each epoch took about 15s instead of 10s, because of the extra computations required by the BN layers. But overall the training time (wall time) to reach the best model was shortened by about 10%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2QIgRNd0ukq"
      },
      "source": [
        "### d.\n",
        "*Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "UsCHIMMG0ukq"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(100,\n",
        "                                    kernel_initializer=\"lecun_normal\",\n",
        "                                    activation=\"selu\"))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=7e-4)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=20, restore_best_weights=True)\n",
        "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_cifar10_selu_model.keras\", save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_selu_{run_index:03d}\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=100,\n",
        "          validation_data=(X_valid_scaled, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJD3-emp0ukr"
      },
      "source": [
        "This model reached the first model's validation loss in just 8 epochs. After 14 epochs, it reached its lowest validation loss, with about 50.3% accuracy, which is better than the original model (46.7%), but not quite as good as the model using batch normalization (50.7%). Each epoch took only 9 seconds. So it's the fastest model to train so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3GyT3ow0ukr"
      },
      "source": [
        "### e.\n",
        "*Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H65NlyR_0ukr"
      },
      "source": [
        "**Warning**: there are now two versions of `AlphaDropout`. One is deprecated and also broken in some recent versions of TF, and unfortunately that's the version in the `tensorflow` library. Luckily, there's a perfectly fine version in the `keras` library (i.e., `keras`, not `tf.keras`). It's neither deprecated nor broken, so let's import and use that one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_YiIAqy0ukr"
      },
      "outputs": [],
      "source": [
        "import keras.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpf3hJnZ0ukr"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(100,\n",
        "                                    kernel_initializer=\"lecun_normal\",\n",
        "                                    activation=\"selu\"))\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-4)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
        "    patience=20, restore_best_weights=True)\n",
        "model_checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"my_cifar10_alpha_dropout_model.keras\", save_best_only=True)\n",
        "run_index = 1 # increment every time you train the model\n",
        "run_logdir = Path() / \"my_cifar10_logs\" / f\"run_alpha_dropout_{run_index:03d}\"\n",
        "tensorboard_cb = tf.keras.callbacks.TensorBoard(run_logdir)\n",
        "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
        "\n",
        "X_means = X_train.mean(axis=0)\n",
        "X_stds = X_train.std(axis=0)\n",
        "X_train_scaled = (X_train - X_means) / X_stds\n",
        "X_valid_scaled = (X_valid - X_means) / X_stds\n",
        "X_test_scaled = (X_test - X_means) / X_stds\n",
        "\n",
        "model.fit(X_train_scaled, y_train, epochs=100,\n",
        "          validation_data=(X_valid_scaled, y_valid),\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model.evaluate(X_valid_scaled, y_valid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xiUd8u_0uks"
      },
      "source": [
        "The model reaches 48.1% accuracy on the validation set. That's worse than without dropout (50.3%). With an extensive hyperparameter search, it might be possible to do better (I tried dropout rates of 5%, 10%, 20% and 40%, and learning rates 1e-4, 3e-4, 5e-4, and 1e-3), but probably not much better in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF-75BZN0uks"
      },
      "source": [
        "Let's use MC Dropout now. We will need the `MCAlphaDropout` class we used earlier, so let's just copy it here for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wDGyrDB0uks"
      },
      "outputs": [],
      "source": [
        "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
        "    def call(self, inputs):\n",
        "        return super().call(inputs, training=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hv730KZ0uks"
      },
      "source": [
        "Now let's create a new model, identical to the one we just trained (with the same weights), but with `MCAlphaDropout` dropout layers instead of `AlphaDropout` layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv5IYsYl0uks"
      },
      "outputs": [],
      "source": [
        "mc_model = tf.keras.Sequential([\n",
        "    (\n",
        "        MCAlphaDropout(layer.rate)\n",
        "        if isinstance(layer, keras.layers.AlphaDropout)\n",
        "        else layer\n",
        "    )\n",
        "    for layer in model.layers\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYQtzKwU0ukt"
      },
      "source": [
        "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pXOJeFka0ukt"
      },
      "outputs": [],
      "source": [
        "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
        "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
        "    return np.mean(Y_probas, axis=0)\n",
        "\n",
        "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
        "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
        "    return Y_probas.argmax(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpCnFYhm0ukt"
      },
      "source": [
        "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvAw0crg0ukt"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
        "accuracy = (y_pred == y_valid[:, 0]).mean()\n",
        "accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgpzSDzE0ukt"
      },
      "source": [
        "We get back to roughly the accuracy of the model without dropout in this case (about 50.3% accuracy).\n",
        "\n",
        "So the best model we got in this exercise is the Batch Normalization model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IH8Wmu2G0uku"
      },
      "source": [
        "### f.\n",
        "*Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUP4_5210uku"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(100,\n",
        "                                    kernel_initializer=\"lecun_normal\",\n",
        "                                    activation=\"selu\"))\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr9Cp1UV0uku"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1,\n",
        "                                   batch_size=batch_size)\n",
        "plot_lr_vs_loss(rates, losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ak8HeEo70uku"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
        "for _ in range(20):\n",
        "    model.add(tf.keras.layers.Dense(100,\n",
        "                                 kernel_initializer=\"lecun_normal\",\n",
        "                                 activation=\"selu\"))\n",
        "\n",
        "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
        "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=2e-2)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_lUaNKx0ukv"
      },
      "outputs": [],
      "source": [
        "n_epochs = 15\n",
        "n_iterations = math.ceil(len(X_train_scaled) / batch_size) * n_epochs\n",
        "onecycle = OneCycleScheduler(n_iterations, max_lr=0.05)\n",
        "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_valid_scaled, y_valid),\n",
        "                    callbacks=[onecycle])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNROjBWo0ukv"
      },
      "source": [
        "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 50.7% to 52.0%)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "nav_menu": {
      "height": "360px",
      "width": "416px"
    },
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}